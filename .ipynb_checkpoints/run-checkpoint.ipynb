{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting albumentations\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/b0/be/3db3cd8af771988748f69eace42047d5edebf01eaa7e1293f3b3f75f989e/albumentations-1.0.0-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 8.1 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting opencv-python-headless>=4.1.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/ce/08/baa108293aad5c07fb8aa613ec9443ce600f948dc4afaa24ab0a6c492a16/opencv_python_headless-4.5.2.54-cp36-cp36m-manylinux2014_x86_64.whl (38.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 38.2 MB 925 kB/s eta 0:00:011     |█████████████████████▏          | 25.3 MB 498 kB/s eta 0:00:26\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from albumentations) (1.5.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from albumentations) (5.3.1)\n",
      "Collecting scikit-image>=0.16.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/0e/ba/53e1bfbdfd0f94514d71502e3acea494a8b4b57c457adbc333ef386485da/scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 993 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from albumentations) (1.18.5)\n",
      "Collecting networkx>=2.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/f3/b7/c7f488101c0bb5e4178f3cde416004280fd40262433496830de8a8c21613/networkx-2.5.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 13.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting imageio>=2.3.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/6e/57/5d899fae74c1752f52869b613a8210a2480e1a69688e65df6cb26117d45d/imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 14.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyWavelets>=1.1.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/59/bb/d2b85265ec9fa3c1922210c9393d4cdf7075cc87cce6fe671d7455f80fbc/PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 44.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from scikit-image>=0.16.1->albumentations) (2.2.2)\n",
      "Collecting tifffile>=2019.7.26\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/42/6b/93a8ee61c6fbe20fa9c17928bd3b80484902b7fd454cecaffba42f5052cb/tifffile-2020.9.3-py3-none-any.whl (148 kB)\n",
      "\u001b[K     |████████████████████████████████| 148 kB 24.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2020.1)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.2.0)\n",
      "Installing collected packages: opencv-python-headless, networkx, imageio, PyWavelets, tifffile, scikit-image, albumentations\n",
      "Successfully installed PyWavelets-1.1.1 albumentations-1.0.0 imageio-2.9.0 networkx-2.5.1 opencv-python-headless-4.5.2.54 scikit-image-0.17.2 tifffile-2020.9.3\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting transformers\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from transformers) (1.7.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/bf/20/3605db440db4f96d5ffd66b231a043ae451ec7e5e4d1a2fb6f20608006c4/tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from transformers) (1.18.5)\n",
      "Collecting huggingface-hub==0.0.8\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from transformers) (2.24.0)\n",
      "Collecting filelock\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from transformers) (20.4)\n",
      "Collecting sacremoses\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 27.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from transformers) (4.46.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/6b/38/0ed2670578d803cb14350c54adb2a79835870aa9e3ad2e732be7359cb0e8/regex-2021.4.4-cp36-cp36m-manylinux2014_x86_64.whl (722 kB)\n",
      "\u001b[K     |████████████████████████████████| 722 kB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.1.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: six in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from sacremoses->transformers) (0.15.1)\n",
      "Collecting click\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/76/0a/b6c5f311e32aeb3b406e03c079ade51e905ea630fc19d1262a46249c1c86/click-8.0.1-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 46.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tokenizers, dataclasses, filelock, huggingface-hub, regex, click, sacremoses, transformers\n",
      "Successfully installed click-8.0.1 dataclasses-0.8 filelock-3.0.12 huggingface-hub-0.0.8 regex-2021.4.4 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting tensorboard\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/44/f5/7feea02a3fb54d5db827ac4b822a7ba8933826b36de21880518250b8733a/tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/60/f9/802efd84988bffd9f644c03b6e66fde8e76c3aa33db4279ddd11c5d61f4b/tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 15.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/1a/c1/499e600ba0c618b451cd9c425ae1c177249940a2086316552fee7d86c954/tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 15.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from tensorboard) (47.3.1.post20200616)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/bd/24/11c3ea5a7e866bf2d97f0501d0b4b1c9bbeade102bb4b588f0d2919a5212/Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 29.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.24.3\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/0e/4b/c4e1839cdb3248e1006837e1d427e1843f80f2e6ba69f3af77f00bb51ac4/grpcio-1.38.0-cp36-cp36m-manylinux2014_x86_64.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/9d/d3/7541e89f1fc456eef157224f597a8bba22589db6369a03eaba68c11f07a0/google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/23/47/835652c7e19530973c73c65e652fc53bd05725d5a7cf9bb8706777869c1e/absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from tensorboard) (2.24.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/6e/33/1ae0f71395e618d6140fbbc9587cc3156591f748226075e0f7d6f9176522/Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 20.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/50/ac/12df156754ee13ea4d57c77de8c4298e8fd00b0e580fecd8c7395374a7d8/google_auth-1.31.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 22.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.12.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from tensorboard) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from tensorboard) (3.12.2)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from werkzeug>=0.11.15->tensorboard) (0.8)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.2 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from grpcio>=1.24.3->tensorboard) (1.15.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.7.0)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/bf/28/c4f5796c67ad06bb91d98d543a5e01805c1ff065e08871f78e52d2a331ad/cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 24.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/e9/93/0c0f002031f18b53af7a6166103c02b9c0667be528944137cc954ec921b3/rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/e8/5d/9dd1c29e5a786525f6342f6c1d812ed2e37edc653ad297048c1668988053/oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 13.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (3.1.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 20.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tensorboard-data-server, tensorboard-plugin-wit, werkzeug, grpcio, oauthlib, requests-oauthlib, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, google-auth-oauthlib, absl-py, markdown, tensorboard\n",
      "Successfully installed absl-py-0.13.0 cachetools-4.2.2 google-auth-1.31.0 google-auth-oauthlib-0.4.4 grpcio-1.38.0 markdown-3.3.4 oauthlib-3.1.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 werkzeug-2.0.1\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting torchtext==0.5\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 6.4 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchtext==0.5) (1.15.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 7.5 MB/s eta 0:00:01     |█████████████████▍              | 655 kB 7.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchtext==0.5) (1.4.0)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchtext==0.5) (2.24.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchtext==0.5) (1.18.5)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchtext==0.5) (4.46.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->torchtext==0.5) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->torchtext==0.5) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->torchtext==0.5) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->torchtext==0.5) (1.25.9)\n",
      "Installing collected packages: sentencepiece, torchtext\n",
      "Successfully installed sentencepiece-0.1.95 torchtext-0.5.0\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting jieba\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.2 MB 481 kB/s eta 0:00:011     |██████████████████████▏         | 13.3 MB 491 kB/s eta 0:00:12\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314478 sha256=ec841adbcef58772897f18fbb465ce613f041add605a84ed9e43ee4e4043b866\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/de/99/39/55dd43d023169a4464b9118a252e188367c3750c62526c46f3\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting timm\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/ee/08/1ccaf8d516935666b7fa5f6aaddf157c66208ea0c93bb847ae09f166354f/timm-0.4.9-py3-none-any.whl (346 kB)\n",
      "\u001b[K     |████████████████████████████████| 346 kB 9.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from timm) (0.5.0)\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from timm) (1.4.0)\n",
      "Requirement already satisfied: six in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchvision->timm) (1.15.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchvision->timm) (1.18.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchvision->timm) (7.1.2)\n",
      "Installing collected packages: timm\n",
      "Successfully installed timm-0.4.9\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting soundfile\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from soundfile) (1.14.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from cffi>=1.0->soundfile) (2.20)\n",
      "Installing collected packages: soundfile\n",
      "Successfully installed soundfile-0.10.3.post1\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting resampy\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/79/75/e22272b9c2185fc8f3af6ce37229708b45e8b855fd4bc38b4d6b040fff65/resampy-0.2.2.tar.gz (323 kB)\n",
      "\u001b[K     |████████████████████████████████| 323 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.10 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from resampy) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.13 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from resampy) (1.5.0)\n",
      "Collecting numba>=0.32\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/4a/c1/e7fdbfc886a9d9c11767533903db0d816c0f656fd6029f4a061742893694/numba-0.53.1-cp36-cp36m-manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 10.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.3 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from resampy) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from numba>=0.32->resampy) (47.3.1.post20200616)\n",
      "Collecting llvmlite<0.37,>=0.36.0rc1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/4d/5a/707cc7e072d71bc19869d093e5cf9b7be98cb42d2398489465474d007ce8/llvmlite-0.36.0-cp36-cp36m-manylinux2010_x86_64.whl (25.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.3 MB 83.9 MB/s eta 0:00:01     |████████████████████████▍       | 19.2 MB 83.9 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: resampy\n",
      "  Building wheel for resampy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=320720 sha256=a59a5988db6fc2f824ec076e3088b5e5c229df5d07713966a8c69c1c39ec1ce0\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/83/39/6b/c82c4d996c0be8bdf4ee84ded29b2294f81aced663aeb3413e\n",
      "Successfully built resampy\n",
      "Installing collected packages: llvmlite, numba, resampy\n",
      "Successfully installed llvmlite-0.36.0 numba-0.53.1 resampy-0.2.2\n"
     ]
    }
   ],
   "source": [
    "!./setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "train (0): 100%|██████████████████████████████| 141/141 [01:36<00:00,  1.47it/s]\n",
      "epoch(0)(video):  0.6482096685392892\n",
      "epoch(0)(audio):  0.634513039061845\n",
      "epoch(0)(text):  0.6507512827718295\n",
      "epoch(0)(fusion):  0.6635245517715165\n",
      "train (1): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.72it/s]\n",
      "train (2): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(2)(video):  0.6825316921619797\n",
      "epoch(2)(audio):  0.658005511853313\n",
      "epoch(2)(text):  0.68377814847154\n",
      "epoch(2)(fusion):  0.7164793808068958\n",
      "train (3): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (4): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(4)(video):  0.6978470196496707\n",
      "epoch(4)(audio):  0.6563006256746932\n",
      "epoch(4)(text):  0.6934482043044924\n",
      "epoch(4)(fusion):  0.7307478142567317\n",
      "train (5): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (6): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(6)(video):  0.6956477698358886\n",
      "epoch(6)(audio):  0.6632107071097291\n",
      "epoch(6)(text):  0.7074001458171614\n",
      "epoch(6)(fusion):  0.754342517825369\n",
      "train (7): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (8): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(8)(video):  0.7177713391827485\n",
      "epoch(8)(audio):  0.6646809891312178\n",
      "epoch(8)(text):  0.7126931652992821\n",
      "epoch(8)(fusion):  0.758925411598933\n",
      "train (9): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (10): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(10)(video):  0.7228428261016849\n",
      "epoch(10)(audio):  0.670038834943339\n",
      "epoch(10)(text):  0.7098672593688208\n",
      "epoch(10)(fusion):  0.7614512141408306\n",
      "train (11): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (12): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(12)(video):  0.7292725222290269\n",
      "epoch(12)(audio):  0.6708198564432607\n",
      "epoch(12)(text):  0.7046485585304719\n",
      "epoch(12)(fusion):  0.7633110896003005\n",
      "train (13): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (14): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(14)(video):  0.7274059334958773\n",
      "epoch(14)(audio):  0.6682195343011723\n",
      "epoch(14)(text):  0.7131325040953097\n",
      "epoch(14)(fusion):  0.7645309381491278\n",
      "train (15): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (16): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(16)(video):  0.7303133054464217\n",
      "epoch(16)(audio):  0.6712876083586382\n",
      "epoch(16)(text):  0.71082769413351\n",
      "epoch(16)(fusion):  0.7684514061145669\n",
      "train (17): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (18): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(18)(video):  0.7297115916672892\n",
      "epoch(18)(audio):  0.6664233258877238\n",
      "epoch(18)(text):  0.7180626533573978\n",
      "epoch(18)(fusion):  0.7683658270777125\n",
      "train (19): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (20): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(20)(video):  0.7453442296801583\n",
      "epoch(20)(audio):  0.6805762644496383\n",
      "epoch(20)(text):  0.7189651660067033\n",
      "epoch(20)(fusion):  0.7774199987269717\n",
      "train (21): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (22): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(22)(video):  0.7457597028885259\n",
      "epoch(22)(audio):  0.6828601160066176\n",
      "epoch(22)(text):  0.7185645596327537\n",
      "epoch(22)(fusion):  0.7792947705514444\n",
      "train (23): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (24): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(24)(video):  0.7485576443013583\n",
      "epoch(24)(audio):  0.6809235534071544\n",
      "epoch(24)(text):  0.7198643488301466\n",
      "epoch(24)(fusion):  0.7781847102569334\n",
      "train (25): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (26): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(26)(video):  0.7438771590635798\n",
      "epoch(26)(audio):  0.6810117243033993\n",
      "epoch(26)(text):  0.7206844599933511\n",
      "epoch(26)(fusion):  0.7785487014379515\n",
      "train (27): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (28): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(28)(video):  0.7458187825206997\n",
      "epoch(28)(audio):  0.6805273521157909\n",
      "epoch(28)(text):  0.7202642606528599\n",
      "epoch(28)(fusion):  0.7791856898833789\n",
      "train (29): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (30): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(30)(video):  0.7428260266340333\n",
      "epoch(30)(audio):  0.6811973407502413\n",
      "epoch(30)(text):  0.7178205511601777\n",
      "epoch(30)(fusion):  0.7793983258456011\n",
      "train (31): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (32): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(32)(video):  0.7455716388733408\n",
      "epoch(32)(audio):  0.6793865191426125\n",
      "epoch(32)(text):  0.7193509532811998\n",
      "epoch(32)(fusion):  0.7797002306427419\n",
      "train (33): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (34): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(34)(video):  0.7449485633753375\n",
      "epoch(34)(audio):  0.6809957521111524\n",
      "epoch(34)(text):  0.7185702269875583\n",
      "epoch(34)(fusion):  0.7797698849651884\n",
      "train (35): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (36): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(36)(video):  0.7403850620408603\n",
      "epoch(36)(audio):  0.6809813795921847\n",
      "epoch(36)(text):  0.718838809772626\n",
      "epoch(36)(fusion):  0.7817811181048786\n",
      "train (37): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (38): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "epoch(38)(video):  0.7398135209839731\n",
      "epoch(38)(audio):  0.6806664886847769\n",
      "epoch(38)(text):  0.716096172567275\n",
      "epoch(38)(fusion):  0.7810829992568235\n",
      "train (39): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (40): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(40)(video):  0.7402664915825502\n",
      "epoch(40)(audio):  0.6778703001412896\n",
      "epoch(40)(text):  0.718339318465071\n",
      "epoch(40)(fusion):  0.7799253786404825\n",
      "train (41): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (42): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(42)(video):  0.7422714310813914\n",
      "epoch(42)(audio):  0.6810601717885372\n",
      "epoch(42)(text):  0.7193423141557473\n",
      "epoch(42)(fusion):  0.7815773882894099\n",
      "train (43): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (44): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(44)(video):  0.7436108538103314\n",
      "epoch(44)(audio):  0.6793668138105755\n",
      "epoch(44)(text):  0.7177758911743812\n",
      "epoch(44)(fusion):  0.7815385621974036\n",
      "train (45): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (46): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(46)(video):  0.7431701237373193\n",
      "epoch(46)(audio):  0.6814590139620669\n",
      "epoch(46)(text):  0.7184695331529979\n",
      "epoch(46)(fusion):  0.7821113349661457\n",
      "train (47): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (48): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(48)(video):  0.7419699061276659\n",
      "epoch(48)(audio):  0.6811877816524158\n",
      "epoch(48)(text):  0.7183077583566306\n",
      "epoch(48)(fusion):  0.7814906219116384\n",
      "train (49): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (50): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(50)(video):  0.7428650598465212\n",
      "epoch(50)(audio):  0.6811089463122293\n",
      "epoch(50)(text):  0.7171191585990251\n",
      "epoch(50)(fusion):  0.7811972959386059\n",
      "train (51): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (52): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(52)(video):  0.7432425003651786\n",
      "epoch(52)(audio):  0.6813037446201009\n",
      "epoch(52)(text):  0.7179463716854964\n",
      "epoch(52)(fusion):  0.7821524642792268\n",
      "train (53): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (54): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(54)(video):  0.7421581787803797\n",
      "epoch(54)(audio):  0.681568583450445\n",
      "epoch(54)(text):  0.7177555494836095\n",
      "epoch(54)(fusion):  0.7817643443344455\n",
      "train (55): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (56): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(56)(video):  0.7445497386924406\n",
      "epoch(56)(audio):  0.6798404599761527\n",
      "epoch(56)(text):  0.7183954063092083\n",
      "epoch(56)(fusion):  0.7819554367731253\n",
      "train (57): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (58): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(58)(video):  0.7429213939795895\n",
      "epoch(58)(audio):  0.680611585980551\n",
      "epoch(58)(text):  0.7190558750319914\n",
      "epoch(58)(fusion):  0.7819784942167961\n",
      "train (59): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (60): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(60)(video):  0.7409777657957275\n",
      "epoch(60)(audio):  0.6810024037088491\n",
      "epoch(60)(text):  0.7172512450575996\n",
      "epoch(60)(fusion):  0.7821263105591763\n",
      "train (61): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (62): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(62)(video):  0.7420725794784246\n",
      "epoch(62)(audio):  0.6818576417129077\n",
      "epoch(62)(text):  0.7172394380162641\n",
      "epoch(62)(fusion):  0.78180490308466\n",
      "train (63): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (64): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(64)(video):  0.7419054305207443\n",
      "epoch(64)(audio):  0.6818674056295401\n",
      "epoch(64)(text):  0.7166581980480071\n",
      "epoch(64)(fusion):  0.7818762094272969\n",
      "train (65): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (66): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(66)(video):  0.740282745396029\n",
      "epoch(66)(audio):  0.6800156884383768\n",
      "epoch(66)(text):  0.7181278303551015\n",
      "epoch(66)(fusion):  0.7811928754836478\n",
      "train (67): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.72it/s]\n",
      "train (68): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(68)(video):  0.7424138435954628\n",
      "epoch(68)(audio):  0.6799140556948695\n",
      "epoch(68)(text):  0.7167298640297233\n",
      "epoch(68)(fusion):  0.780716265644469\n",
      "train (69): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (70): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(70)(video):  0.7421087014234\n",
      "epoch(70)(audio):  0.681524412885842\n",
      "epoch(70)(text):  0.716780752126323\n",
      "epoch(70)(fusion):  0.7806338878190304\n",
      "train (71): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (72): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(72)(video):  0.741677078896532\n",
      "epoch(72)(audio):  0.6808687024113345\n",
      "epoch(72)(text):  0.7184592054662663\n",
      "epoch(72)(fusion):  0.7814856735579314\n",
      "train (73): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (74): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(74)(video):  0.7430335032378912\n",
      "epoch(74)(audio):  0.6819894156116436\n",
      "epoch(74)(text):  0.7178569360886091\n",
      "epoch(74)(fusion):  0.781972276972001\n",
      "train (75): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (76): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(76)(video):  0.7420612000183087\n",
      "epoch(76)(audio):  0.6805830614851668\n",
      "epoch(76)(text):  0.7175187145894125\n",
      "epoch(76)(fusion):  0.781741776697633\n",
      "train (77): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (78): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(78)(video):  0.7426878288111229\n",
      "epoch(78)(audio):  0.6808827647066984\n",
      "epoch(78)(text):  0.7175511323821409\n",
      "epoch(78)(fusion):  0.7819088083178675\n",
      "train (79): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (80): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "epoch(80)(video):  0.7408534860719443\n",
      "epoch(80)(audio):  0.6813882557662185\n",
      "epoch(80)(text):  0.718452276022728\n",
      "epoch(80)(fusion):  0.7816993633298154\n",
      "train (81): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (82): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(82)(video):  0.743048358140217\n",
      "epoch(82)(audio):  0.6806476931508297\n",
      "epoch(82)(text):  0.719140252133609\n",
      "epoch(82)(fusion):  0.7814696315970735\n",
      "train (83): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (84): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(84)(video):  0.7403550275335015\n",
      "epoch(84)(audio):  0.680455978611015\n",
      "epoch(84)(text):  0.7186185016543486\n",
      "epoch(84)(fusion):  0.781540136162549\n",
      "train (85): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (86): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(86)(video):  0.7409845081904827\n",
      "epoch(86)(audio):  0.6818045015196231\n",
      "epoch(86)(text):  0.7186369604908758\n",
      "epoch(86)(fusion):  0.7816153816148376\n",
      "train (87):  34%|██████████▏                   | 48/141 [00:39<00:43,  2.13it/s]^C\n"
     ]
    }
   ],
   "source": [
    "# 采用asr的结果\n",
    "!python -W ignore main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "train (0): 100%|██████████████████████████████| 141/141 [01:25<00:00,  1.66it/s]\n",
      "epoch(0)(video):  0.6453660831056209\n",
      "epoch(0)(audio):  0.6310173637975652\n",
      "epoch(0)(text):  0.6393302034628048\n",
      "epoch(0)(fusion):  0.6541986625848379\n",
      "train (1): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (2): 100%|██████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(2)(video):  0.6827392172159827\n",
      "epoch(2)(audio):  0.6570411049694206\n",
      "epoch(2)(text):  0.6857805290566336\n",
      "epoch(2)(fusion):  0.7144750555489762\n",
      "train (3): 100%|██████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (4): 100%|██████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(4)(video):  0.6904383118876172\n",
      "epoch(4)(audio):  0.6550363140166056\n",
      "epoch(4)(text):  0.7036508054175634\n",
      "epoch(4)(fusion):  0.7336560067461474\n",
      "train (5): 100%|██████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (6): 100%|██████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(6)(video):  0.7078987848860042\n",
      "epoch(6)(audio):  0.6692717283037988\n",
      "epoch(6)(text):  0.7209426581035547\n",
      "epoch(6)(fusion):  0.757785171932195\n",
      "train (7): 100%|██████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (8): 100%|██████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(8)(video):  0.7319875394079428\n",
      "epoch(8)(audio):  0.6758056829465021\n",
      "epoch(8)(text):  0.725259469560105\n",
      "epoch(8)(fusion):  0.7623471952721884\n",
      "train (9): 100%|██████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (10): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.68it/s]\n",
      "epoch(10)(video):  0.7387579442470666\n",
      "epoch(10)(audio):  0.6768616782727327\n",
      "epoch(10)(text):  0.7267811693947507\n",
      "epoch(10)(fusion):  0.7673652448846422\n",
      "train (11): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (12): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(12)(video):  0.7379696604976747\n",
      "epoch(12)(audio):  0.6767724943373816\n",
      "epoch(12)(text):  0.7264266752871837\n",
      "epoch(12)(fusion):  0.7669348660125249\n",
      "train (13): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (14): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(14)(video):  0.7278324692219378\n",
      "epoch(14)(audio):  0.6707284999607492\n",
      "epoch(14)(text):  0.7234565914518012\n",
      "epoch(14)(fusion):  0.7681672845957277\n",
      "train (15): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (16): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "epoch(16)(video):  0.7391841596978351\n",
      "epoch(16)(audio):  0.6727707839706167\n",
      "epoch(16)(text):  0.7270484115492969\n",
      "epoch(16)(fusion):  0.7721218799573051\n",
      "train (17): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (18): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(18)(video):  0.7440976173799672\n",
      "epoch(18)(audio):  0.6764732426575691\n",
      "epoch(18)(text):  0.7261807220844498\n",
      "epoch(18)(fusion):  0.7737911741294248\n",
      "train (19): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (20): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(20)(video):  0.7407867485573459\n",
      "epoch(20)(audio):  0.6743090279651691\n",
      "epoch(20)(text):  0.7276882790442957\n",
      "epoch(20)(fusion):  0.7728262951479035\n",
      "train (21): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (22): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(22)(video):  0.7439811560813203\n",
      "epoch(22)(audio):  0.6747846340385295\n",
      "epoch(22)(text):  0.7272797146728293\n",
      "epoch(22)(fusion):  0.7762283503722575\n",
      "train (23): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (24): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.68it/s]\n",
      "epoch(24)(video):  0.7420077105942109\n",
      "epoch(24)(audio):  0.6757087853300497\n",
      "epoch(24)(text):  0.7264562767404108\n",
      "epoch(24)(fusion):  0.7773843034946512\n",
      "train (25): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (26): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(26)(video):  0.723795945766733\n",
      "epoch(26)(audio):  0.6761534763480076\n",
      "epoch(26)(text):  0.7257256138882798\n",
      "epoch(26)(fusion):  0.7754687457813317\n",
      "train (27): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (28): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(28)(video):  0.7448012076104469\n",
      "epoch(28)(audio):  0.6765914235439522\n",
      "epoch(28)(text):  0.7262806754042925\n",
      "epoch(28)(fusion):  0.7776563505619235\n",
      "train (29): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (30): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(30)(video):  0.7436475197589095\n",
      "epoch(30)(audio):  0.6773924511002861\n",
      "epoch(30)(text):  0.7268763340841028\n",
      "epoch(30)(fusion):  0.7798764069718627\n",
      "train (31): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (32): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(32)(video):  0.7437236811667854\n",
      "epoch(32)(audio):  0.676500386196634\n",
      "epoch(32)(text):  0.7259502512867803\n",
      "epoch(32)(fusion):  0.7814630177476046\n",
      "train (33): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (34): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "epoch(34)(video):  0.7417111678421201\n",
      "epoch(34)(audio):  0.6726678913270788\n",
      "epoch(34)(text):  0.7265230951675474\n",
      "epoch(34)(fusion):  0.7785910930730207\n",
      "train (35): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (36): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(36)(video):  0.7333042692165861\n",
      "epoch(36)(audio):  0.6750203847228229\n",
      "epoch(36)(text):  0.7267693253120661\n",
      "epoch(36)(fusion):  0.7779878364367154\n",
      "train (37): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (38): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(38)(video):  0.737540880481447\n",
      "epoch(38)(audio):  0.675165883798575\n",
      "epoch(38)(text):  0.7262512630329544\n",
      "epoch(38)(fusion):  0.7801943620942057\n",
      "train (39): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (40): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(40)(video):  0.7400649036840813\n",
      "epoch(40)(audio):  0.6752167728740447\n",
      "epoch(40)(text):  0.7275859398294192\n",
      "epoch(40)(fusion):  0.7801549678924113\n",
      "train (41): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (42): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(42)(video):  0.7423969883644785\n",
      "epoch(42)(audio):  0.6771465906747086\n",
      "epoch(42)(text):  0.7261127528807577\n",
      "epoch(42)(fusion):  0.7794708594629087\n",
      "train (43): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (44): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(44)(video):  0.7416090701164532\n",
      "epoch(44)(audio):  0.6712321745286438\n",
      "epoch(44)(text):  0.724400470907011\n",
      "epoch(44)(fusion):  0.7777214411624441\n",
      "train (45): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (46): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(46)(video):  0.7348046453789401\n",
      "epoch(46)(audio):  0.67625823030341\n",
      "epoch(46)(text):  0.7223775955206829\n",
      "epoch(46)(fusion):  0.7759051506693359\n",
      "train (47): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (48): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "epoch(48)(video):  0.7323869607911657\n",
      "epoch(48)(audio):  0.6722165865067051\n",
      "epoch(48)(text):  0.725871709078348\n",
      "epoch(48)(fusion):  0.7772692938673047\n",
      "train (49): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (50): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(50)(video):  0.7419488958163116\n",
      "epoch(50)(audio):  0.6715163333601177\n",
      "epoch(50)(text):  0.7263390063068835\n",
      "epoch(50)(fusion):  0.7782964698838185\n",
      "train (51): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (52): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(52)(video):  0.7311251459197559\n",
      "epoch(52)(audio):  0.6732511642390807\n",
      "epoch(52)(text):  0.725406957961234\n",
      "epoch(52)(fusion):  0.7791317720496866\n",
      "train (53): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (54): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(54)(video):  0.7294979200399258\n",
      "epoch(54)(audio):  0.6730078952825916\n",
      "epoch(54)(text):  0.7263520127464698\n",
      "epoch(54)(fusion):  0.7780633593754281\n",
      "train (55): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (56): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(56)(video):  0.7271220331404484\n",
      "epoch(56)(audio):  0.6719618267150426\n",
      "epoch(56)(text):  0.7239189583448419\n",
      "epoch(56)(fusion):  0.7784380225191001\n",
      "train (57): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (58): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(58)(video):  0.7358100572022739\n",
      "epoch(58)(audio):  0.673940349495208\n",
      "epoch(58)(text):  0.7265032349237255\n",
      "epoch(58)(fusion):  0.777719099790068\n",
      "train (59): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (60): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(60)(video):  0.7314873811910538\n",
      "epoch(60)(audio):  0.6743631475791502\n",
      "epoch(60)(text):  0.7231814877524404\n",
      "epoch(60)(fusion):  0.7777006921986037\n",
      "train (61): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (62): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "epoch(62)(video):  0.7366010753059691\n",
      "epoch(62)(audio):  0.6736759624473044\n",
      "epoch(62)(text):  0.7241297857381654\n",
      "epoch(62)(fusion):  0.7774024576626901\n",
      "train (63): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (64): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(64)(video):  0.7328470229632484\n",
      "epoch(64)(audio):  0.6697744633957211\n",
      "epoch(64)(text):  0.7226836212016619\n",
      "epoch(64)(fusion):  0.7769104984046806\n",
      "train (65): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (66): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(66)(video):  0.7335088898857443\n",
      "epoch(66)(audio):  0.673338648566348\n",
      "epoch(66)(text):  0.7229687803678938\n",
      "epoch(66)(fusion):  0.7756446116126072\n",
      "train (67): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (68): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "epoch(68)(video):  0.7314750129192411\n",
      "epoch(68)(audio):  0.6716962466043664\n",
      "epoch(68)(text):  0.722546851500117\n",
      "epoch(68)(fusion):  0.7767100960749819\n",
      "train (69): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (70): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(70)(video):  0.72616726413008\n",
      "epoch(70)(audio):  0.6713693918852315\n",
      "epoch(70)(text):  0.7245001335532578\n",
      "epoch(70)(fusion):  0.7755506694852582\n",
      "train (71): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (72): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(72)(video):  0.7294367938230315\n",
      "epoch(72)(audio):  0.6708936990124832\n",
      "epoch(72)(text):  0.7221122595810142\n",
      "epoch(72)(fusion):  0.7749120579572336\n",
      "train (73): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (74): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "epoch(74)(video):  0.7262237710697242\n",
      "epoch(74)(audio):  0.6732987349855295\n",
      "epoch(74)(text):  0.7238740033687354\n",
      "epoch(74)(fusion):  0.7765363073354874\n",
      "train (75): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (76): 100%|█████████████████████████████| 141/141 [01:24<00:00,  1.68it/s]\n",
      "epoch(76)(video):  0.7307481180082267\n",
      "epoch(76)(audio):  0.6711732107498197\n",
      "epoch(76)(text):  0.7237386464648639\n",
      "epoch(76)(fusion):  0.7773943538996915\n",
      "train (77): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (78): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(78)(video):  0.7308696387650929\n",
      "epoch(78)(audio):  0.6700671840619686\n",
      "epoch(78)(text):  0.7244799603973733\n",
      "epoch(78)(fusion):  0.7765642689428358\n",
      "train (79): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (80): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(80)(video):  0.719449652723984\n",
      "epoch(80)(audio):  0.6707651927059042\n",
      "epoch(80)(text):  0.7235395928232669\n",
      "epoch(80)(fusion):  0.7743371117428066\n",
      "train (81): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (82): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(82)(video):  0.7281619732833289\n",
      "epoch(82)(audio):  0.6704741427143828\n",
      "epoch(82)(text):  0.723168007402543\n",
      "epoch(82)(fusion):  0.7756721130371982\n",
      "train (83): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (84): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(84)(video):  0.7217268507501633\n",
      "epoch(84)(audio):  0.6709815163219989\n",
      "epoch(84)(text):  0.7208032869738222\n",
      "epoch(84)(fusion):  0.7758232771507706\n",
      "train (85): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (86): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(86)(video):  0.7263057634666867\n",
      "epoch(86)(audio):  0.667769246087238\n",
      "epoch(86)(text):  0.7245824715147339\n",
      "epoch(86)(fusion):  0.773388372851644\n",
      "train (87): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (88): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(88)(video):  0.7231976830209861\n",
      "epoch(88)(audio):  0.6634307456283456\n",
      "epoch(88)(text):  0.7213613594560491\n",
      "epoch(88)(fusion):  0.7730601888308147\n",
      "train (89): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (90): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(90)(video):  0.7217621151258341\n",
      "epoch(90)(audio):  0.6662160398144317\n",
      "epoch(90)(text):  0.7223200009088583\n",
      "epoch(90)(fusion):  0.7732265557120689\n",
      "train (91): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (92): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(92)(video):  0.7174118606224864\n",
      "epoch(92)(audio):  0.6663206262376494\n",
      "epoch(92)(text):  0.7219746156878906\n",
      "epoch(92)(fusion):  0.7727533029795\n",
      "train (93): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (94): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(94)(video):  0.7239782740506473\n",
      "epoch(94)(audio):  0.6674195434851593\n",
      "epoch(94)(text):  0.7217852044346231\n",
      "epoch(94)(fusion):  0.7730330033882259\n",
      "train (95): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (96): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(96)(video):  0.7104932338777554\n",
      "epoch(96)(audio):  0.6663982299351929\n",
      "epoch(96)(text):  0.7230367087953083\n",
      "epoch(96)(fusion):  0.7697696932227208\n",
      "train (97): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (98): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(98)(video):  0.717726326718626\n",
      "epoch(98)(audio):  0.6682744731533162\n",
      "epoch(98)(text):  0.7191298045956719\n",
      "epoch(98)(fusion):  0.7722978617332252\n",
      "train (99): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# 采用CLR\n",
    "!python -W ignore main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "train (0): 100%|██████████████████████████████| 141/141 [01:24<00:00,  1.68it/s]\n",
      "epoch(0)(video):  0.6878608662219391\n",
      "epoch(0)(audio):  0.6304760625488759\n",
      "epoch(0)(text):  0.6662787868098137\n",
      "epoch(0)(fusion):  0.6731877566402384\n",
      "train (1): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (2): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.72it/s]\n",
      "epoch(2)(video):  0.7017086549938456\n",
      "epoch(2)(audio):  0.6563719651082779\n",
      "epoch(2)(text):  0.7026323545128494\n",
      "epoch(2)(fusion):  0.7279400431314754\n",
      "train (3): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (4): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(4)(video):  0.6838730476071748\n",
      "epoch(4)(audio):  0.6588403857542847\n",
      "epoch(4)(text):  0.7068846826035228\n",
      "epoch(4)(fusion):  0.7418831193360024\n",
      "train (5): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (6): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(6)(video):  0.7118417322072728\n",
      "epoch(6)(audio):  0.6642894638675134\n",
      "epoch(6)(text):  0.7187449737522058\n",
      "epoch(6)(fusion):  0.7569651802555988\n",
      "train (7): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (8): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(8)(video):  0.7223775764270901\n",
      "epoch(8)(audio):  0.6660602580663615\n",
      "epoch(8)(text):  0.7231724986340559\n",
      "epoch(8)(fusion):  0.7598117332403141\n",
      "train (9): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (10): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(10)(video):  0.7301413459349297\n",
      "epoch(10)(audio):  0.6672207191482448\n",
      "epoch(10)(text):  0.7267567965849417\n",
      "epoch(10)(fusion):  0.7632910070598526\n",
      "train (11): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (12): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(12)(video):  0.7277715539098323\n",
      "epoch(12)(audio):  0.6712512389896585\n",
      "epoch(12)(text):  0.7268253518270198\n",
      "epoch(12)(fusion):  0.7666297906632469\n",
      "train (13): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (14): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(14)(video):  0.7197655215341724\n",
      "epoch(14)(audio):  0.6716019122412333\n",
      "epoch(14)(text):  0.7231428654840449\n",
      "epoch(14)(fusion):  0.7656711272398496\n",
      "train (15): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (16): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(16)(video):  0.736650924122147\n",
      "epoch(16)(audio):  0.676045668620828\n",
      "epoch(16)(text):  0.7228572615818817\n",
      "epoch(16)(fusion):  0.7760795086012797\n",
      "train (17): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (18): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(18)(video):  0.7434208446710074\n",
      "epoch(18)(audio):  0.6710786020136044\n",
      "epoch(18)(text):  0.7227038095458206\n",
      "epoch(18)(fusion):  0.7714582774551386\n",
      "train (19): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (20): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(20)(video):  0.7578937947756925\n",
      "epoch(20)(audio):  0.6747850667885669\n",
      "epoch(20)(text):  0.7273703845600585\n",
      "epoch(20)(fusion):  0.7797647434789519\n",
      "train (21): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (22): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(22)(video):  0.7604961899899859\n",
      "epoch(22)(audio):  0.6804054902924447\n",
      "epoch(22)(text):  0.7312749152319068\n",
      "epoch(22)(fusion):  0.7828136930090739\n",
      "train (23): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (24): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(24)(video):  0.762258721681528\n",
      "epoch(24)(audio):  0.6791132772328446\n",
      "epoch(24)(text):  0.7295943404649883\n",
      "epoch(24)(fusion):  0.7835531977101705\n",
      "train (25): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (26): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(26)(video):  0.764754136237378\n",
      "epoch(26)(audio):  0.677560216334278\n",
      "epoch(26)(text):  0.7299170171706578\n",
      "epoch(26)(fusion):  0.7853436335967884\n",
      "train (27): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (28): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(28)(video):  0.7623711860590423\n",
      "epoch(28)(audio):  0.6798510456284306\n",
      "epoch(28)(text):  0.7293541243219062\n",
      "epoch(28)(fusion):  0.7856621677959117\n",
      "train (29): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (30): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "epoch(30)(video):  0.7628649567805345\n",
      "epoch(30)(audio):  0.6778661092872954\n",
      "epoch(30)(text):  0.7294016912505357\n",
      "epoch(30)(fusion):  0.7854497975315311\n",
      "train (31): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (32): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(32)(video):  0.7603527715006295\n",
      "epoch(32)(audio):  0.6795995448617083\n",
      "epoch(32)(text):  0.729652900710862\n",
      "epoch(32)(fusion):  0.786707556723348\n",
      "train (33): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (34): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(34)(video):  0.759682462549174\n",
      "epoch(34)(audio):  0.6786802877567588\n",
      "epoch(34)(text):  0.7295815200774511\n",
      "epoch(34)(fusion):  0.7859579909447061\n",
      "train (35): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (36): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(36)(video):  0.7613215043528138\n",
      "epoch(36)(audio):  0.6796224455028573\n",
      "epoch(36)(text):  0.7295267459705359\n",
      "epoch(36)(fusion):  0.7862216415908131\n",
      "train (37): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (38): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(38)(video):  0.7591001996248429\n",
      "epoch(38)(audio):  0.6782711588021749\n",
      "epoch(38)(text):  0.7282585623969937\n",
      "epoch(38)(fusion):  0.7850072853812499\n",
      "train (39): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (40): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(40)(video):  0.7607332329286569\n",
      "epoch(40)(audio):  0.6788363383573917\n",
      "epoch(40)(text):  0.7276394863132745\n",
      "epoch(40)(fusion):  0.7857545743423274\n",
      "train (41): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (42): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(42)(video):  0.7609279870936684\n",
      "epoch(42)(audio):  0.6793365136896462\n",
      "epoch(42)(text):  0.7274038354773047\n",
      "epoch(42)(fusion):  0.7851502139835819\n",
      "train (43): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (44): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(44)(video):  0.7619971111448832\n",
      "epoch(44)(audio):  0.6778749036299508\n",
      "epoch(44)(text):  0.7286450028502345\n",
      "epoch(44)(fusion):  0.7858211887015282\n",
      "train (45): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (46): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(46)(video):  0.762136482701318\n",
      "epoch(46)(audio):  0.6779706300566164\n",
      "epoch(46)(text):  0.7285263131231453\n",
      "epoch(46)(fusion):  0.7868709148904194\n",
      "train (47): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (48): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(48)(video):  0.7619900522754769\n",
      "epoch(48)(audio):  0.679359829349248\n",
      "epoch(48)(text):  0.7293010074419399\n",
      "epoch(48)(fusion):  0.7850969473851727\n",
      "train (49): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (50): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(50)(video):  0.7599534240874177\n",
      "epoch(50)(audio):  0.6785062683395652\n",
      "epoch(50)(text):  0.7282066726971231\n",
      "epoch(50)(fusion):  0.7848399656893426\n",
      "train (51): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (52):  38%|███████████▎                  | 53/141 [00:40<00:41,  2.13it/s]"
     ]
    }
   ],
   "source": [
    "# early fusion 去掉audio\n",
    "!python -W ignore main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python -W ignore ensemble_inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -W ignore end2end_main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "train (0): 100%|██████████████████████████████| 157/157 [04:32<00:00,  1.74s/it]\n",
      "6.818717795572463\n",
      "train (1): 100%|██████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "5.22633198719875\n",
      "train (2): 100%|██████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "4.040535131077857\n",
      "train (3): 100%|██████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "3.497894019837592\n",
      "train (4): 100%|██████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "3.135594062744432\n",
      "train (5): 100%|██████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "2.949744964101512\n",
      "train (6): 100%|██████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "2.7477826365999354\n",
      "train (7): 100%|██████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "2.6255849037960077\n",
      "train (8): 100%|██████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "2.452668469422942\n",
      "train (9): 100%|██████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "2.364424360785515\n",
      "train (10): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "2.331585179468629\n",
      "train (11): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "2.233017121150995\n",
      "train (12): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "2.254295363547696\n",
      "train (13): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "2.244896477195108\n",
      "train (14): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "2.1817877717838168\n",
      "train (15): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "2.085770916027628\n",
      "train (16): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "2.050969423761793\n",
      "train (17): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "1.9404507731176486\n",
      "train (18): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "1.9753528954876456\n",
      "train (19): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "1.8860492015340526\n",
      "train (20): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "1.9609372008378338\n",
      "train (21): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "1.9508991681846084\n",
      "train (22): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "1.8973085022276375\n",
      "train (23): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "1.8941494647864323\n",
      "train (24): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "1.8566543342201574\n",
      "train (25): 100%|█████████████████████████████| 157/157 [04:14<00:00,  1.62s/it]\n",
      "1.8415338461566124\n",
      "train (26): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "1.7684999681582116\n",
      "train (27): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "1.7981826549123048\n",
      "train (28): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "1.7722153652245831\n",
      "train (29): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "1.7334051189149262\n",
      "train (30): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "1.8041696669949088\n",
      "train (31): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "1.7277859684767995\n",
      "train (32): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "1.7425518688882233\n",
      "train (33): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "1.7142959909074624\n",
      "train (34): 100%|█████████████████████████████| 157/157 [04:14<00:00,  1.62s/it]\n",
      "1.7294749374602252\n",
      "train (35): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "1.6819196024518104\n",
      "train (36): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "1.7443310364036804\n",
      "train (37): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "1.6962386012836626\n",
      "train (38): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "1.6548183955204714\n",
      "train (39): 100%|█████████████████████████████| 157/157 [04:14<00:00,  1.62s/it]\n",
      "1.6770283363427325\n",
      "train (40): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "1.6824505701186552\n",
      "train (41): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "1.6329042414191421\n",
      "train (42): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "1.652237662084543\n",
      "train (43): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "1.6677449356978107\n",
      "train (44): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "1.63304131236046\n",
      "train (45): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "1.645081889477505\n",
      "train (46): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "1.6461388563654225\n",
      "train (47): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "1.6161704632886655\n",
      "train (48): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.61s/it]\n",
      "1.6360029824979745\n",
      "train (49): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "1.63800142525108\n",
      "train (50): 100%|█████████████████████████████| 157/157 [04:13<00:00,  1.62s/it]\n",
      "1.6194096211415188\n"
     ]
    }
   ],
   "source": [
    "# without projector\n",
    "!python -W ignore enhance_main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "train (0): 100%|██████████████████████████████| 141/141 [02:12<00:00,  1.07it/s]\n",
      "epoch(0)(video):  0.689400477257959\n",
      "epoch(0)(audio):  0.6339316603997511\n",
      "epoch(0)(text):  0.6678973810421356\n",
      "epoch(0)(fusion):  0.6766956817360982\n",
      "train (1): 100%|██████████████████████████████| 141/141 [02:11<00:00,  1.07it/s]\n",
      "train (2): 100%|██████████████████████████████| 141/141 [02:11<00:00,  1.07it/s]\n",
      "epoch(2)(video):  0.6968389461750143\n",
      "epoch(2)(audio):  0.6606365134970522\n",
      "epoch(2)(text):  0.7057502537402386\n",
      "epoch(2)(fusion):  0.7250427182789762\n",
      "train (3): 100%|██████████████████████████████| 141/141 [02:11<00:00,  1.07it/s]\n",
      "train (4): 100%|██████████████████████████████| 141/141 [02:11<00:00,  1.07it/s]\n",
      "epoch(4)(video):  0.6961784256149993\n",
      "epoch(4)(audio):  0.6580860672488744\n",
      "epoch(4)(text):  0.7093778266631535\n",
      "epoch(4)(fusion):  0.7418929848549519\n",
      "train (5): 100%|██████████████████████████████| 141/141 [02:12<00:00,  1.06it/s]\n",
      "train (6): 100%|██████████████████████████████| 141/141 [02:12<00:00,  1.06it/s]\n",
      "epoch(6)(video):  0.7022707874917484\n",
      "epoch(6)(audio):  0.6605284397835461\n",
      "epoch(6)(text):  0.7182722264803103\n",
      "epoch(6)(fusion):  0.7579290084604993\n",
      "train (7): 100%|██████████████████████████████| 141/141 [02:12<00:00,  1.06it/s]\n",
      "train (8): 100%|██████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(8)(video):  0.70190920386627\n",
      "epoch(8)(audio):  0.6636423998271407\n",
      "epoch(8)(text):  0.7199867299911129\n",
      "epoch(8)(fusion):  0.7561586857753302\n",
      "train (9): 100%|██████████████████████████████| 141/141 [02:12<00:00,  1.06it/s]\n",
      "train (10): 100%|█████████████████████████████| 141/141 [02:12<00:00,  1.06it/s]\n",
      "epoch(10)(video):  0.7257764462070049\n",
      "epoch(10)(audio):  0.6709497621003391\n",
      "epoch(10)(text):  0.7274730832549835\n",
      "epoch(10)(fusion):  0.7681802397717719\n",
      "train (11): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (12): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(12)(video):  0.7346731454585491\n",
      "epoch(12)(audio):  0.6691556608779082\n",
      "epoch(12)(text):  0.7208478477219165\n",
      "epoch(12)(fusion):  0.7698747176599716\n",
      "train (13): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "train (14): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(14)(video):  0.7087182506561284\n",
      "epoch(14)(audio):  0.6740238586208948\n",
      "epoch(14)(text):  0.7202343371202176\n",
      "epoch(14)(fusion):  0.7668128525030643\n",
      "train (15): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "train (16): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "epoch(16)(video):  0.7020293444042501\n",
      "epoch(16)(audio):  0.6755947627082787\n",
      "epoch(16)(text):  0.7288322451574287\n",
      "epoch(16)(fusion):  0.7690757045844491\n",
      "train (17): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "train (18): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "epoch(18)(video):  0.7239008181643853\n",
      "epoch(18)(audio):  0.6747885755847159\n",
      "epoch(18)(text):  0.722984408617855\n",
      "epoch(18)(fusion):  0.7696998786437077\n",
      "train (19): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "train (20): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "epoch(20)(video):  0.7586680981665597\n",
      "epoch(20)(audio):  0.680402718572449\n",
      "epoch(20)(text):  0.7331172879136169\n",
      "epoch(20)(fusion):  0.7834812638524588\n",
      "train (21): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "train (22): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "epoch(22)(video):  0.7601151846289459\n",
      "epoch(22)(audio):  0.6814291060631343\n",
      "epoch(22)(text):  0.7334563350719074\n",
      "epoch(22)(fusion):  0.7856752380530629\n",
      "train (23): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "train (24): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "epoch(24)(video):  0.761589000847235\n",
      "epoch(24)(audio):  0.6811751184630861\n",
      "epoch(24)(text):  0.7305990479896342\n",
      "epoch(24)(fusion):  0.7844794835606523\n",
      "train (25): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "train (26): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "epoch(26)(video):  0.7591563896539354\n",
      "epoch(26)(audio):  0.6800636069051719\n",
      "epoch(26)(text):  0.729761265720847\n",
      "epoch(26)(fusion):  0.7866107443208684\n",
      "train (27): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "train (28): 100%|█████████████████████████████| 141/141 [02:12<00:00,  1.06it/s]\n",
      "epoch(28)(video):  0.757965599608149\n",
      "epoch(28)(audio):  0.6835610850005447\n",
      "epoch(28)(text):  0.7305230358093162\n",
      "epoch(28)(fusion):  0.7867650425660172\n",
      "train (29): 100%|█████████████████████████████| 141/141 [02:12<00:00,  1.06it/s]\n",
      "train (30): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(30)(video):  0.7607644496189362\n",
      "epoch(30)(audio):  0.6828058436436684\n",
      "epoch(30)(text):  0.7306694325422493\n",
      "epoch(30)(fusion):  0.7876028349529263\n",
      "train (31): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (32): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(32)(video):  0.7621671422976916\n",
      "epoch(32)(audio):  0.6830738348285099\n",
      "epoch(32)(text):  0.7289229083286696\n",
      "epoch(32)(fusion):  0.7861773628705564\n",
      "train (33): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (34): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(34)(video):  0.760170857880535\n",
      "epoch(34)(audio):  0.6797751286874796\n",
      "epoch(34)(text):  0.7280163655431514\n",
      "epoch(34)(fusion):  0.7872834612703499\n",
      "train (35): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (36): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(36)(video):  0.7625826738287443\n",
      "epoch(36)(audio):  0.6819867094256037\n",
      "epoch(36)(text):  0.7286398015694275\n",
      "epoch(36)(fusion):  0.7862490481911302\n",
      "train (37): 100%|█████████████████████████████| 141/141 [02:12<00:00,  1.06it/s]\n",
      "train (38): 100%|█████████████████████████████| 141/141 [02:12<00:00,  1.06it/s]\n",
      "epoch(38)(video):  0.7583984701989214\n",
      "epoch(38)(audio):  0.6817595594733463\n",
      "epoch(38)(text):  0.7271650662366111\n",
      "epoch(38)(fusion):  0.785566949355874\n",
      "train (39): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "train (40): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(40)(video):  0.762345095041433\n",
      "epoch(40)(audio):  0.6816726901245905\n",
      "epoch(40)(text):  0.727754156009576\n",
      "epoch(40)(fusion):  0.7869281352300206\n",
      "train (41): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (42): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(42)(video):  0.7634704816589063\n",
      "epoch(42)(audio):  0.6821950028219714\n",
      "epoch(42)(text):  0.7288979570706675\n",
      "epoch(42)(fusion):  0.7869674552217377\n",
      "train (43): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (44): 100%|█████████████████████████████| 141/141 [02:12<00:00,  1.06it/s]\n",
      "epoch(44)(video):  0.7622445459684247\n",
      "epoch(44)(audio):  0.6813492491988826\n",
      "epoch(44)(text):  0.7295534013990324\n",
      "epoch(44)(fusion):  0.7869517658085038\n",
      "train (45): 100%|█████████████████████████████| 141/141 [02:12<00:00,  1.06it/s]\n",
      "train (46): 100%|█████████████████████████████| 141/141 [02:12<00:00,  1.06it/s]\n",
      "epoch(46)(video):  0.760180483800907\n",
      "epoch(46)(audio):  0.6797109355713719\n",
      "epoch(46)(text):  0.7270037778367858\n",
      "epoch(46)(fusion):  0.7861259280137862\n",
      "train (47):   0%|                                       | 0/141 [00:00<?, ?it/s]^C\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/spawn.py\", line 105, in spawn_main\n",
      "    exitcode = _main(fd)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/spawn.py\", line 114, in _main\n",
      "    prepare(preparation_data)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/spawn.py\", line 225, in prepare\n",
      "    _fixup_main_from_path(data['init_main_from_path'])\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/spawn.py\", line 277, in _fixup_main_from_path\n",
      "    run_name=\"__mp_main__\")\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/runpy.py\", line 263, in run_path\n",
      "    pkg_name=pkg_name, script_name=fname)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/runpy.py\", line 96, in _run_module_code\n",
      "    mod_name, mod_spec, pkg_name, script_name)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/tione/notebook/TAAC-2021/main.py\", line 10, in <module>\n",
      "    from dataloader.dataloader import MultimodaFeaturesDataset,Datasetfortextcnn\n",
      "  File \"/home/tione/notebook/TAAC-2021/dataloader/dataloader.py\", line 13, in <module>\n",
      "    import albumentations as A\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/albumentations/__init__.py\", line 5, in <module>\n",
      "    from .core.composition import *\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/albumentations/core/composition.py\", line 8, in <module>\n",
      "    from albumentations.augmentations.keypoints_utils import KeypointsProcessor\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/albumentations/augmentations/__init__.py\", line 5, in <module>\n",
      "    from .transforms import *\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/albumentations/augmentations/transforms.py\", line 13, in <module>\n",
      "    from skimage.measure import label\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/skimage/measure/__init__.py\", line 8, in <module>\n",
      "    from ._polygon import approximate_polygon, subdivide_polygon\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/skimage/measure/_polygon.py\", line 2, in <module>\n",
      "    from scipy import signal\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/scipy/signal/__init__.py\", line 289, in <module>\n",
      "    from . import sigtools, windows\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/scipy/signal/windows/__init__.py\", line 41, in <module>\n",
      "    from .windows import *\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/scipy/signal/windows/windows.py\", line 7, in <module>\n",
      "    from scipy import linalg, special, fft as sp_fft\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/scipy/linalg/__init__.py\", line 194, in <module>\n",
      "    from .misc import *\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 674, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 758, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 842, in path_stats\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 82, in _path_stat\n",
      "KeyboardInterrupt\n",
      "train (47):   0%|                                       | 0/141 [00:09<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 121, in <module>\n",
      "    loss = training_loop(model, train_loader, loss_compute, modal_name_list,train_dataset.device, epoch,TBoard)\n",
      "  File \"/home/tione/notebook/TAAC-2021/src/loop/run_epoch.py\", line 15, in training_loop\n",
      "    for i, batch in enumerate(tqdm(loader, desc=f'train ({epoch})')):\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/tqdm/std.py\", line 1129, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 279, in __iter__\n",
      "    return _MultiProcessingDataLoaderIter(self)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 719, in __init__\n",
      "    w.start()\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/process.py\", line 105, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/context.py\", line 223, in _Popen\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/context.py\", line 284, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n",
      "    super().__init__(process_obj)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 62, in _launch\n",
      "    f.write(fp.getbuffer())\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# 使用eff特征，自监督 rd = 2023\n",
    "!python -W ignore main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n"
     ]
    }
   ],
   "source": [
    "!python -W ignore inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/ipykernel/__main__.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization: Kaiming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization: Kaiming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization: Kaiming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization: Kaiming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization: Kaiming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# ensemble 模型在验证集上测试\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5,6,7'\n",
    "import yaml\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import utils.train_util as train_util\n",
    "from dataloader.dataloader import TestingDataset\n",
    "from src.loss.loss_compute import SimpleLossCompute\n",
    "from src.model.baseline_model import Baseline\n",
    "from src.loop.run_epoch import training_loop,validating_loop\n",
    "from dataloader.dataloader import MultimodaFeaturesDataset,Datasetfortextcnn\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 8\n",
    "modal_name_list = ['video','audio','text']\n",
    "config_path = './config/config.yaml'\n",
    "config = yaml.load(open(config_path))\n",
    "dataset = MultimodaFeaturesDataset(config['DatasetConfig'],job='valdation')\n",
    "loader = DataLoader(dataset,num_workers=8,\n",
    "                    batch_size=batch_size,\n",
    "                    pin_memory=False,\n",
    "                    collate_fn=dataset.collate_fn)\n",
    "\n",
    "model_path_1 = '../checkpoint/0609/01/epoch_48 0.7888.pt'\n",
    "model_path_2 = '../checkpoint/0609/01/epoch_30 0.7886.pt'\n",
    "model_path_3 = '../checkpoint/0609/01/epoch_24 0.7871.pt'\n",
    "\n",
    "model_path_4 = '../checkpoint/0609/02/epoch_46 0.7868.pt'\n",
    "model_path_5 = '../checkpoint/0609/02/epoch_28 0.7856.pt'\n",
    "\n",
    "model_path_6 = '../checkpoint/0608/01/epoch_86 0.7877.pt'\n",
    "model_path_7 = '../checkpoint/0608/01/epoch_50 0.7873.pt'\n",
    "model_path_8 = '../checkpoint/0608/01/epoch_28 0.7869.pt'\n",
    "\n",
    "model_path_9 = '../checkpoint/0608/03/epoch_28 0.7877.pt'\n",
    "model_path_10 = '../checkpoint/0608/03/epoch_52 0.7890.pt'\n",
    "model_path_11 = '../checkpoint/0608/03/epoch_74 0.7896.pt'\n",
    "models_path = [model_path_1,\n",
    "               model_path_4,\n",
    "               model_path_6,\n",
    "               model_path_10,model_path_11]\n",
    "model_weights = [0.2,0.1,0.2,0.25,0.25] #0.791\n",
    "# model_weights = [0.1,0.1,0.1,0.35,0.35] # 0.789\n",
    "# model_weights = [0.2,0.2,0.2,0.2,0.2] # 0.7911\n",
    "#model_weights = np.array(np.random.random(11))\n",
    "#model_weights = model_weights/sum(model_weights)\n",
    "device = 'cuda'\n",
    "top_k=20\n",
    "# output_json = './0604_resnet_ensemble.json'\n",
    "models = []\n",
    "for path in models_path:\n",
    "    if(path.split('/')[2]+path.split('/')[3]=='060801'):\n",
    "        config['ModelConfig']['fusion_head_params']['concat_feat_dim']['fusion'] = 30720\n",
    "        config['ModelConfig']['audio_head_params']['max_frames'] = 300\n",
    "    else:\n",
    "        config['ModelConfig']['fusion_head_params']['concat_feat_dim']['fusion'] = 29696\n",
    "        config['ModelConfig']['audio_head_params']['max_frames'] = 200\n",
    "    model = Baseline(config['ModelConfig'])\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "tagging_class_num = 82\n",
    "evl_metrics = [train_util.EvaluationMetrics(tagging_class_num, top_k=20)\n",
    "                           for i in range(len(modal_name_list)+1)] #+1 for fusion\n",
    "for i in range(len(evl_metrics)):\n",
    "    evl_metrics[i].clear()\n",
    "metric_dict = {}\n",
    "gap_dict = {}\n",
    "with torch.no_grad():\n",
    "    for i,batch in tqdm(enumerate(loader)):\n",
    "        if(len(batch)==5):\n",
    "            video,audio,text,text_mask,label = batch\n",
    "            video = video.to(device)\n",
    "            audio = audio.to(device)\n",
    "            text = text.to(device)\n",
    "            text_mask = text_mask.to(device)\n",
    "            label = label.to(device)\n",
    "        else:\n",
    "            video,audio,text,label = batch\n",
    "            video = video.to(device)\n",
    "            audio = audio.to(device)\n",
    "            text = text.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "        inputs_dict={}\n",
    "        inputs_dict['video'] = video\n",
    "        inputs_dict['audio'] = audio\n",
    "        inputs_dict['text'] = text \n",
    "        if(len(batch)==5):\n",
    "            inputs_dict['attention_mask'] = text_mask\n",
    "        else:\n",
    "            inputs_dict['attention_mask'] = None\n",
    "\n",
    "        B = video.shape[0]\n",
    "        pred_dict_ensemble = {}\n",
    "        for modal_name in (modal_name_list+['fusion']):\n",
    "            pred_dict_ensemble['tagging_output_'+modal_name] = {}\n",
    "            pred_dict_ensemble['tagging_output_'+modal_name]['predictions'] = torch.zeros(B,82).cuda()\n",
    "\n",
    "        for i,model in enumerate(models):\n",
    "            pred_dict = model(inputs_dict)\n",
    "            for modal_name in (modal_name_list+['fusion']):\n",
    "                pred_dict_ensemble['tagging_output_'+modal_name]['predictions'] += model_weights[i]*pred_dict['tagging_output_'+modal_name]['predictions']\n",
    "        '''\n",
    "        for modal_name in (modal_name_list+['fusion']):\n",
    "            pred_dict_ensemble['tagging_output_'+modal_name]['predictions'] = pred_dict_ensemble['tagging_output_'+modal_name]['predictions']/len(models)\n",
    "        '''\n",
    "        for index,modal_name in enumerate(modal_name_list+['fusion']):\n",
    "            pred = pred_dict_ensemble['tagging_output_'+modal_name]\n",
    "            pred = pred['predictions'].detach().cpu().numpy()\n",
    "            val_label = label.cpu().numpy()\n",
    "            gap = train_util.calculate_gap(pred, val_label)\n",
    "            evl_metrics[index].accumulate(pred, val_label, loss=0)\n",
    "    for index,modal_name in enumerate(modal_name_list+['fusion']):\n",
    "        metric_dict[modal_name] = evl_metrics[index].get()\n",
    "        gap_dict[modal_name] = metric_dict[modal_name]['gap']\n",
    "    print(gap_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -W ignore main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linecache\n",
    "data_num_per_sample = 6\n",
    "train_file = '../dataset/tagging/GroundTruth/datafile/train_resnet.txt'\n",
    "val_file = '../dataset/tagging/GroundTruth/datafile/val_resnet.txt'\n",
    "train_full_file = '../dataset/tagging/GroundTruth/datafile/train_full_resnet.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_full_file,'w') as f:\n",
    "    for index in range(4500):\n",
    "        data_list = []\n",
    "        for line_i in range(data_num_per_sample*index+1,data_num_per_sample*(index+1)):\n",
    "            line = linecache.getline(train_file,line_i)\n",
    "            # line = line.strip('\\r\\n')\n",
    "            data_list.append(line)\n",
    "        for line in data_list:\n",
    "            f.write(line)\n",
    "        f.write('\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_full_file,'a') as f:\n",
    "    for index in range(500):\n",
    "        data_list = []\n",
    "        for line_i in range(data_num_per_sample*index+1,data_num_per_sample*(index+1)):\n",
    "            line = linecache.getline(val_file,line_i)\n",
    "            # line = line.strip('\\r\\n')\n",
    "            data_list.append(line)\n",
    "        for line in data_list:\n",
    "            f.write(line)\n",
    "        f.write('\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "m = torch.load('../checkpoint/0607/enhance_VT/30_wp.pt',map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fusion_head_dict.video.hidden1_weights torch.Size([28672, 1024])\n",
      "fusion_head_dict.video.gating_weights_1 torch.Size([1024, 128])\n",
      "fusion_head_dict.video.gating_weights_2 torch.Size([128, 1024])\n",
      "fusion_head_dict.video.bn_1.weight torch.Size([1024])\n",
      "fusion_head_dict.video.bn_1.bias torch.Size([1024])\n",
      "fusion_head_dict.video.bn_1.running_mean torch.Size([1024])\n",
      "fusion_head_dict.video.bn_1.running_var torch.Size([1024])\n",
      "fusion_head_dict.video.bn_1.num_batches_tracked torch.Size([])\n",
      "fusion_head_dict.video.bn_2.weight torch.Size([128])\n",
      "fusion_head_dict.video.bn_2.bias torch.Size([128])\n",
      "fusion_head_dict.video.bn_2.running_mean torch.Size([128])\n",
      "fusion_head_dict.video.bn_2.running_var torch.Size([128])\n",
      "fusion_head_dict.video.bn_2.num_batches_tracked torch.Size([])\n",
      "fusion_head_dict.audio.hidden1_weights torch.Size([1024, 1024])\n",
      "fusion_head_dict.audio.gating_weights_1 torch.Size([1024, 128])\n",
      "fusion_head_dict.audio.gating_weights_2 torch.Size([128, 1024])\n",
      "fusion_head_dict.audio.bn_1.weight torch.Size([1024])\n",
      "fusion_head_dict.audio.bn_1.bias torch.Size([1024])\n",
      "fusion_head_dict.audio.bn_1.running_mean torch.Size([1024])\n",
      "fusion_head_dict.audio.bn_1.running_var torch.Size([1024])\n",
      "fusion_head_dict.audio.bn_1.num_batches_tracked torch.Size([])\n",
      "fusion_head_dict.audio.bn_2.weight torch.Size([128])\n",
      "fusion_head_dict.audio.bn_2.bias torch.Size([128])\n",
      "fusion_head_dict.audio.bn_2.running_mean torch.Size([128])\n",
      "fusion_head_dict.audio.bn_2.running_var torch.Size([128])\n",
      "fusion_head_dict.audio.bn_2.num_batches_tracked torch.Size([])\n",
      "fusion_head_dict.text.hidden1_weights torch.Size([1024, 1024])\n",
      "fusion_head_dict.text.gating_weights_1 torch.Size([1024, 128])\n",
      "fusion_head_dict.text.gating_weights_2 torch.Size([128, 1024])\n",
      "fusion_head_dict.text.bn_1.weight torch.Size([1024])\n",
      "fusion_head_dict.text.bn_1.bias torch.Size([1024])\n",
      "fusion_head_dict.text.bn_1.running_mean torch.Size([1024])\n",
      "fusion_head_dict.text.bn_1.running_var torch.Size([1024])\n",
      "fusion_head_dict.text.bn_1.num_batches_tracked torch.Size([])\n",
      "fusion_head_dict.text.bn_2.weight torch.Size([128])\n",
      "fusion_head_dict.text.bn_2.bias torch.Size([128])\n",
      "fusion_head_dict.text.bn_2.running_mean torch.Size([128])\n",
      "fusion_head_dict.text.bn_2.running_var torch.Size([128])\n",
      "fusion_head_dict.text.bn_2.num_batches_tracked torch.Size([])\n",
      "fusion_head_dict.fusion.hidden1_weights torch.Size([30720, 1024])\n",
      "fusion_head_dict.fusion.gating_weights_1 torch.Size([1024, 128])\n",
      "fusion_head_dict.fusion.gating_weights_2 torch.Size([128, 1024])\n",
      "fusion_head_dict.fusion.bn_1.weight torch.Size([1024])\n",
      "fusion_head_dict.fusion.bn_1.bias torch.Size([1024])\n",
      "fusion_head_dict.fusion.bn_1.running_mean torch.Size([1024])\n",
      "fusion_head_dict.fusion.bn_1.running_var torch.Size([1024])\n",
      "fusion_head_dict.fusion.bn_1.num_batches_tracked torch.Size([])\n",
      "fusion_head_dict.fusion.bn_2.weight torch.Size([128])\n",
      "fusion_head_dict.fusion.bn_2.bias torch.Size([128])\n",
      "fusion_head_dict.fusion.bn_2.running_mean torch.Size([128])\n",
      "fusion_head_dict.fusion.bn_2.running_var torch.Size([128])\n",
      "fusion_head_dict.fusion.bn_2.num_batches_tracked torch.Size([])\n",
      "classifier_dict.video.linear_1.weight torch.Size([1024, 1024])\n",
      "classifier_dict.video.linear_1.bias torch.Size([1024])\n",
      "classifier_dict.video.linear_2.weight torch.Size([82, 1024])\n",
      "classifier_dict.video.linear_2.bias torch.Size([82])\n",
      "classifier_dict.audio.linear_1.weight torch.Size([1024, 1024])\n",
      "classifier_dict.audio.linear_1.bias torch.Size([1024])\n",
      "classifier_dict.audio.linear_2.weight torch.Size([82, 1024])\n",
      "classifier_dict.audio.linear_2.bias torch.Size([82])\n",
      "classifier_dict.text.linear_1.weight torch.Size([1024, 1024])\n",
      "classifier_dict.text.linear_1.bias torch.Size([1024])\n",
      "classifier_dict.text.linear_2.weight torch.Size([82, 1024])\n",
      "classifier_dict.text.linear_2.bias torch.Size([82])\n",
      "classifier_dict.fusion.linear_1.weight torch.Size([1024, 1024])\n",
      "classifier_dict.fusion.linear_1.bias torch.Size([1024])\n",
      "classifier_dict.fusion.linear_2.weight torch.Size([82, 1024])\n",
      "classifier_dict.fusion.linear_2.bias torch.Size([82])\n",
      "head_dict.video.cluster_weights torch.Size([3584, 2048])\n",
      "head_dict.video.cluster_weights_2 torch.Size([1, 224, 128])\n",
      "head_dict.video.linear_1.weight torch.Size([3584, 1792])\n",
      "head_dict.video.linear_1.bias torch.Size([3584])\n",
      "head_dict.video.attention_1.weight torch.Size([16, 3584])\n",
      "head_dict.video.attention_1.bias torch.Size([16])\n",
      "head_dict.video.bn_1.weight torch.Size([2048])\n",
      "head_dict.video.bn_1.bias torch.Size([2048])\n",
      "head_dict.video.bn_1.running_mean torch.Size([2048])\n",
      "head_dict.video.bn_1.running_var torch.Size([2048])\n",
      "head_dict.video.bn_1.num_batches_tracked torch.Size([])\n",
      "head_dict.video.bn_2.weight torch.Size([28672])\n",
      "head_dict.video.bn_2.bias torch.Size([28672])\n",
      "head_dict.video.bn_2.running_mean torch.Size([28672])\n",
      "head_dict.video.bn_2.running_var torch.Size([28672])\n",
      "head_dict.video.bn_2.num_batches_tracked torch.Size([])\n",
      "head_dict.audio.cluster_weights torch.Size([256, 1024])\n",
      "head_dict.audio.cluster_weights_2 torch.Size([1, 16, 64])\n",
      "head_dict.audio.linear_1.weight torch.Size([256, 128])\n",
      "head_dict.audio.linear_1.bias torch.Size([256])\n",
      "head_dict.audio.attention_1.weight torch.Size([16, 256])\n",
      "head_dict.audio.attention_1.bias torch.Size([16])\n",
      "head_dict.audio.bn_1.weight torch.Size([1024])\n",
      "head_dict.audio.bn_1.bias torch.Size([1024])\n",
      "head_dict.audio.bn_1.running_mean torch.Size([1024])\n",
      "head_dict.audio.bn_1.running_var torch.Size([1024])\n",
      "head_dict.audio.bn_1.num_batches_tracked torch.Size([])\n",
      "head_dict.audio.bn_2.weight torch.Size([1024])\n",
      "head_dict.audio.bn_2.bias torch.Size([1024])\n",
      "head_dict.audio.bn_2.running_mean torch.Size([1024])\n",
      "head_dict.audio.bn_2.running_var torch.Size([1024])\n",
      "head_dict.audio.bn_2.num_batches_tracked torch.Size([])\n",
      "head_dict.text.model.embeddings.position_ids torch.Size([1, 512])\n",
      "head_dict.text.model.embeddings.word_embeddings.weight torch.Size([21128, 768])\n",
      "head_dict.text.model.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "head_dict.text.model.embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "head_dict.text.model.embeddings.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.embeddings.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.pooler.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.pooler.dense.bias torch.Size([768])\n",
      "head_dict.text.linear.weight torch.Size([1024, 768])\n",
      "head_dict.text.linear.bias torch.Size([1024])\n",
      "head_dict.text.bn.weight torch.Size([1024])\n",
      "head_dict.text.bn.bias torch.Size([1024])\n",
      "head_dict.text.bn.running_mean torch.Size([1024])\n",
      "head_dict.text.bn.running_var torch.Size([1024])\n",
      "head_dict.text.bn.num_batches_tracked torch.Size([])\n",
      "projector_text.0.weight torch.Size([2048, 1024])\n",
      "projector_text.0.bias torch.Size([2048])\n",
      "projector_text.1.weight torch.Size([2048])\n",
      "projector_text.1.bias torch.Size([2048])\n",
      "projector_text.1.running_mean torch.Size([2048])\n",
      "projector_text.1.running_var torch.Size([2048])\n",
      "projector_text.1.num_batches_tracked torch.Size([])\n",
      "projector_text.3.weight torch.Size([28672, 2048])\n",
      "projector_text.3.bias torch.Size([28672])\n"
     ]
    }
   ],
   "source": [
    "state_dict = {}\n",
    "for k,v in m.items():\n",
    "    state_dict[k] = v.shape\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "c = torch.load('../checkpoint/0608/enhance_VT_lr/50_wp.pt',map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fusion_head_dict.video.hidden1_weights torch.Size([28672, 1024])\n",
      "fusion_head_dict.video.gating_weights_1 torch.Size([1024, 128])\n",
      "fusion_head_dict.video.gating_weights_2 torch.Size([128, 1024])\n",
      "fusion_head_dict.video.bn_1.weight torch.Size([1024])\n",
      "fusion_head_dict.video.bn_1.bias torch.Size([1024])\n",
      "fusion_head_dict.video.bn_1.running_mean torch.Size([1024])\n",
      "fusion_head_dict.video.bn_1.running_var torch.Size([1024])\n",
      "fusion_head_dict.video.bn_1.num_batches_tracked torch.Size([])\n",
      "fusion_head_dict.video.bn_2.weight torch.Size([128])\n",
      "fusion_head_dict.video.bn_2.bias torch.Size([128])\n",
      "fusion_head_dict.video.bn_2.running_mean torch.Size([128])\n",
      "fusion_head_dict.video.bn_2.running_var torch.Size([128])\n",
      "fusion_head_dict.video.bn_2.num_batches_tracked torch.Size([])\n",
      "fusion_head_dict.audio.hidden1_weights torch.Size([1024, 1024])\n",
      "fusion_head_dict.audio.gating_weights_1 torch.Size([1024, 128])\n",
      "fusion_head_dict.audio.gating_weights_2 torch.Size([128, 1024])\n",
      "fusion_head_dict.audio.bn_1.weight torch.Size([1024])\n",
      "fusion_head_dict.audio.bn_1.bias torch.Size([1024])\n",
      "fusion_head_dict.audio.bn_1.running_mean torch.Size([1024])\n",
      "fusion_head_dict.audio.bn_1.running_var torch.Size([1024])\n",
      "fusion_head_dict.audio.bn_1.num_batches_tracked torch.Size([])\n",
      "fusion_head_dict.audio.bn_2.weight torch.Size([128])\n",
      "fusion_head_dict.audio.bn_2.bias torch.Size([128])\n",
      "fusion_head_dict.audio.bn_2.running_mean torch.Size([128])\n",
      "fusion_head_dict.audio.bn_2.running_var torch.Size([128])\n",
      "fusion_head_dict.audio.bn_2.num_batches_tracked torch.Size([])\n",
      "fusion_head_dict.text.hidden1_weights torch.Size([1024, 1024])\n",
      "fusion_head_dict.text.gating_weights_1 torch.Size([1024, 128])\n",
      "fusion_head_dict.text.gating_weights_2 torch.Size([128, 1024])\n",
      "fusion_head_dict.text.bn_1.weight torch.Size([1024])\n",
      "fusion_head_dict.text.bn_1.bias torch.Size([1024])\n",
      "fusion_head_dict.text.bn_1.running_mean torch.Size([1024])\n",
      "fusion_head_dict.text.bn_1.running_var torch.Size([1024])\n",
      "fusion_head_dict.text.bn_1.num_batches_tracked torch.Size([])\n",
      "fusion_head_dict.text.bn_2.weight torch.Size([128])\n",
      "fusion_head_dict.text.bn_2.bias torch.Size([128])\n",
      "fusion_head_dict.text.bn_2.running_mean torch.Size([128])\n",
      "fusion_head_dict.text.bn_2.running_var torch.Size([128])\n",
      "fusion_head_dict.text.bn_2.num_batches_tracked torch.Size([])\n",
      "fusion_head_dict.fusion.hidden1_weights torch.Size([30720, 1024])\n",
      "fusion_head_dict.fusion.gating_weights_1 torch.Size([1024, 128])\n",
      "fusion_head_dict.fusion.gating_weights_2 torch.Size([128, 1024])\n",
      "fusion_head_dict.fusion.bn_1.weight torch.Size([1024])\n",
      "fusion_head_dict.fusion.bn_1.bias torch.Size([1024])\n",
      "fusion_head_dict.fusion.bn_1.running_mean torch.Size([1024])\n",
      "fusion_head_dict.fusion.bn_1.running_var torch.Size([1024])\n",
      "fusion_head_dict.fusion.bn_1.num_batches_tracked torch.Size([])\n",
      "fusion_head_dict.fusion.bn_2.weight torch.Size([128])\n",
      "fusion_head_dict.fusion.bn_2.bias torch.Size([128])\n",
      "fusion_head_dict.fusion.bn_2.running_mean torch.Size([128])\n",
      "fusion_head_dict.fusion.bn_2.running_var torch.Size([128])\n",
      "fusion_head_dict.fusion.bn_2.num_batches_tracked torch.Size([])\n",
      "classifier_dict.video.linear_1.weight torch.Size([1024, 1024])\n",
      "classifier_dict.video.linear_1.bias torch.Size([1024])\n",
      "classifier_dict.video.linear_2.weight torch.Size([82, 1024])\n",
      "classifier_dict.video.linear_2.bias torch.Size([82])\n",
      "classifier_dict.audio.linear_1.weight torch.Size([1024, 1024])\n",
      "classifier_dict.audio.linear_1.bias torch.Size([1024])\n",
      "classifier_dict.audio.linear_2.weight torch.Size([82, 1024])\n",
      "classifier_dict.audio.linear_2.bias torch.Size([82])\n",
      "classifier_dict.text.linear_1.weight torch.Size([1024, 1024])\n",
      "classifier_dict.text.linear_1.bias torch.Size([1024])\n",
      "classifier_dict.text.linear_2.weight torch.Size([82, 1024])\n",
      "classifier_dict.text.linear_2.bias torch.Size([82])\n",
      "classifier_dict.fusion.linear_1.weight torch.Size([1024, 1024])\n",
      "classifier_dict.fusion.linear_1.bias torch.Size([1024])\n",
      "classifier_dict.fusion.linear_2.weight torch.Size([82, 1024])\n",
      "classifier_dict.fusion.linear_2.bias torch.Size([82])\n",
      "head_dict.video.cluster_weights torch.Size([3584, 2048])\n",
      "head_dict.video.cluster_weights_2 torch.Size([1, 224, 128])\n",
      "head_dict.video.linear_1.weight torch.Size([3584, 1792])\n",
      "head_dict.video.linear_1.bias torch.Size([3584])\n",
      "head_dict.video.attention_1.weight torch.Size([16, 3584])\n",
      "head_dict.video.attention_1.bias torch.Size([16])\n",
      "head_dict.video.bn_1.weight torch.Size([2048])\n",
      "head_dict.video.bn_1.bias torch.Size([2048])\n",
      "head_dict.video.bn_1.running_mean torch.Size([2048])\n",
      "head_dict.video.bn_1.running_var torch.Size([2048])\n",
      "head_dict.video.bn_1.num_batches_tracked torch.Size([])\n",
      "head_dict.video.bn_2.weight torch.Size([28672])\n",
      "head_dict.video.bn_2.bias torch.Size([28672])\n",
      "head_dict.video.bn_2.running_mean torch.Size([28672])\n",
      "head_dict.video.bn_2.running_var torch.Size([28672])\n",
      "head_dict.video.bn_2.num_batches_tracked torch.Size([])\n",
      "head_dict.audio.cluster_weights torch.Size([256, 1024])\n",
      "head_dict.audio.cluster_weights_2 torch.Size([1, 16, 64])\n",
      "head_dict.audio.linear_1.weight torch.Size([256, 128])\n",
      "head_dict.audio.linear_1.bias torch.Size([256])\n",
      "head_dict.audio.attention_1.weight torch.Size([16, 256])\n",
      "head_dict.audio.attention_1.bias torch.Size([16])\n",
      "head_dict.audio.bn_1.weight torch.Size([1024])\n",
      "head_dict.audio.bn_1.bias torch.Size([1024])\n",
      "head_dict.audio.bn_1.running_mean torch.Size([1024])\n",
      "head_dict.audio.bn_1.running_var torch.Size([1024])\n",
      "head_dict.audio.bn_1.num_batches_tracked torch.Size([])\n",
      "head_dict.audio.bn_2.weight torch.Size([1024])\n",
      "head_dict.audio.bn_2.bias torch.Size([1024])\n",
      "head_dict.audio.bn_2.running_mean torch.Size([1024])\n",
      "head_dict.audio.bn_2.running_var torch.Size([1024])\n",
      "head_dict.audio.bn_2.num_batches_tracked torch.Size([])\n",
      "head_dict.text.model.embeddings.position_ids torch.Size([1, 512])\n",
      "head_dict.text.model.embeddings.word_embeddings.weight torch.Size([21128, 768])\n",
      "head_dict.text.model.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "head_dict.text.model.embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "head_dict.text.model.embeddings.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.embeddings.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "head_dict.text.model.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "head_dict.text.model.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "head_dict.text.model.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "head_dict.text.model.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "head_dict.text.model.pooler.dense.weight torch.Size([768, 768])\n",
      "head_dict.text.model.pooler.dense.bias torch.Size([768])\n",
      "head_dict.text.linear.weight torch.Size([1024, 768])\n",
      "head_dict.text.linear.bias torch.Size([1024])\n",
      "head_dict.text.bn.weight torch.Size([1024])\n",
      "head_dict.text.bn.bias torch.Size([1024])\n",
      "head_dict.text.bn.running_mean torch.Size([1024])\n",
      "head_dict.text.bn.running_var torch.Size([1024])\n",
      "head_dict.text.bn.num_batches_tracked torch.Size([])\n",
      "projector_text.0.weight torch.Size([2048, 1024])\n",
      "projector_text.0.bias torch.Size([2048])\n",
      "projector_text.1.weight torch.Size([2048])\n",
      "projector_text.1.bias torch.Size([2048])\n",
      "projector_text.1.running_mean torch.Size([2048])\n",
      "projector_text.1.running_var torch.Size([2048])\n",
      "projector_text.1.num_batches_tracked torch.Size([])\n",
      "projector_text.3.weight torch.Size([28672, 2048])\n",
      "projector_text.3.bias torch.Size([28672])\n"
     ]
    }
   ],
   "source": [
    "c_state_dict = {}\n",
    "for k,v in c.items():\n",
    "    c_state_dict[k] = v.shape\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "train (0): 100%|████████████████████████████████| 47/47 [00:58<00:00,  1.25s/it]\n",
      "epoch(0)(video):  0.6585780047869804\n",
      "epoch(0)(audio):  0.5971345972440153\n",
      "epoch(0)(text):  0.6265664092942673\n",
      "epoch(0)(fusion):  0.6509482892112433\n",
      "train (1): 100%|████████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (2): 100%|████████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(2)(video):  0.6953765668456061\n",
      "epoch(2)(audio):  0.6504415003528529\n",
      "epoch(2)(text):  0.6882380646485264\n",
      "epoch(2)(fusion):  0.7231490527227318\n",
      "train (3): 100%|████████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (4): 100%|████████████████████████████████| 47/47 [00:52<00:00,  1.11s/it]\n",
      "epoch(4)(video):  0.6595217186277845\n",
      "epoch(4)(audio):  0.6568054238640837\n",
      "epoch(4)(text):  0.7064165227195107\n",
      "epoch(4)(fusion):  0.7386683252334627\n",
      "train (5): 100%|████████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (6): 100%|████████████████████████████████| 47/47 [00:52<00:00,  1.11s/it]\n",
      "epoch(6)(video):  0.7045145079868814\n",
      "epoch(6)(audio):  0.666672505494487\n",
      "epoch(6)(text):  0.7176892196346863\n",
      "epoch(6)(fusion):  0.7579018683276808\n",
      "train (7): 100%|████████████████████████████████| 47/47 [00:52<00:00,  1.11s/it]\n",
      "train (8): 100%|████████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(8)(video):  0.7228488466118931\n",
      "epoch(8)(audio):  0.6696925209172528\n",
      "epoch(8)(text):  0.719915163017291\n",
      "epoch(8)(fusion):  0.7587101524501223\n",
      "train (9): 100%|████████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (10): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(10)(video):  0.7278435570056275\n",
      "epoch(10)(audio):  0.6701552293636485\n",
      "epoch(10)(text):  0.7301145842122416\n",
      "epoch(10)(fusion):  0.7656491857879701\n",
      "train (11): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (12): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(12)(video):  0.7207913210618782\n",
      "epoch(12)(audio):  0.6700120438601647\n",
      "epoch(12)(text):  0.7271100944538514\n",
      "epoch(12)(fusion):  0.7719070850581984\n",
      "train (13): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.11s/it]\n",
      "train (14): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(14)(video):  0.7210878161601857\n",
      "epoch(14)(audio):  0.6729506384304906\n",
      "epoch(14)(text):  0.7306009001854182\n",
      "epoch(14)(fusion):  0.770286467122022\n",
      "train (15): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.11s/it]\n",
      "train (16): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(16)(video):  0.7227088551857398\n",
      "epoch(16)(audio):  0.6741821248872202\n",
      "epoch(16)(text):  0.7287274386870644\n",
      "epoch(16)(fusion):  0.7741504989450355\n",
      "train (17): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (18): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(18)(video):  0.736590327462869\n",
      "epoch(18)(audio):  0.67294750014793\n",
      "epoch(18)(text):  0.7261938400269123\n",
      "epoch(18)(fusion):  0.7773808081861918\n",
      "train (19): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.11s/it]\n",
      "train (20): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.11s/it]\n",
      "epoch(20)(video):  0.760923980327719\n",
      "epoch(20)(audio):  0.6738042680993424\n",
      "epoch(20)(text):  0.7319969469239727\n",
      "epoch(20)(fusion):  0.781868739307366\n",
      "train (21): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (22): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(22)(video):  0.7606307296641373\n",
      "epoch(22)(audio):  0.676901096167909\n",
      "epoch(22)(text):  0.7339675453824741\n",
      "epoch(22)(fusion):  0.7847347768169598\n",
      "train (23): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (24): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(24)(video):  0.7624089439567278\n",
      "epoch(24)(audio):  0.6766069402657814\n",
      "epoch(24)(text):  0.7326915832792583\n",
      "epoch(24)(fusion):  0.7853593254367771\n",
      "train (25): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (26): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.11s/it]\n",
      "epoch(26)(video):  0.7625076943397382\n",
      "epoch(26)(audio):  0.6766030545427645\n",
      "epoch(26)(text):  0.7332440800168528\n",
      "epoch(26)(fusion):  0.7862104501040298\n",
      "train (27): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.11s/it]\n",
      "train (28): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(28)(video):  0.7589586461227863\n",
      "epoch(28)(audio):  0.6780560421972287\n",
      "epoch(28)(text):  0.7345538611994785\n",
      "epoch(28)(fusion):  0.7868243071670753\n",
      "train (29): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (30): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(30)(video):  0.760667153573814\n",
      "epoch(30)(audio):  0.6772250303530728\n",
      "epoch(30)(text):  0.7362473360620303\n",
      "epoch(30)(fusion):  0.7866870774299753\n",
      "train (31): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.11s/it]\n",
      "train (32): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(32)(video):  0.7630808245466121\n",
      "epoch(32)(audio):  0.6768499453392994\n",
      "epoch(32)(text):  0.7348463981660226\n",
      "epoch(32)(fusion):  0.7885412187255234\n",
      "train (33): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (34): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(34)(video):  0.7608325053742913\n",
      "epoch(34)(audio):  0.6774248534183771\n",
      "epoch(34)(text):  0.7324815346967662\n",
      "epoch(34)(fusion):  0.7874838767155039\n",
      "train (35): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (36): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(36)(video):  0.7637214895518195\n",
      "epoch(36)(audio):  0.6768030705302706\n",
      "epoch(36)(text):  0.7317474507040224\n",
      "epoch(36)(fusion):  0.7886844636287265\n",
      "train (37): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (38): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(38)(video):  0.7584882847282967\n",
      "epoch(38)(audio):  0.6776853121852683\n",
      "epoch(38)(text):  0.7325224907017389\n",
      "epoch(38)(fusion):  0.7875332661585814\n",
      "train (39): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (40): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(40)(video):  0.7630511504666051\n",
      "epoch(40)(audio):  0.6768457037902218\n",
      "epoch(40)(text):  0.7327995260092903\n",
      "epoch(40)(fusion):  0.789361967071398\n",
      "train (41): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.11s/it]\n",
      "train (42): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(42)(video):  0.7640693924514999\n",
      "epoch(42)(audio):  0.6761774209765602\n",
      "epoch(42)(text):  0.7311950694753446\n",
      "epoch(42)(fusion):  0.7883935459084641\n",
      "train (43): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (44): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(44)(video):  0.7637319253951863\n",
      "epoch(44)(audio):  0.6758449932823457\n",
      "epoch(44)(text):  0.7326552078444172\n",
      "epoch(44)(fusion):  0.7891636503537062\n",
      "train (45): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (46): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(46)(video):  0.7632797392013091\n",
      "epoch(46)(audio):  0.6764023292167722\n",
      "epoch(46)(text):  0.7322748869788377\n",
      "epoch(46)(fusion):  0.788516586649501\n",
      "train (47): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (48): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(48)(video):  0.7642004902477745\n",
      "epoch(48)(audio):  0.6778066659225516\n",
      "epoch(48)(text):  0.7317218806333569\n",
      "epoch(48)(fusion):  0.7887075451757131\n",
      "train (49): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (50): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.11s/it]\n",
      "epoch(50)(video):  0.7646265876618271\n",
      "epoch(50)(audio):  0.6764836320973446\n",
      "epoch(50)(text):  0.7322268878088904\n",
      "epoch(50)(fusion):  0.7888455726017167\n",
      "train (51): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (52): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(52)(video):  0.7630237819020325\n",
      "epoch(52)(audio):  0.677452494205937\n",
      "epoch(52)(text):  0.7320583386522816\n",
      "epoch(52)(fusion):  0.7885880212278203\n",
      "train (53): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (54): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(54)(video):  0.7639086009913388\n",
      "epoch(54)(audio):  0.6752970665132186\n",
      "epoch(54)(text):  0.7317938957707685\n",
      "epoch(54)(fusion):  0.7886152615898545\n",
      "train (55): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (56): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(56)(video):  0.7634165921282104\n",
      "epoch(56)(audio):  0.6768180417975836\n",
      "epoch(56)(text):  0.733153212051952\n",
      "epoch(56)(fusion):  0.7894566343430518\n",
      "train (57): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.11s/it]\n",
      "train (58): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(58)(video):  0.7646769524916927\n",
      "epoch(58)(audio):  0.6768443081691663\n",
      "epoch(58)(text):  0.7318746184555611\n",
      "epoch(58)(fusion):  0.7890609406243873\n",
      "train (59): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (60): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(60)(video):  0.7629286272958079\n",
      "epoch(60)(audio):  0.6765518108407216\n",
      "epoch(60)(text):  0.7318314012432539\n",
      "epoch(60)(fusion):  0.7883663824752682\n",
      "train (61): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (62): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(62)(video):  0.7627957790402072\n",
      "epoch(62)(audio):  0.6760258567910897\n",
      "epoch(62)(text):  0.7327262958997803\n",
      "epoch(62)(fusion):  0.7881578980410653\n",
      "train (63): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (64): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(64)(video):  0.7627765534919069\n",
      "epoch(64)(audio):  0.6763034683007882\n",
      "epoch(64)(text):  0.7321602525436396\n",
      "epoch(64)(fusion):  0.7885158652968818\n",
      "train (65): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (66): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(66)(video):  0.7636645964972485\n",
      "epoch(66)(audio):  0.6747541190525616\n",
      "epoch(66)(text):  0.7311491590903999\n",
      "epoch(66)(fusion):  0.7877869763806\n",
      "train (67): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (68): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.13s/it]\n",
      "epoch(68)(video):  0.7636168466984249\n",
      "epoch(68)(audio):  0.676899852206023\n",
      "epoch(68)(text):  0.7314239992667947\n",
      "epoch(68)(fusion):  0.789321141052742\n",
      "train (69): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.11s/it]\n",
      "train (70): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(70)(video):  0.7642660662389743\n",
      "epoch(70)(audio):  0.6767305743864632\n",
      "epoch(70)(text):  0.7325228359318494\n",
      "epoch(70)(fusion):  0.7889745608156413\n",
      "train (71): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (72): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(72)(video):  0.7637549168414074\n",
      "epoch(72)(audio):  0.6766966565669931\n",
      "epoch(72)(text):  0.7308511493807905\n",
      "epoch(72)(fusion):  0.7882271327888887\n",
      "train (73): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (74): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(74)(video):  0.7636337497947318\n",
      "epoch(74)(audio):  0.675143003044817\n",
      "epoch(74)(text):  0.7323547895283578\n",
      "epoch(74)(fusion):  0.7886180671573764\n",
      "train (75): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (76): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(76)(video):  0.7632610610303681\n",
      "epoch(76)(audio):  0.6755880616634606\n",
      "epoch(76)(text):  0.7324059860865121\n",
      "epoch(76)(fusion):  0.7882519762562245\n",
      "train (77): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (78): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(78)(video):  0.7642788068351032\n",
      "epoch(78)(audio):  0.6770933610881664\n",
      "epoch(78)(text):  0.7321419092868534\n",
      "epoch(78)(fusion):  0.7892358547835299\n",
      "train (79): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (80): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(80)(video):  0.7640463705873383\n",
      "epoch(80)(audio):  0.6754539739888735\n",
      "epoch(80)(text):  0.7328327106160014\n",
      "epoch(80)(fusion):  0.7884727926505194\n",
      "train (81): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (82): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(82)(video):  0.7622719706077009\n",
      "epoch(82)(audio):  0.6771429114988223\n",
      "epoch(82)(text):  0.7317742042390578\n",
      "epoch(82)(fusion):  0.7886571790512527\n",
      "train (83): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (84): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(84)(video):  0.7638768519248897\n",
      "epoch(84)(audio):  0.6771598329568012\n",
      "epoch(84)(text):  0.7315474383458854\n",
      "epoch(84)(fusion):  0.7886639927462462\n",
      "train (85): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (86): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.11s/it]\n",
      "epoch(86)(video):  0.7642503955357705\n",
      "epoch(86)(audio):  0.6776422773830044\n",
      "epoch(86)(text):  0.7320148051660448\n",
      "epoch(86)(fusion):  0.7887545103592591\n",
      "train (87): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.11s/it]\n",
      "train (88): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(88)(video):  0.7638088299771575\n",
      "epoch(88)(audio):  0.675645038226211\n",
      "epoch(88)(text):  0.7328847830553707\n",
      "epoch(88)(fusion):  0.7883031021925742\n",
      "train (89): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "train (90): 100%|███████████████████████████████| 47/47 [00:52<00:00,  1.12s/it]\n",
      "epoch(90)(video):  0.7631350285078821\n",
      "epoch(90)(audio):  0.6771745223115183\n",
      "epoch(90)(text):  0.7326169195666542\n",
      "epoch(90)(fusion):  0.7884548967036595\n",
      "train (91):   0%|                                        | 0/47 [00:00<?, ?it/s]^C\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/spawn.py\", line 105, in spawn_main\n",
      "    exitcode = _main(fd)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/spawn.py\", line 114, in _main\n",
      "    prepare(preparation_data)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/spawn.py\", line 225, in prepare\n",
      "    _fixup_main_from_path(data['init_main_from_path'])\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/spawn.py\", line 277, in _fixup_main_from_path\n",
      "    run_name=\"__mp_main__\")\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/runpy.py\", line 263, in run_path\n",
      "train (91):   0%|                                        | 0/47 [00:03<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "    pkg_name=pkg_name, script_name=fname)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/runpy.py\", line 96, in _run_module_code\n",
      "    mod_name, mod_spec, pkg_name, script_name)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "  File \"main.py\", line 121, in <module>\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/tione/notebook/TAAC-2021/main.py\", line 10, in <module>\n",
      "    from dataloader.dataloader import MultimodaFeaturesDataset,Datasetfortextcnn\n",
      "  File \"/home/tione/notebook/TAAC-2021/dataloader/dataloader.py\", line 14, in <module>\n",
      "    from albumentations.pytorch import ToTensorV2\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/albumentations/pytorch/__init__.py\", line 3, in <module>\n",
      "    loss = training_loop(model, train_loader, loss_compute, modal_name_list,train_dataset.device, epoch,TBoard)\n",
      "  File \"/home/tione/notebook/TAAC-2021/src/loop/run_epoch.py\", line 15, in training_loop\n",
      "    from .transforms import *\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/albumentations/pytorch/transforms.py\", line 7, in <module>\n",
      "    from torchvision.transforms import functional as F\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/torchvision/__init__.py\", line 4, in <module>\n",
      "    for i, batch in enumerate(tqdm(loader, desc=f'train ({epoch})')):\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/tqdm/std.py\", line 1129, in __iter__\n",
      "    from torchvision import datasets\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/torchvision/datasets/__init__.py\", line 1, in <module>\n",
      "    from .lsun import LSUN, LSUNClass\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/torchvision/datasets/lsun.py\", line 1, in <module>\n",
      "    for obj in iterable:\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 279, in __iter__\n",
      "    from .vision import VisionDataset\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\n",
      "KeyboardInterrupt\n",
      "    return _MultiProcessingDataLoaderIter(self)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 719, in __init__\n",
      "    w.start()\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/process.py\", line 105, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/context.py\", line 223, in _Popen\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/context.py\", line 284, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n",
      "    super().__init__(process_obj)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 62, in _launch\n",
      "    f.write(fp.getbuffer())\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# 0615/01 多卡试试 更大的batch size\n",
    "!python -W ignore main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "train (0): 100%|████████████████████████████████| 44/44 [00:46<00:00,  1.06s/it]\n",
      "epoch(0)(video):  0.6616481288095377\n",
      "epoch(0)(audio):  0.5934669996690477\n",
      "epoch(0)(text):  0.6175830888628007\n",
      "epoch(0)(fusion):  0.6441689285917185\n",
      "train (1): 100%|████████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (2): 100%|████████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(2)(video):  0.7141182394763405\n",
      "epoch(2)(audio):  0.6495006763541227\n",
      "epoch(2)(text):  0.686726007216266\n",
      "epoch(2)(fusion):  0.7128472897338756\n",
      "train (3): 100%|████████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (4): 100%|████████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(4)(video):  0.6589121429907483\n",
      "epoch(4)(audio):  0.6563851215631941\n",
      "epoch(4)(text):  0.7022549335537613\n",
      "epoch(4)(fusion):  0.7184382875210268\n",
      "train (5): 100%|████████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (6): 100%|████████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(6)(video):  0.6634542851795413\n",
      "epoch(6)(audio):  0.6617390495899117\n",
      "epoch(6)(text):  0.7114834345520097\n",
      "epoch(6)(fusion):  0.7546578531923374\n",
      "train (7): 100%|████████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (8): 100%|████████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(8)(video):  0.7238807200324874\n",
      "epoch(8)(audio):  0.6684347178634366\n",
      "epoch(8)(text):  0.7226233647875211\n",
      "epoch(8)(fusion):  0.7559131408547057\n",
      "train (9): 100%|████████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (10): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(10)(video):  0.7254099684115606\n",
      "epoch(10)(audio):  0.6708197935635964\n",
      "epoch(10)(text):  0.725189708231789\n",
      "epoch(10)(fusion):  0.7651270091386436\n",
      "train (11): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (12): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(12)(video):  0.7233988217449158\n",
      "epoch(12)(audio):  0.6718844940786248\n",
      "epoch(12)(text):  0.7311455542704991\n",
      "epoch(12)(fusion):  0.7655888222378678\n",
      "train (13): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (14): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(14)(video):  0.7029008993896351\n",
      "epoch(14)(audio):  0.6728803671251314\n",
      "epoch(14)(text):  0.7278839573291995\n",
      "epoch(14)(fusion):  0.7640026481609168\n",
      "train (15): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (16): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(16)(video):  0.7332366837053342\n",
      "epoch(16)(audio):  0.6714645653842488\n",
      "epoch(16)(text):  0.726253643460746\n",
      "epoch(16)(fusion):  0.7724018012825029\n",
      "train (17): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (18): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(18)(video):  0.7408201661578708\n",
      "epoch(18)(audio):  0.6699336456488352\n",
      "epoch(18)(text):  0.7316498855173523\n",
      "epoch(18)(fusion):  0.7773299460026484\n",
      "train (19): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (20): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(20)(video):  0.7591956114912741\n",
      "epoch(20)(audio):  0.6742081527602322\n",
      "epoch(20)(text):  0.7312503401455959\n",
      "epoch(20)(fusion):  0.781375023859633\n",
      "train (21): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (22): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(22)(video):  0.7577880330026511\n",
      "epoch(22)(audio):  0.6775065347186281\n",
      "epoch(22)(text):  0.7329961475462407\n",
      "epoch(22)(fusion):  0.7829438415057985\n",
      "train (23): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (24): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(24)(video):  0.7629025696078398\n",
      "epoch(24)(audio):  0.6777866269066253\n",
      "epoch(24)(text):  0.7314092000254109\n",
      "epoch(24)(fusion):  0.7829143377884221\n",
      "train (25): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (26): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(26)(video):  0.7602884567855687\n",
      "epoch(26)(audio):  0.6771461791844285\n",
      "epoch(26)(text):  0.7332263065010013\n",
      "epoch(26)(fusion):  0.7832922375871842\n",
      "train (27): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (28): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(28)(video):  0.7598285835149312\n",
      "epoch(28)(audio):  0.6730756266185491\n",
      "epoch(28)(text):  0.7335568001969851\n",
      "epoch(28)(fusion):  0.7840278615424491\n",
      "train (29): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (30): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(30)(video):  0.7601524502342512\n",
      "epoch(30)(audio):  0.6762321665128342\n",
      "epoch(30)(text):  0.7353037222514686\n",
      "epoch(30)(fusion):  0.7836093516422438\n",
      "train (31): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (32): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(32)(video):  0.76247708065005\n",
      "epoch(32)(audio):  0.6747175494982133\n",
      "epoch(32)(text):  0.736014656801391\n",
      "epoch(32)(fusion):  0.7861858665008183\n",
      "train (33): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (34): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(34)(video):  0.7620699498402057\n",
      "epoch(34)(audio):  0.6753924213079047\n",
      "epoch(34)(text):  0.7330053697507908\n",
      "epoch(34)(fusion):  0.7864344807619054\n",
      "train (35): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (36): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(36)(video):  0.7644113774701178\n",
      "epoch(36)(audio):  0.6748636151578911\n",
      "epoch(36)(text):  0.7329067970952202\n",
      "epoch(36)(fusion):  0.7857198101454661\n",
      "train (37): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (38): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.08it/s]\n",
      "epoch(38)(video):  0.763030362281378\n",
      "epoch(38)(audio):  0.6763634869478922\n",
      "epoch(38)(text):  0.7318999968557306\n",
      "epoch(38)(fusion):  0.7863306923372376\n",
      "train (39): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.08it/s]\n",
      "train (40): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(40)(video):  0.764073161747507\n",
      "epoch(40)(audio):  0.6755847198530556\n",
      "epoch(40)(text):  0.7344960993761969\n",
      "epoch(40)(fusion):  0.7864189543572433\n",
      "train (41): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (42): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(42)(video):  0.7643382390644579\n",
      "epoch(42)(audio):  0.6741072428993028\n",
      "epoch(42)(text):  0.7336171183981129\n",
      "epoch(42)(fusion):  0.7863958066881312\n",
      "train (43): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (44): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(44)(video):  0.7649088682844977\n",
      "epoch(44)(audio):  0.6741379742586008\n",
      "epoch(44)(text):  0.7344638937586792\n",
      "epoch(44)(fusion):  0.7873253996680394\n",
      "train (45): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (46): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(46)(video):  0.766282314980329\n",
      "epoch(46)(audio):  0.675503786578038\n",
      "epoch(46)(text):  0.7340092894373971\n",
      "epoch(46)(fusion):  0.7876134566809064\n",
      "train (47): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (48): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(48)(video):  0.7649276529538684\n",
      "epoch(48)(audio):  0.6747810902574574\n",
      "epoch(48)(text):  0.7330178015387406\n",
      "epoch(48)(fusion):  0.7875290242290718\n",
      "train (49): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (50): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(50)(video):  0.7649627410825703\n",
      "epoch(50)(audio):  0.6735050119991353\n",
      "epoch(50)(text):  0.7332024672076999\n",
      "epoch(50)(fusion):  0.7866167687888614\n",
      "train (51): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (52): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(52)(video):  0.7658114160523909\n",
      "epoch(52)(audio):  0.676439970604524\n",
      "epoch(52)(text):  0.7328025300721899\n",
      "epoch(52)(fusion):  0.7866683771478458\n",
      "train (53): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (54): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.08it/s]\n",
      "epoch(54)(video):  0.7647274095595336\n",
      "epoch(54)(audio):  0.676383567467845\n",
      "epoch(54)(text):  0.7333281054127321\n",
      "epoch(54)(fusion):  0.7870099139621433\n",
      "train (55): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (56): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(56)(video):  0.7644146573822691\n",
      "epoch(56)(audio):  0.6754626501702335\n",
      "epoch(56)(text):  0.7337779952608017\n",
      "epoch(56)(fusion):  0.7869900192645222\n",
      "train (57): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "train (58): 100%|███████████████████████████████| 44/44 [00:40<00:00,  1.09it/s]\n",
      "epoch(58)(video):  0.7641522881281947\n",
      "epoch(58)(audio):  0.6733774704705704\n",
      "epoch(58)(text):  0.7334552033212068\n",
      "epoch(58)(fusion):  0.786042064458756\n",
      "train (59):  80%|████████████████████████▋      | 35/44 [00:33<00:07,  1.23it/s]^C\n"
     ]
    }
   ],
   "source": [
    "!python -W ignore main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "train (0): 100%|████████████████████████████████| 79/79 [01:29<00:00,  1.13s/it]\n",
      "12.190704593175575\n",
      "train (1): 100%|████████████████████████████████| 79/79 [01:22<00:00,  1.05s/it]\n",
      "7.743183757685408\n",
      "train (2): 100%|████████████████████████████████| 79/79 [01:22<00:00,  1.05s/it]\n",
      "6.832556090777433\n",
      "train (3): 100%|████████████████████████████████| 79/79 [01:22<00:00,  1.05s/it]\n",
      "5.81865706624864\n",
      "train (4): 100%|████████████████████████████████| 79/79 [01:22<00:00,  1.05s/it]\n",
      "5.602242234386975\n",
      "train (5): 100%|████████████████████████████████| 79/79 [01:23<00:00,  1.05s/it]\n",
      "5.41833361794677\n",
      "train (6): 100%|████████████████████████████████| 79/79 [01:23<00:00,  1.05s/it]\n",
      "5.105599285681037\n",
      "train (7): 100%|████████████████████████████████| 79/79 [01:22<00:00,  1.05s/it]\n",
      "4.801339747030524\n",
      "train (8): 100%|████████████████████████████████| 79/79 [01:22<00:00,  1.05s/it]\n",
      "4.819867680344401\n",
      "train (9): 100%|████████████████████████████████| 79/79 [01:22<00:00,  1.05s/it]\n",
      "4.516751944264279\n",
      "train (10): 100%|███████████████████████████████| 79/79 [01:22<00:00,  1.05s/it]\n",
      "4.332833966122398\n",
      "train (11):  11%|███▋                            | 9/79 [00:13<01:29,  1.28s/it]"
     ]
    }
   ],
   "source": [
    "# 修复mask的自监督\n",
    "!python -W ignore enhance_main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "train (0): 100%|████████████████████████████████| 47/47 [01:24<00:00,  1.81s/it]\n",
      "epoch(0)(video):  0.6570029053911018\n",
      "epoch(0)(audio):  0.5977309284894636\n",
      "epoch(0)(text):  0.6344456246144599\n",
      "epoch(0)(fusion):  0.6517066539551074\n",
      "train (1): 100%|████████████████████████████████| 47/47 [00:42<00:00,  1.10it/s]\n",
      "train (2): 100%|████████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "epoch(2)(video):  0.7051911926935437\n",
      "epoch(2)(audio):  0.6519766775507388\n",
      "epoch(2)(text):  0.6954199796586572\n",
      "epoch(2)(fusion):  0.7198793214483767\n",
      "train (3): 100%|████████████████████████████████| 47/47 [00:42<00:00,  1.10it/s]\n",
      "train (4): 100%|████████████████████████████████| 47/47 [00:43<00:00,  1.09it/s]\n",
      "epoch(4)(video):  0.7062804267863271\n",
      "epoch(4)(audio):  0.6517274779749817\n",
      "epoch(4)(text):  0.7098202586270581\n",
      "epoch(4)(fusion):  0.7401088939698077\n",
      "train (5): 100%|████████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "train (6): 100%|████████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "epoch(6)(video):  0.7115292638717685\n",
      "epoch(6)(audio):  0.6569187536762072\n",
      "epoch(6)(text):  0.7222360061887568\n",
      "epoch(6)(fusion):  0.7559795095125006\n",
      "train (7): 100%|████████████████████████████████| 47/47 [00:43<00:00,  1.09it/s]\n",
      "train (8): 100%|████████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "epoch(8)(video):  0.7193815171835118\n",
      "epoch(8)(audio):  0.6625783002810334\n",
      "epoch(8)(text):  0.726340343459528\n",
      "epoch(8)(fusion):  0.7573733707089492\n",
      "train (9): 100%|████████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "train (10): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "epoch(10)(video):  0.7148266886696404\n",
      "epoch(10)(audio):  0.6684612333975622\n",
      "epoch(10)(text):  0.7287412820819449\n",
      "epoch(10)(fusion):  0.7603369002456691\n",
      "train (11): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.10it/s]\n",
      "train (12): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "epoch(12)(video):  0.6990299631929198\n",
      "epoch(12)(audio):  0.6718983913119941\n",
      "epoch(12)(text):  0.7319517901077203\n",
      "epoch(12)(fusion):  0.7674353738715946\n",
      "train (13): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.10it/s]\n",
      "train (14): 100%|███████████████████████████████| 47/47 [00:43<00:00,  1.09it/s]\n",
      "epoch(14)(video):  0.6981987364099287\n",
      "epoch(14)(audio):  0.671086192866296\n",
      "epoch(14)(text):  0.7348380328212979\n",
      "epoch(14)(fusion):  0.7680366423942541\n",
      "train (15): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "train (16): 100%|███████████████████████████████| 47/47 [00:43<00:00,  1.09it/s]\n",
      "epoch(16)(video):  0.7192347394204738\n",
      "epoch(16)(audio):  0.6726434541757049\n",
      "epoch(16)(text):  0.7356842971379511\n",
      "epoch(16)(fusion):  0.7736842662593504\n",
      "train (17): 100%|███████████████████████████████| 47/47 [00:43<00:00,  1.08it/s]\n",
      "train (18): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "epoch(18)(video):  0.7380115236859115\n",
      "epoch(18)(audio):  0.6725442129667152\n",
      "epoch(18)(text):  0.7329314923658337\n",
      "epoch(18)(fusion):  0.7758447259544546\n",
      "train (19): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.10it/s]\n",
      "train (20): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "epoch(20)(video):  0.7610523379866443\n",
      "epoch(20)(audio):  0.6753824998477201\n",
      "epoch(20)(text):  0.7345291143130787\n",
      "epoch(20)(fusion):  0.7829464394331613\n",
      "train (21): 100%|███████████████████████████████| 47/47 [00:43<00:00,  1.09it/s]\n",
      "train (22): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "epoch(22)(video):  0.7603043860383111\n",
      "epoch(22)(audio):  0.6771067844897426\n",
      "epoch(22)(text):  0.7365882727928859\n",
      "epoch(22)(fusion):  0.7861596847905711\n",
      "train (23): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.12it/s]\n",
      "train (24): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "epoch(24)(video):  0.763225247721227\n",
      "epoch(24)(audio):  0.6763293210231538\n",
      "epoch(24)(text):  0.7368599457649887\n",
      "epoch(24)(fusion):  0.7866113210462514\n",
      "train (25): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.10it/s]\n",
      "train (26): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.10it/s]\n",
      "epoch(26)(video):  0.758077940893506\n",
      "epoch(26)(audio):  0.6776485634188111\n",
      "epoch(26)(text):  0.7383588967403003\n",
      "epoch(26)(fusion):  0.7869403172674072\n",
      "train (27): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.12it/s]\n",
      "train (28): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "epoch(28)(video):  0.7596586090858971\n",
      "epoch(28)(audio):  0.6799077935913288\n",
      "epoch(28)(text):  0.7375622020909502\n",
      "epoch(28)(fusion):  0.7869601115407161\n",
      "train (29): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.12it/s]\n",
      "train (30): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "epoch(30)(video):  0.756673517511614\n",
      "epoch(30)(audio):  0.6768975716211508\n",
      "epoch(30)(text):  0.7376068692681637\n",
      "epoch(30)(fusion):  0.784135505363828\n",
      "train (31): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "train (32): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.10it/s]\n",
      "epoch(32)(video):  0.764014819351658\n",
      "epoch(32)(audio):  0.6777100525449532\n",
      "epoch(32)(text):  0.7377866269158859\n",
      "epoch(32)(fusion):  0.7864349061317656\n",
      "train (33): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "train (34): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.10it/s]\n",
      "epoch(34)(video):  0.7592639916556316\n",
      "epoch(34)(audio):  0.6744512020914163\n",
      "epoch(34)(text):  0.7370762010570071\n",
      "epoch(34)(fusion):  0.7855548334512656\n",
      "train (35): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "train (36): 100%|███████████████████████████████| 47/47 [00:43<00:00,  1.09it/s]\n",
      "epoch(36)(video):  0.7613520252182872\n",
      "epoch(36)(audio):  0.6753177933329916\n",
      "epoch(36)(text):  0.735853236334135\n",
      "epoch(36)(fusion):  0.7853674273546061\n",
      "train (37): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "train (38): 100%|███████████████████████████████| 47/47 [00:43<00:00,  1.07it/s]\n",
      "epoch(38)(video):  0.7591686541904877\n",
      "epoch(38)(audio):  0.6753860662231281\n",
      "epoch(38)(text):  0.7363019210736531\n",
      "epoch(38)(fusion):  0.7867695129924125\n",
      "train (39): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "train (40): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.10it/s]\n",
      "epoch(40)(video):  0.7646792499817485\n",
      "epoch(40)(audio):  0.6761879401756468\n",
      "epoch(40)(text):  0.7372168460094537\n",
      "epoch(40)(fusion):  0.7872828565623178\n",
      "train (41): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.10it/s]\n",
      "train (42): 100%|███████████████████████████████| 47/47 [00:43<00:00,  1.09it/s]\n",
      "epoch(42)(video):  0.7633636632331905\n",
      "epoch(42)(audio):  0.6744433592717889\n",
      "epoch(42)(text):  0.7370764424267583\n",
      "epoch(42)(fusion):  0.7881955742351643\n",
      "train (43): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "train (44): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "epoch(44)(video):  0.7647433031600522\n",
      "epoch(44)(audio):  0.6766658086604936\n",
      "epoch(44)(text):  0.7369854261274977\n",
      "epoch(44)(fusion):  0.78753649046703\n",
      "train (45): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "train (46): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.10it/s]\n",
      "epoch(46)(video):  0.7634936305724603\n",
      "epoch(46)(audio):  0.6763422426331287\n",
      "epoch(46)(text):  0.7380617962759934\n",
      "epoch(46)(fusion):  0.7879057356556242\n",
      "train (47): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "train (48): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "epoch(48)(video):  0.7647408918641556\n",
      "epoch(48)(audio):  0.6762914792549969\n",
      "epoch(48)(text):  0.7373008338869118\n",
      "epoch(48)(fusion):  0.7875994665425703\n",
      "train (49): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.12it/s]\n",
      "train (50): 100%|███████████████████████████████| 47/47 [00:43<00:00,  1.09it/s]\n",
      "epoch(50)(video):  0.7641767387860252\n",
      "epoch(50)(audio):  0.6758615693941785\n",
      "epoch(50)(text):  0.7379500688242598\n",
      "epoch(50)(fusion):  0.7882412426899051\n",
      "train (51): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "train (52): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.10it/s]\n",
      "epoch(52)(video):  0.7652322348092171\n",
      "epoch(52)(audio):  0.6764744440279383\n",
      "epoch(52)(text):  0.7363818326240691\n",
      "epoch(52)(fusion):  0.7884268623667275\n",
      "train (53): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.10it/s]\n",
      "train (54): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.09it/s]\n",
      "epoch(54)(video):  0.763505961732032\n",
      "epoch(54)(audio):  0.6754582408241556\n",
      "epoch(54)(text):  0.7364885825161309\n",
      "epoch(54)(fusion):  0.7876242788007908\n",
      "train (55): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.12it/s]\n",
      "train (56): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "epoch(56)(video):  0.7642015089133517\n",
      "epoch(56)(audio):  0.6758062968366538\n",
      "epoch(56)(text):  0.7374951049252289\n",
      "epoch(56)(fusion):  0.787976025251543\n",
      "train (57): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.10it/s]\n",
      "train (58): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.09it/s]\n",
      "epoch(58)(video):  0.7653449997736391\n",
      "epoch(58)(audio):  0.6764912717216529\n",
      "epoch(58)(text):  0.7375705974476012\n",
      "epoch(58)(fusion):  0.7889385772996538\n",
      "train (59): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "train (60): 100%|███████████████████████████████| 47/47 [00:44<00:00,  1.06it/s]\n",
      "epoch(60)(video):  0.764360785114123\n",
      "epoch(60)(audio):  0.6760866006128448\n",
      "epoch(60)(text):  0.7376608272864641\n",
      "epoch(60)(fusion):  0.7874168293695899\n",
      "train (61): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "train (62): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "epoch(62)(video):  0.7639609284890758\n",
      "epoch(62)(audio):  0.6763785477825772\n",
      "epoch(62)(text):  0.7376109421265282\n",
      "epoch(62)(fusion):  0.7871641733801973\n",
      "train (63): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.12it/s]\n",
      "train (64): 100%|███████████████████████████████| 47/47 [00:42<00:00,  1.11it/s]\n",
      "epoch(64)(video):  0.7640871476643762\n",
      "epoch(64)(audio):  0.6749844243981868\n",
      "epoch(64)(text):  0.7370017222387\n",
      "epoch(64)(fusion):  0.7881558857878903\n",
      "train (65): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.13it/s]\n",
      "train (66): 100%|███████████████████████████████| 47/47 [00:40<00:00,  1.15it/s]\n",
      "epoch(66)(video):  0.7654809081559039\n",
      "epoch(66)(audio):  0.6748750602223785\n",
      "epoch(66)(text):  0.7369828992230231\n",
      "epoch(66)(fusion):  0.7873178543793348\n",
      "train (67): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.15it/s]\n",
      "train (68): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "epoch(68)(video):  0.7645210892117393\n",
      "epoch(68)(audio):  0.675238214389932\n",
      "epoch(68)(text):  0.7377951065825895\n",
      "epoch(68)(fusion):  0.787951355515751\n",
      "train (69): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "train (70): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "epoch(70)(video):  0.7660369484832453\n",
      "epoch(70)(audio):  0.6768195455540598\n",
      "epoch(70)(text):  0.7383800968696619\n",
      "epoch(70)(fusion):  0.787641692448156\n",
      "train (71): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "train (72): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "epoch(72)(video):  0.7647987117198422\n",
      "epoch(72)(audio):  0.675788877088943\n",
      "epoch(72)(text):  0.7374700909350939\n",
      "epoch(72)(fusion):  0.7877741250741914\n",
      "train (73): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "train (74): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "epoch(74)(video):  0.7651948566460793\n",
      "epoch(74)(audio):  0.6750701070414151\n",
      "epoch(74)(text):  0.7369129536195758\n",
      "epoch(74)(fusion):  0.7866993971648438\n",
      "train (75): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "train (76): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "epoch(76)(video):  0.7652567742244252\n",
      "epoch(76)(audio):  0.6750174963757893\n",
      "epoch(76)(text):  0.7367900228179788\n",
      "epoch(76)(fusion):  0.7879851827375471\n",
      "train (77): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "train (78): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "epoch(78)(video):  0.7642101365177003\n",
      "epoch(78)(audio):  0.6757163615330567\n",
      "epoch(78)(text):  0.7373498213983207\n",
      "epoch(78)(fusion):  0.7881467578599024\n",
      "train (79): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "train (80): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "epoch(80)(video):  0.7653722952068955\n",
      "epoch(80)(audio):  0.6766899896247407\n",
      "epoch(80)(text):  0.7373294996779732\n",
      "epoch(80)(fusion):  0.7877605505446427\n",
      "train (81): 100%|███████████████████████████████| 47/47 [00:40<00:00,  1.15it/s]\n",
      "train (82): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "epoch(82)(video):  0.7634474680780828\n",
      "epoch(82)(audio):  0.6756604134725926\n",
      "epoch(82)(text):  0.7369989392673945\n",
      "epoch(82)(fusion):  0.7877544292967951\n",
      "train (83): 100%|███████████████████████████████| 47/47 [00:40<00:00,  1.15it/s]\n",
      "train (84): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "epoch(84)(video):  0.7648052789798125\n",
      "epoch(84)(audio):  0.6760763161076748\n",
      "epoch(84)(text):  0.7362341986483043\n",
      "epoch(84)(fusion):  0.7880640827456764\n",
      "train (85): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "train (86): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "epoch(86)(video):  0.7658823736907198\n",
      "epoch(86)(audio):  0.6760308230208265\n",
      "epoch(86)(text):  0.7376926170497765\n",
      "epoch(86)(fusion):  0.7881845193782138\n",
      "train (87): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.15it/s]\n",
      "train (88): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "epoch(88)(video):  0.7646343738027402\n",
      "epoch(88)(audio):  0.6754323540846758\n",
      "epoch(88)(text):  0.7378398312522386\n",
      "epoch(88)(fusion):  0.7879495354362038\n",
      "train (89): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "train (90): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "epoch(90)(video):  0.7643680536247819\n",
      "epoch(90)(audio):  0.6771643275527296\n",
      "epoch(90)(text):  0.7372694277011062\n",
      "epoch(90)(fusion):  0.787486498257665\n",
      "train (91): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.15it/s]\n",
      "train (92): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "epoch(92)(video):  0.7650403907894628\n",
      "epoch(92)(audio):  0.6758742894384098\n",
      "epoch(92)(text):  0.7373768884177238\n",
      "epoch(92)(fusion):  0.7875079250288611\n",
      "train (93): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "train (94): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "epoch(94)(video):  0.7638521236771123\n",
      "epoch(94)(audio):  0.6761096015589112\n",
      "epoch(94)(text):  0.737762857607812\n",
      "epoch(94)(fusion):  0.7880269975712015\n",
      "train (95): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "train (96): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "epoch(96)(video):  0.7654398346631511\n",
      "epoch(96)(audio):  0.6757716471096981\n",
      "epoch(96)(text):  0.7373530088044836\n",
      "epoch(96)(fusion):  0.7869741691607145\n",
      "train (97): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "train (98): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n",
      "epoch(98)(video):  0.7639644336254524\n",
      "epoch(98)(audio):  0.6745348150252953\n",
      "epoch(98)(text):  0.7375296996530587\n",
      "epoch(98)(fusion):  0.7871655503536141\n",
      "train (99): 100%|███████████████████████████████| 47/47 [00:41<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# 修复mask的自监督 + 修复mask的main\n",
    "!python -W ignore main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_root = '../dataset/videos/video_5k/train_5k/'\n",
    "\n",
    "# ########## get train_5k_A video file lists\n",
    "videos_train_5k_A_dir = os.path.join(dataset_root, 'videos/train_5k_A')\n",
    "videos_train_5k_A_files = [os.path.join(videos_train_5k_A_dir, f) for f in os.listdir(videos_train_5k_A_dir) if os.path.isfile(os.path.join(videos_train_5k_A_dir, f))]\n",
    "\n",
    "print(\"videos_train_5k_A_dir= {}\".format(videos_train_5k_A_dir))\n",
    "print(\"len(videos/train_5k_A)= {}\".format(len(videos_train_5k_A_files)))\n",
    "\n",
    "# ########## display\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# video\n",
    "test_video_path = videos_train_5k_A_files[3000]\n",
    "print(test_video_path)\n",
    "print(os.path.exists(test_video_path))\n",
    "html_str = '''\n",
    "<video controls width=\\\"500\\\" height=\\\"500\\\" src=\\\"{}\\\">animation</video>\n",
    "'''.format(test_video_path)\n",
    "print(html_str)\n",
    "display(HTML(html_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "train (0): 100%|████████████████████████████████| 94/94 [01:55<00:00,  1.23s/it]\n",
      "epoch(0)(video):  0.6840189556313095\n",
      "epoch(0)(audio):  0.6254347990857769\n",
      "epoch(0)(text):  0.6648310093361605\n",
      "epoch(0)(fusion):  0.6674136591306753\n",
      "train (1):  76%|████████████████████████▏       | 71/94 [01:27<00:26,  1.15s/it]^C\n"
     ]
    }
   ],
   "source": [
    "!python -W ignore main.py   --device_ids 0 \\\n",
    "                            --pretrained_model ../checkpoint/0616/enhance_VT/50_wop.pt \\\n",
    "                            --saved_path ../checkpoint/0616/01/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_py3",
   "language": "python",
   "name": "conda_pytorch_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
