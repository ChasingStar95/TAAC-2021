{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -W ignore main.py   --device_ids 0 \\\n",
    "                            --pretrained_model ../checkpoint/50_wp.pt \\\n",
    "                            --saved_path ../checkpoint/0616/01/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -W ignore inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -W ignore end2end_main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ensemble 模型在验证集上测试\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5,6,7'\n",
    "import yaml\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import utils.train_util as train_util\n",
    "from dataloader.dataloader import TestingDataset\n",
    "from src.loss.loss_compute import SimpleLossCompute\n",
    "from src.model.baseline_model import Baseline\n",
    "from src.loop.run_epoch import training_loop,validating_loop\n",
    "from dataloader.dataloader import MultimodaFeaturesDataset,Datasetfortextcnn\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 8\n",
    "modal_name_list = ['video','audio','text']\n",
    "config_path = './config/config.yaml'\n",
    "config = yaml.load(open(config_path))\n",
    "dataset = MultimodaFeaturesDataset(config['DatasetConfig'],job='valdation')\n",
    "loader = DataLoader(dataset,num_workers=8,\n",
    "                    batch_size=batch_size,\n",
    "                    pin_memory=False,\n",
    "                    collate_fn=dataset.collate_fn)\n",
    "\n",
    "model_path_1 = '../checkpoint/0609/01/epoch_48 0.7888.pt'\n",
    "model_path_2 = '../checkpoint/0609/01/epoch_30 0.7886.pt'\n",
    "model_path_3 = '../checkpoint/0609/01/epoch_24 0.7871.pt'\n",
    "\n",
    "model_path_4 = '../checkpoint/0609/02/epoch_46 0.7868.pt'\n",
    "model_path_5 = '../checkpoint/0609/02/epoch_28 0.7856.pt'\n",
    "\n",
    "model_path_6 = '../checkpoint/0608/01/epoch_86 0.7877.pt'\n",
    "model_path_7 = '../checkpoint/0608/01/epoch_50 0.7873.pt'\n",
    "model_path_8 = '../checkpoint/0608/01/epoch_28 0.7869.pt'\n",
    "\n",
    "model_path_9 = '../checkpoint/0608/03/epoch_28 0.7877.pt'\n",
    "model_path_10 = '../checkpoint/0608/03/epoch_52 0.7890.pt'\n",
    "model_path_11 = '../checkpoint/0608/03/epoch_74 0.7896.pt'\n",
    "models_path = [model_path_1,\n",
    "               model_path_4,\n",
    "               model_path_6,\n",
    "               model_path_10,model_path_11]\n",
    "model_weights = [0.2,0.1,0.2,0.25,0.25] #0.791\n",
    "# model_weights = [0.1,0.1,0.1,0.35,0.35] # 0.789\n",
    "# model_weights = [0.2,0.2,0.2,0.2,0.2] # 0.7911\n",
    "#model_weights = np.array(np.random.random(11))\n",
    "#model_weights = model_weights/sum(model_weights)\n",
    "device = 'cuda'\n",
    "top_k=20\n",
    "# output_json = './0604_resnet_ensemble.json'\n",
    "models = []\n",
    "for path in models_path:\n",
    "    if(path.split('/')[2]+path.split('/')[3]=='060801'):\n",
    "        config['ModelConfig']['fusion_head_params']['concat_feat_dim']['fusion'] = 30720\n",
    "        config['ModelConfig']['audio_head_params']['max_frames'] = 300\n",
    "    else:\n",
    "        config['ModelConfig']['fusion_head_params']['concat_feat_dim']['fusion'] = 29696\n",
    "        config['ModelConfig']['audio_head_params']['max_frames'] = 200\n",
    "    model = Baseline(config['ModelConfig'])\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "tagging_class_num = 82\n",
    "evl_metrics = [train_util.EvaluationMetrics(tagging_class_num, top_k=20)\n",
    "                           for i in range(len(modal_name_list)+1)] #+1 for fusion\n",
    "for i in range(len(evl_metrics)):\n",
    "    evl_metrics[i].clear()\n",
    "metric_dict = {}\n",
    "gap_dict = {}\n",
    "with torch.no_grad():\n",
    "    for i,batch in tqdm(enumerate(loader)):\n",
    "        if(len(batch)==5):\n",
    "            video,audio,text,text_mask,label = batch\n",
    "            video = video.to(device)\n",
    "            audio = audio.to(device)\n",
    "            text = text.to(device)\n",
    "            text_mask = text_mask.to(device)\n",
    "            label = label.to(device)\n",
    "        else:\n",
    "            video,audio,text,label = batch\n",
    "            video = video.to(device)\n",
    "            audio = audio.to(device)\n",
    "            text = text.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "        inputs_dict={}\n",
    "        inputs_dict['video'] = video\n",
    "        inputs_dict['audio'] = audio\n",
    "        inputs_dict['text'] = text \n",
    "        if(len(batch)==5):\n",
    "            inputs_dict['attention_mask'] = text_mask\n",
    "        else:\n",
    "            inputs_dict['attention_mask'] = None\n",
    "\n",
    "        B = video.shape[0]\n",
    "        pred_dict_ensemble = {}\n",
    "        for modal_name in (modal_name_list+['fusion']):\n",
    "            pred_dict_ensemble['tagging_output_'+modal_name] = {}\n",
    "            pred_dict_ensemble['tagging_output_'+modal_name]['predictions'] = torch.zeros(B,82).cuda()\n",
    "\n",
    "        for i,model in enumerate(models):\n",
    "            pred_dict = model(inputs_dict)\n",
    "            for modal_name in (modal_name_list+['fusion']):\n",
    "                pred_dict_ensemble['tagging_output_'+modal_name]['predictions'] += model_weights[i]*pred_dict['tagging_output_'+modal_name]['predictions']\n",
    "        '''\n",
    "        for modal_name in (modal_name_list+['fusion']):\n",
    "            pred_dict_ensemble['tagging_output_'+modal_name]['predictions'] = pred_dict_ensemble['tagging_output_'+modal_name]['predictions']/len(models)\n",
    "        '''\n",
    "        for index,modal_name in enumerate(modal_name_list+['fusion']):\n",
    "            pred = pred_dict_ensemble['tagging_output_'+modal_name]\n",
    "            pred = pred['predictions'].detach().cpu().numpy()\n",
    "            val_label = label.cpu().numpy()\n",
    "            gap = train_util.calculate_gap(pred, val_label)\n",
    "            evl_metrics[index].accumulate(pred, val_label, loss=0)\n",
    "    for index,modal_name in enumerate(modal_name_list+['fusion']):\n",
    "        metric_dict[modal_name] = evl_metrics[index].get()\n",
    "        gap_dict[modal_name] = metric_dict[modal_name]['gap']\n",
    "    print(gap_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_root = '../dataset/videos/video_5k/train_5k/'\n",
    "\n",
    "# ########## get train_5k_A video file lists\n",
    "videos_train_5k_A_dir = os.path.join(dataset_root, 'videos/train_5k_A')\n",
    "videos_train_5k_A_files = [os.path.join(videos_train_5k_A_dir, f) for f in os.listdir(videos_train_5k_A_dir) if os.path.isfile(os.path.join(videos_train_5k_A_dir, f))]\n",
    "\n",
    "print(\"videos_train_5k_A_dir= {}\".format(videos_train_5k_A_dir))\n",
    "print(\"len(videos/train_5k_A)= {}\".format(len(videos_train_5k_A_files)))\n",
    "\n",
    "# ########## display\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# video\n",
    "test_video_path = videos_train_5k_A_files[3000]\n",
    "print(test_video_path)\n",
    "print(os.path.exists(test_video_path))\n",
    "html_str = '''\n",
    "<video controls width=\\\"500\\\" height=\\\"500\\\" src=\\\"{}\\\">animation</video>\n",
    "'''.format(test_video_path)\n",
    "print(html_str)\n",
    "display(HTML(html_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_py3",
   "language": "python",
   "name": "conda_pytorch_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
