{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "config_path = './config/config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/ipykernel/__main__.py:1: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "config = yaml.load(open(config_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset和dataloader 定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import dataloader.dataloader\n",
    "importlib.reload(dataloader.dataloader)\n",
    "from dataloader.dataloader import MultimodaFeaturesDataset\n",
    "dataset = MultimodaFeaturesDataset(config['DatasetConfig'],job='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(dataset,num_workers=0,batch_size=config['DatasetConfig']['batch_size'],collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteor = iter(loader)\n",
    "batch = next(iteor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "video,audio,text,text_mask,label = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization: Kaiming\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import src.model.baseline_model\n",
    "importlib.reload(src.model.baseline_model)\n",
    "from src.model.baseline_model import Baseline\n",
    "model = Baseline(config['ModelConfig'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_dict={}\n",
    "inputs_dict['video'] = video\n",
    "inputs_dict['audio'] = audio\n",
    "inputs_dict['text'] = text\n",
    "inputs_dict['attention_mask'] = text_mask\n",
    "prob_dict = model(inputs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['video_loss_weight', 'tagging_output_video', 'text_loss_weight', 'tagging_output_text', 'tagging_output_fusion', 'video_embedding'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 82])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_dict['tagging_output_fusion']['predictions'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "preds = torch.zeros((32,82))\n",
    "for modal_name in model.modal_name_list:    \n",
    "    preds += prob_dict['tagging_output_' + modal_name]['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 82])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([82])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(preds,dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1311.8595, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.stack(preds))/len(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.loss.loss_compute\n",
    "importlib.reload(src.loss.loss_compute)\n",
    "from src.loss.loss_compute import SimpleLossCompute\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "criterion = nn.BCELoss(reduction='sum')# sum应该没问题吧\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_compute = SimpleLossCompute(criterion,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (1):   0%|          | 0/157 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video\n",
      "after nextvlad shape:  torch.Size([32, 16384])\n",
      "after SE shape:  torch.Size([32, 1024])\n",
      "audio\n",
      "after nextvlad shape:  torch.Size([32, 1024])\n",
      "after SE shape:  torch.Size([32, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (1):   0%|          | 0/157 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import src.loop.run_epoch\n",
    "importlib.reload(src.loop.run_epoch)\n",
    "from src.loop.run_epoch import training_loop\n",
    "training_loop(model, loader, loss_compute, epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "for batch in loader:\n",
    "    video,audio,text,label = batch\n",
    "    inputs_dict={}\n",
    "    inputs_dict['video'] = video\n",
    "    inputs_dict['audio'] = audio\n",
    "    inputs_dict['text'] = text\n",
    "    inputs_dict['label'] = label\n",
    "    # 预测\n",
    "    pred = model(inputs_dict)\n",
    "    # 计算损失\n",
    "    #loss = loss_compute(pred['tagging_output_fusion']['predictions'],label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 1])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred['video_loss_weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 3\n",
    "kf = GroupKFold(n_splits = N_FOLDS)\n",
    "for fold,(train_idx,val_idx) in enumerate(kf.split(df_train)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _BaseKFold.split at 0x7f2db0fc5308>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf.split(range(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = []\n",
    "val_indices = []\n",
    "for fold,(train_idx,val_idx) in enumerate(kf.split(range(5000))):\n",
    "    print(fold)\n",
    "    train_indices.append(train_idx)\n",
    "    val_indices.append(val_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "validation_size = len(full_dataset) - train_size\n",
    "train_dataset, validation_dataset = random_split(full_dataset, [train_size, validation_size])\n",
    "\n",
    "#full_loader = DataLoader(full_dataset, batch_size=4,sampler = sampler_(full_dataset), pin_memory=True) \n",
    "#train_loader = DataLoader(train_dataset, batch_size=4, sampler = sampler_(train_dataset))\n",
    "#val_loader = DataLoader(validation_dataset, batch_size=1, sampler = sampler_(validation_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import dataloader.dataloader\n",
    "importlib.reload(dataloader.dataloader)\n",
    "from dataloader.dataloader import MultimodaFeaturesDataset\n",
    "full_dataset = MultimodaFeaturesDataset(config['DatasetConfig'],job='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting albumentations\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/03/58/63fb1d742dc42d9ba2800ea741de1f2bc6bb05548d8724aa84794042eaf2/albumentations-0.5.2-py3-none-any.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 6.6 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from albumentations) (1.18.5)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from albumentations) (5.3.1)\n",
      "Collecting opencv-python-headless>=4.1.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/cc/09/3ed889f37b1bb1dff85f10d91b1f9e8b8a812a7e8413c4e906c21aab9469/opencv_python_headless-4.5.2.52-cp36-cp36m-manylinux2014_x86_64.whl (38.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 38.2 MB 845 kB/s eta 0:00:011 eta 0:00:01\n",
      "\u001b[?25hCollecting imgaug>=0.4.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
      "\u001b[K     |████████████████████████████████| 948 kB 8.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from albumentations) (1.5.0)\n",
      "Collecting scikit-image>=0.16.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/0e/ba/53e1bfbdfd0f94514d71502e3acea494a8b4b57c457adbc333ef386485da/scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 1.8 MB/s eta 0:00:01     |█████▋                          | 2.2 MB 14.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Shapely\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/9d/18/557d4f55453fe00f59807b111cc7b39ce53594e13ada88e16738fb4ff7fb/Shapely-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 18.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opencv-python\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/ec/de/e5308044f192cfb10ebe394bf9c6f38f9d77a3f57328354e756633c068f9/opencv_python-4.5.2.52-cp36-cp36m-manylinux2014_x86_64.whl (51.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 51.0 MB 455 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations) (7.1.2)\n",
      "Collecting imageio\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/6e/57/5d899fae74c1752f52869b613a8210a2480e1a69688e65df6cb26117d45d/imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 8.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations) (2.2.2)\n",
      "Collecting networkx>=2.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/f3/b7/c7f488101c0bb5e4178f3cde416004280fd40262433496830de8a8c21613/networkx-2.5.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 54.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tifffile>=2019.7.26\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/42/6b/93a8ee61c6fbe20fa9c17928bd3b80484902b7fd454cecaffba42f5052cb/tifffile-2020.9.3-py3-none-any.whl (148 kB)\n",
      "\u001b[K     |████████████████████████████████| 148 kB 75.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyWavelets>=1.1.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/59/bb/d2b85265ec9fa3c1922210c9393d4cdf7075cc87cce6fe671d7455f80fbc/PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 55.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2020.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.2.0)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n",
      "Installing collected packages: opencv-python-headless, networkx, tifffile, PyWavelets, imageio, scikit-image, Shapely, opencv-python, imgaug, albumentations\n",
      "Successfully installed PyWavelets-1.1.1 Shapely-1.7.1 albumentations-0.5.2 imageio-2.9.0 imgaug-0.4.0 networkx-2.5.1 opencv-python-4.5.2.52 opencv-python-headless-4.5.2.52 scikit-image-0.17.2 tifffile-2020.9.3\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting transformers\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 7.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from transformers) (2.24.0)\n",
      "Collecting filelock\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/6b/38/0ed2670578d803cb14350c54adb2a79835870aa9e3ad2e732be7359cb0e8/regex-2021.4.4-cp36-cp36m-manylinux2014_x86_64.whl (722 kB)\n",
      "\u001b[K     |████████████████████████████████| 722 kB 7.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from transformers) (4.46.1)\n",
      "Collecting sacremoses\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 24.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/bf/20/3605db440db4f96d5ffd66b231a043ae451ec7e5e4d1a2fb6f20608006c4/tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 28.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from transformers) (1.7.0)\n",
      "Collecting huggingface-hub==0.0.8\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from sacremoses->transformers) (0.15.1)\n",
      "Collecting click\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/76/0a/b6c5f311e32aeb3b406e03c079ade51e905ea630fc19d1262a46249c1c86/click-8.0.1-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 23.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.1.0)\n",
      "Installing collected packages: dataclasses, filelock, regex, click, sacremoses, tokenizers, huggingface-hub, transformers\n",
      "Successfully installed click-8.0.1 dataclasses-0.8 filelock-3.0.12 huggingface-hub-0.0.8 regex-2021.4.4 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting tensorboard\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/44/f5/7feea02a3fb54d5db827ac4b822a7ba8933826b36de21880518250b8733a/tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/bd/24/11c3ea5a7e866bf2d97f0501d0b4b1c9bbeade102bb4b588f0d2919a5212/Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 22.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from tensorboard) (47.3.1.post20200616)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/35/d2/0a79bc7e201c1b38ce46d607eb9398dc362dff1b054c7bba8e4e195c2ed7/google_auth-1.30.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 52.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from tensorboard) (2.24.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/6e/33/1ae0f71395e618d6140fbbc9587cc3156591f748226075e0f7d6f9176522/Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 58.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.12.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from tensorboard) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from tensorboard) (3.12.2)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/60/f9/802efd84988bffd9f644c03b6e66fde8e76c3aa33db4279ddd11c5d61f4b/tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 67.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.4\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/92/c9/ef0fae29182d7a867d203f0eff8296b60da92098cc41db33a434f4be84bf/absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 21.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.24.3\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/0e/4b/c4e1839cdb3248e1006837e1d427e1843f80f2e6ba69f3af77f00bb51ac4/grpcio-1.38.0-cp36-cp36m-manylinux2014_x86_64.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 13.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/1a/c1/499e600ba0c618b451cd9c425ae1c177249940a2086316552fee7d86c954/tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 71.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/9d/d3/7541e89f1fc456eef157224f597a8bba22589db6369a03eaba68c11f07a0/google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from werkzeug>=0.11.15->tensorboard) (0.8)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 47.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/e9/93/0c0f002031f18b53af7a6166103c02b9c0667be528944137cc954ec921b3/rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (1.15.0)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/bf/28/c4f5796c67ad06bb91d98d543a5e01805c1ff065e08871f78e52d2a331ad/cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.7.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 16.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (3.1.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 22.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: werkzeug, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, markdown, tensorboard-data-server, absl-py, grpcio, tensorboard-plugin-wit, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard\n",
      "Successfully installed absl-py-0.12.0 cachetools-4.2.2 google-auth-1.30.1 google-auth-oauthlib-0.4.4 grpcio-1.38.0 markdown-3.3.4 oauthlib-3.1.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 werkzeug-2.0.1\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting torchtext==0.5\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 7.8 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchtext==0.5) (2.24.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchtext==0.5) (4.46.1)\n",
      "Requirement already satisfied: six in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchtext==0.5) (1.15.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchtext==0.5) (1.18.5)\n",
      "Requirement already satisfied: torch in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchtext==0.5) (1.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->torchtext==0.5) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->torchtext==0.5) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->torchtext==0.5) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->torchtext==0.5) (3.0.4)\n",
      "Installing collected packages: sentencepiece, torchtext\n",
      "Successfully installed sentencepiece-0.1.95 torchtext-0.5.0\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting jieba\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.2 MB 463 kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314478 sha256=85a1c6c0c72a93fe3442066bfe82ee00c2553532c665217c956e643a65cf8550\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/de/99/39/55dd43d023169a4464b9118a252e188367c3750c62526c46f3\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n"
     ]
    }
   ],
   "source": [
    "!./setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization: Kaiming\n",
      "train (0): 100%|██████████████████████████████| 141/141 [00:27<00:00,  5.05it/s]\n",
      "2e-05\n",
      "epoch(0)(audio):  0.632467349668176\n",
      "epoch(0)(fusion):  0.6264328776171949\n",
      "train (1): 100%|██████████████████████████████| 141/141 [00:26<00:00,  5.32it/s]\n",
      "4e-05\n",
      "train (2): 100%|██████████████████████████████| 141/141 [00:26<00:00,  5.33it/s]\n",
      "6e-05\n",
      "epoch(2)(audio):  0.6569888757579462\n",
      "epoch(2)(fusion):  0.6573209705746819\n",
      "train (3): 100%|██████████████████████████████| 141/141 [00:26<00:00,  5.28it/s]\n",
      "8e-05\n",
      "train (4): 100%|██████████████████████████████| 141/141 [00:26<00:00,  5.29it/s]\n",
      "0.0001\n",
      "epoch(4)(audio):  0.6544772896044209\n",
      "epoch(4)(fusion):  0.6595788999027771\n",
      "train (5): 100%|██████████████████████████████| 141/141 [00:26<00:00,  5.24it/s]\n",
      "0.0001\n",
      "train (6): 100%|██████████████████████████████| 141/141 [00:26<00:00,  5.24it/s]\n",
      "0.0001\n",
      "epoch(6)(audio):  0.6672585731823004\n",
      "epoch(6)(fusion):  0.6682582690920501\n",
      "train (7): 100%|██████████████████████████████| 141/141 [00:26<00:00,  5.25it/s]\n",
      "0.0001\n",
      "train (8): 100%|██████████████████████████████| 141/141 [00:26<00:00,  5.31it/s]\n",
      "0.0001\n",
      "epoch(8)(audio):  0.6536446824241222\n",
      "epoch(8)(fusion):  0.6590928124774115\n",
      "train (9): 100%|██████████████████████████████| 141/141 [00:26<00:00,  5.30it/s]\n",
      "0.0001\n",
      "train (10): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.30it/s]\n",
      "0.0001\n",
      "epoch(10)(audio):  0.6673235681038726\n",
      "epoch(10)(fusion):  0.6702280218454206\n",
      "train (11): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.32it/s]\n",
      "0.0001\n",
      "train (12): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.32it/s]\n",
      "0.0001\n",
      "epoch(12)(audio):  0.668071422992051\n",
      "epoch(12)(fusion):  0.6707842003586602\n",
      "train (13): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.29it/s]\n",
      "0.0001\n",
      "train (14): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.30it/s]\n",
      "0.0001\n",
      "epoch(14)(audio):  0.6678734775010744\n",
      "epoch(14)(fusion):  0.6749328166745454\n",
      "train (15): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.27it/s]\n",
      "0.0001\n",
      "train (16): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.25it/s]\n",
      "0.0001\n",
      "epoch(16)(audio):  0.6706692323246245\n",
      "epoch(16)(fusion):  0.6759734115013093\n",
      "train (17): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.28it/s]\n",
      "0.0001\n",
      "train (18): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.24it/s]\n",
      "0.0001\n",
      "epoch(18)(audio):  0.6707874181909783\n",
      "epoch(18)(fusion):  0.6747952469491886\n",
      "train (19): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.26it/s]\n",
      "0.0001\n",
      "train (20): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.34it/s]\n",
      "1e-05\n",
      "epoch(20)(audio):  0.6763445459840891\n",
      "epoch(20)(fusion):  0.6820174696396337\n",
      "train (21): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.23it/s]\n",
      "1e-05\n",
      "train (22): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.29it/s]\n",
      "1e-05\n",
      "epoch(22)(audio):  0.6793519717049901\n",
      "epoch(22)(fusion):  0.6835323489923553\n",
      "train (23): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.26it/s]\n",
      "1e-05\n",
      "train (24): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.33it/s]\n",
      "1e-05\n",
      "epoch(24)(audio):  0.6780258656829652\n",
      "epoch(24)(fusion):  0.6844807252985049\n",
      "train (25): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.28it/s]\n",
      "1e-05\n",
      "train (26): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.32it/s]\n",
      "1e-05\n",
      "epoch(26)(audio):  0.681537421079801\n",
      "epoch(26)(fusion):  0.686275067201339\n",
      "train (27): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.25it/s]\n",
      "1e-05\n",
      "train (28): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.26it/s]\n",
      "1e-05\n",
      "epoch(28)(audio):  0.6809904703731146\n",
      "epoch(28)(fusion):  0.6846736574857296\n",
      "train (29): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.31it/s]\n",
      "1e-05\n",
      "train (30): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.30it/s]\n",
      "1e-05\n",
      "epoch(30)(audio):  0.6801866863218508\n",
      "epoch(30)(fusion):  0.6861911194536893\n",
      "train (31): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.26it/s]\n",
      "1e-05\n",
      "train (32): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.27it/s]\n",
      "1e-05\n",
      "epoch(32)(audio):  0.6809136841846727\n",
      "epoch(32)(fusion):  0.686503067722771\n",
      "train (33): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.32it/s]\n",
      "1e-05\n",
      "train (34): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.25it/s]\n",
      "1e-05\n",
      "epoch(34)(audio):  0.6788108109834691\n",
      "epoch(34)(fusion):  0.6848687696314738\n",
      "train (35): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.29it/s]\n",
      "1e-05\n",
      "train (36): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.26it/s]\n",
      "1e-05\n",
      "epoch(36)(audio):  0.6789343761829\n",
      "epoch(36)(fusion):  0.6866245278789841\n",
      "train (37): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.25it/s]\n",
      "1e-05\n",
      "train (38): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.29it/s]\n",
      "1e-05\n",
      "epoch(38)(audio):  0.6799711303094542\n",
      "epoch(38)(fusion):  0.6863787606945417\n",
      "train (39): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.22it/s]\n",
      "1e-05\n",
      "train (40): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.24it/s]\n",
      "1.0000000000000002e-06\n",
      "epoch(40)(audio):  0.6797878885252013\n",
      "epoch(40)(fusion):  0.6867956418913337\n",
      "train (41): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.25it/s]\n",
      "1.0000000000000002e-06\n",
      "train (42): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.28it/s]\n",
      "1.0000000000000002e-06\n",
      "epoch(42)(audio):  0.6779581719212053\n",
      "epoch(42)(fusion):  0.6855206013614054\n",
      "train (43): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.31it/s]\n",
      "1.0000000000000002e-06\n",
      "train (44): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.33it/s]\n",
      "1.0000000000000002e-06\n",
      "epoch(44)(audio):  0.678874490560895\n",
      "epoch(44)(fusion):  0.6868507415178283\n",
      "train (45): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.27it/s]\n",
      "1.0000000000000002e-06\n",
      "train (46): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.23it/s]\n",
      "1.0000000000000002e-06\n",
      "epoch(46)(audio):  0.6789278697474985\n",
      "epoch(46)(fusion):  0.6864843390611075\n",
      "train (47): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.26it/s]\n",
      "1.0000000000000002e-06\n",
      "train (48): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.31it/s]\n",
      "1.0000000000000002e-06\n",
      "epoch(48)(audio):  0.6788571116347236\n",
      "epoch(48)(fusion):  0.6868129006672612\n",
      "train (49): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.31it/s]\n",
      "1.0000000000000002e-06\n",
      "train (50): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.29it/s]\n",
      "1.0000000000000002e-06\n",
      "epoch(50)(audio):  0.6785195217476662\n",
      "epoch(50)(fusion):  0.686260781689996\n",
      "train (51): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.29it/s]\n",
      "1.0000000000000002e-06\n",
      "train (52): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.29it/s]\n",
      "1.0000000000000002e-06\n",
      "epoch(52)(audio):  0.6790214200182311\n",
      "epoch(52)(fusion):  0.6861065183546667\n",
      "train (53): 100%|█████████████████████████████| 141/141 [00:26<00:00,  5.26it/s]\n",
      "1.0000000000000002e-06\n",
      "train (54):  33%|█████████▊                    | 46/141 [00:21<00:10,  9.21it/s]^C\n"
     ]
    }
   ],
   "source": [
    "!python -W ignore main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -W ignore end2end_main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference.py:18: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config = yaml.load(open(config_path))\n",
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "  0%|                                                  | 0/5000 [00:00<?, ?it/s]/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "100%|███████████████████████████████████████| 5000/5000 [03:33<00:00, 23.41it/s]\n"
     ]
    }
   ],
   "source": [
    "!python inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "train (0): 100%|██████████████████████████████| 142/142 [00:27<00:00,  5.10it/s]\n",
      "10.946835289538747\n",
      "train (1): 100%|██████████████████████████████| 142/142 [00:25<00:00,  5.51it/s]\n",
      "7.524781791257187\n",
      "train (2): 100%|██████████████████████████████| 142/142 [00:26<00:00,  5.46it/s]\n",
      "5.306214795985692\n",
      "train (3): 100%|██████████████████████████████| 142/142 [00:26<00:00,  5.39it/s]\n",
      "3.967252230980027\n",
      "train (4): 100%|██████████████████████████████| 142/142 [00:26<00:00,  5.40it/s]\n",
      "3.0418758014558067\n",
      "train (5): 100%|██████████████████████████████| 142/142 [00:25<00:00,  5.47it/s]\n",
      "2.625848537599537\n",
      "train (6): 100%|██████████████████████████████| 142/142 [00:26<00:00,  5.44it/s]\n",
      "2.312776572267774\n",
      "train (7): 100%|██████████████████████████████| 142/142 [00:25<00:00,  5.51it/s]\n",
      "2.0628988205547065\n",
      "train (8): 100%|██████████████████████████████| 142/142 [00:26<00:00,  5.44it/s]\n",
      "1.9037314032165098\n",
      "train (9): 100%|██████████████████████████████| 142/142 [00:26<00:00,  5.43it/s]\n",
      "1.784770817404062\n",
      "train (10): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.49it/s]\n",
      "1.640711270587545\n",
      "train (11): 100%|█████████████████████████████| 142/142 [00:26<00:00,  5.37it/s]\n",
      "1.5876181683909725\n",
      "train (12): 100%|█████████████████████████████| 142/142 [00:27<00:00,  5.24it/s]\n",
      "1.5615364665716467\n",
      "train (13): 100%|█████████████████████████████| 142/142 [00:26<00:00,  5.37it/s]\n",
      "1.4792130492942435\n",
      "train (14): 100%|█████████████████████████████| 142/142 [00:26<00:00,  5.44it/s]\n",
      "1.443764888064962\n",
      "train (15): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.49it/s]\n",
      "1.378556965522363\n",
      "train (16): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.49it/s]\n",
      "1.4031092134160055\n",
      "train (17): 100%|█████████████████████████████| 142/142 [00:26<00:00,  5.44it/s]\n",
      "1.3759777512348874\n",
      "train (18): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.48it/s]\n",
      "1.362973349195131\n",
      "train (19): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.50it/s]\n",
      "1.3386385675886987\n",
      "train (20): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.49it/s]\n",
      "1.2956474247113081\n",
      "train (21): 100%|█████████████████████████████| 142/142 [00:26<00:00,  5.44it/s]\n",
      "1.276390618001911\n",
      "train (22): 100%|█████████████████████████████| 142/142 [00:26<00:00,  5.27it/s]\n",
      "1.2669548425875918\n",
      "train (23): 100%|█████████████████████████████| 142/142 [00:26<00:00,  5.44it/s]\n",
      "1.274584153168638\n",
      "train (24): 100%|█████████████████████████████| 142/142 [00:26<00:00,  5.44it/s]\n",
      "1.2628839129293468\n",
      "train (25): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.47it/s]\n",
      "1.2572355329150884\n",
      "train (26): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.49it/s]\n",
      "1.264695036998937\n",
      "train (27): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.62it/s]\n",
      "1.2475354822588638\n",
      "train (28): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.53it/s]\n",
      "1.2592588365077972\n",
      "train (29): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.50it/s]\n",
      "1.1852773453148318\n",
      "train (30): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.48it/s]\n",
      "1.234607223893555\n",
      "train (31): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.48it/s]\n",
      "1.2318926281492475\n",
      "train (32): 100%|█████████████████████████████| 142/142 [00:26<00:00,  5.45it/s]\n",
      "1.2130731566691062\n",
      "train (33): 100%|█████████████████████████████| 142/142 [00:26<00:00,  5.46it/s]\n",
      "1.1978330662552739\n",
      "train (34): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.51it/s]\n",
      "1.2400807927192097\n",
      "train (35): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.52it/s]\n",
      "1.2324566790755367\n",
      "train (36): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.52it/s]\n",
      "1.208794293689056\n",
      "train (37): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.48it/s]\n",
      "1.214623209456323\n",
      "train (38): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.48it/s]\n",
      "1.2071052126481499\n",
      "train (39): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.50it/s]\n",
      "1.2259673383873952\n",
      "train (40): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.47it/s]\n",
      "1.185798982919102\n",
      "train (41): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.50it/s]\n",
      "1.210378648949341\n",
      "train (42): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.49it/s]\n",
      "1.2351521497041407\n",
      "train (43): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.53it/s]\n",
      "1.210829283150149\n",
      "train (44): 100%|█████████████████████████████| 142/142 [00:26<00:00,  5.42it/s]\n",
      "1.2189992976860262\n",
      "train (45): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.48it/s]\n",
      "1.2340216212709185\n",
      "train (46): 100%|█████████████████████████████| 142/142 [00:26<00:00,  5.39it/s]\n",
      "1.2326995251883923\n",
      "train (47): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.49it/s]\n",
      "1.2058705104068972\n",
      "train (48): 100%|█████████████████████████████| 142/142 [00:26<00:00,  5.44it/s]\n",
      "1.2148618836637954\n",
      "train (49): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.49it/s]\n",
      "1.1614535908463974\n",
      "train (50): 100%|█████████████████████████████| 142/142 [00:25<00:00,  5.50it/s]\n",
      "1.200235990571304\n"
     ]
    }
   ],
   "source": [
    "!python -W ignore enhance_main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCNN 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "# from munch import Munch\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5,6,7'\n",
    "import glob\n",
    "from dataloader.dataloader import MultimodaFeaturesDataset,Datasetfortextcnn\n",
    "from src.loss.loss_compute import SimpleLossCompute\n",
    "from src.model.baseline_model import Baseline\n",
    "from src.loop.run_epoch import training_loop,validating_loop\n",
    "from torch.utils import tensorboard as tensorboard\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/ipykernel/__main__.py:2: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "config_path = './config/config.yaml'\n",
    "config = yaml.load(open(config_path))\n",
    "device_ids = [0]\n",
    "train_dataset = Datasetfortextcnn(config['DatasetConfig'],job='training')\n",
    "train_loader = DataLoader(train_dataset,num_workers=8,\n",
    "                          batch_size=config['DatasetConfig']['batch_size']*len(device_ids),\n",
    "                          shuffle=True,\n",
    "                          pin_memory=True,\n",
    "                          collate_fn=train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(train_loader)\n",
    "batch = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video,audio,text,label = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_dict={}\n",
    "inputs_dict['video'] = video\n",
    "inputs_dict['audio'] = audio\n",
    "inputs_dict['text'] = text \n",
    "inputs_dict['attention_mask'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c402f1633a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ModelConfig'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/notebook/TAAC-2021/src/model/baseline_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_config)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodal\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_head_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_head_params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodal\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_head_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_cnn_params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmodal\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodal\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_head_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_head_params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebook/TAAC-2021/src/text_head/__init__.py\u001b[0m in \u001b[0;36mget_instance\u001b[0;34m(name, paramters)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'BERT'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBERT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'TextCnn'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTextCnn\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparamters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/notebook/TAAC-2021/src/text_head/fine_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embed_num, embed_dim, feature_dim, kernel_num, kernel_sizes, embedding_path, embedding_name, dropout)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkernel_sizes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, embeddings, freeze, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;36m4.0000\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m5.1000\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m6.3000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \"\"\"\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;34m'Embeddings parameter is expected to be 2-dimensional'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "model = Baseline(config['ModelConfig'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(inputs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "embedding_path = '../pretrained/'\n",
    "embedding_name = 'sgns.baidubaike.bigram-char'\n",
    "embed = nn.Embedding(14769, 300)\n",
    "vectors = torchtext.vocab.Vectors(name=embedding_name, cache=embedding_path)\n",
    "embed = embed.from_pretrained(vectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vectors at 0x7fa1b40c7ac8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model_path = '../checkpoint/0527/01/30.pt'\n",
    "extractor = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {k:v for k,v in extractor.items() if k in model_dict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/ipykernel/__main__.py:18: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "# from munch import Munch\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5,6,7'\n",
    "import glob\n",
    "from dataloader.dataloader import MultimodaFeaturesDataset,DualDataset\n",
    "from src.loss.loss_compute import SimpleLossCompute,ContrastiveLossCompute\n",
    "from src.model.baseline_model import Baseline,Dual\n",
    "from src.loop.run_epoch import training_loop,validating_loop,dual_training_loop\n",
    "from torch.utils import tensorboard as tensorboard\n",
    "from datetime import datetime\n",
    "\n",
    "config_path = './config/config.yaml'\n",
    "config = yaml.load(open(config_path))\n",
    "device_ids = [0]\n",
    "# 定义数据集并封装dataloader\n",
    "train_dataset = DualDataset(config['DatasetConfig'],job='training')\n",
    "train_dataset.data_num_per_sample = 4\n",
    "train_dataset.meta_path = '/home/tione/notebook/dataset/tagging/GroundTruth/datafile/self_sup_VAT.txt'\n",
    "train_loader = DataLoader(train_dataset,num_workers=8,\n",
    "                          batch_size=64,\n",
    "                          shuffle=True,\n",
    "                          pin_memory=True,\n",
    "                          collate_fn=train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "ite = iter(train_loader)\n",
    "batch = next(ite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/ipykernel/__main__.py:19: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization: Kaiming\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "# from munch import Munch\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5,6,7'\n",
    "import glob\n",
    "from dataloader.dataloader import MultimodaFeaturesDataset,Datasetfortextcnn\n",
    "from src.loss.loss_compute import SimpleLossCompute\n",
    "from src.model.baseline_model import Baseline\n",
    "from src.loop.run_epoch import training_loop,validating_loop\n",
    "from torch.utils import tensorboard as tensorboard\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import random\n",
    "config_path = './config/config.yaml'\n",
    "config = yaml.load(open(config_path))\n",
    "model = Baseline(config['ModelConfig'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_state_dict = {}\n",
    "for k,v in model.state_dict().items():\n",
    "    key = k.split('.')\n",
    "    if(key[0]!='classifier_dict'):\n",
    "        update_state_dict[k] = v\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_py3",
   "language": "python",
   "name": "conda_pytorch_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
