{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting albumentations\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/03/58/63fb1d742dc42d9ba2800ea741de1f2bc6bb05548d8724aa84794042eaf2/albumentations-0.5.2-py3-none-any.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 6.6 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting scikit-image>=0.16.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/0e/ba/53e1bfbdfd0f94514d71502e3acea494a8b4b57c457adbc333ef386485da/scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 477 kB/s eta 0:00:01�███| 12.4 MB 477 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from albumentations) (1.18.5)\n",
      "Collecting imgaug>=0.4.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
      "\u001b[K     |████████████████████████████████| 948 kB 104.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from albumentations) (1.5.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from albumentations) (5.3.1)\n",
      "Collecting opencv-python-headless>=4.1.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/cc/09/3ed889f37b1bb1dff85f10d91b1f9e8b8a812a7e8413c4e906c21aab9469/opencv_python_headless-4.5.2.52-cp36-cp36m-manylinux2014_x86_64.whl (38.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 38.2 MB 451 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting imageio>=2.3.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/6e/57/5d899fae74c1752f52869b613a8210a2480e1a69688e65df6cb26117d45d/imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 11.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx>=2.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/f3/b7/c7f488101c0bb5e4178f3cde416004280fd40262433496830de8a8c21613/networkx-2.5.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 90.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tifffile>=2019.7.26\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/42/6b/93a8ee61c6fbe20fa9c17928bd3b80484902b7fd454cecaffba42f5052cb/tifffile-2020.9.3-py3-none-any.whl (148 kB)\n",
      "\u001b[K     |████████████████████████████████| 148 kB 23.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from scikit-image>=0.16.1->albumentations) (2.2.2)\n",
      "Collecting PyWavelets>=1.1.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/59/bb/d2b85265ec9fa3c1922210c9393d4cdf7075cc87cce6fe671d7455f80fbc/PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 16.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
      "Collecting Shapely\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/9d/18/557d4f55453fe00f59807b111cc7b39ce53594e13ada88e16738fb4ff7fb/Shapely-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 67.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n",
      "Collecting opencv-python\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/ec/de/e5308044f192cfb10ebe394bf9c6f38f9d77a3f57328354e756633c068f9/opencv_python-4.5.2.52-cp36-cp36m-manylinux2014_x86_64.whl (51.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 51.0 MB 441 kB/s eta 0:00:011��████▊                     | 17.1 MB 956 kB/s eta 0:00:36\n",
      "\u001b[?25hRequirement already satisfied: decorator<5,>=4.3 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2020.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.2.0)\n",
      "Installing collected packages: imageio, networkx, tifffile, PyWavelets, scikit-image, Shapely, opencv-python, imgaug, opencv-python-headless, albumentations\n",
      "Successfully installed PyWavelets-1.1.1 Shapely-1.7.1 albumentations-0.5.2 imageio-2.9.0 imgaug-0.4.0 networkx-2.5.1 opencv-python-4.5.2.52 opencv-python-headless-4.5.2.52 scikit-image-0.17.2 tifffile-2020.9.3\n"
     ]
    }
   ],
   "source": [
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.7.12\n",
      "  latest version: 4.10.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda/envs/pytorch_py3\n",
      "\n",
      "  added / updated specs:\n",
      "    - ipywidgets\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    attrs-21.2.0               |     pyhd8ed1ab_0          44 KB  conda-forge\n",
      "    bleach-3.3.0               |     pyh44b312d_0         111 KB  conda-forge\n",
      "    ca-certificates-2020.12.5  |       ha878542_0         137 KB  conda-forge\n",
      "    certifi-2020.12.5          |   py36h5fab9bb_1         143 KB  conda-forge\n",
      "    defusedxml-0.7.1           |     pyhd8ed1ab_0          23 KB  conda-forge\n",
      "    importlib-metadata-4.0.1   |   py36h5fab9bb_0          30 KB  conda-forge\n",
      "    ipywidgets-7.6.3           |     pyhd3deb0d_0         101 KB  conda-forge\n",
      "    jinja2-2.11.3              |     pyh44b312d_0          93 KB  conda-forge\n",
      "    jsonschema-3.2.0           |     pyhd8ed1ab_3          45 KB  conda-forge\n",
      "    jupyterlab_widgets-1.0.0   |     pyhd8ed1ab_1         130 KB  conda-forge\n",
      "    markupsafe-1.1.1           |   py36he6145b8_2          27 KB  conda-forge\n",
      "    mistune-0.8.4              |py36h1d69622_1002          54 KB  conda-forge\n",
      "    nbconvert-5.6.1            |   py36h9f0ad1d_1         487 KB  conda-forge\n",
      "    nbformat-5.1.3             |     pyhd8ed1ab_0          47 KB  conda-forge\n",
      "    notebook-5.7.10            |   py36h9f0ad1d_0         7.6 MB  conda-forge\n",
      "    packaging-20.9             |     pyh44b312d_0          35 KB  conda-forge\n",
      "    pandoc-2.13                |       h7f98852_0        11.3 MB  conda-forge\n",
      "    pandocfilters-1.4.2        |             py_1           9 KB  conda-forge\n",
      "    prometheus_client-0.10.1   |     pyhd8ed1ab_0          46 KB  conda-forge\n",
      "    pyrsistent-0.17.3          |   py36h1d69622_1          89 KB  conda-forge\n",
      "    send2trash-1.5.0           |             py_0          12 KB  conda-forge\n",
      "    terminado-0.10.0           |   py36h5fab9bb_0          26 KB  conda-forge\n",
      "    testpath-0.5.0             |     pyhd8ed1ab_0          86 KB  conda-forge\n",
      "    typing_extensions-3.7.4.3  |             py_0          25 KB  conda-forge\n",
      "    webencodings-0.5.1         |             py_1          12 KB  conda-forge\n",
      "    widgetsnbextension-3.5.1   |   py36h5fab9bb_4         1.8 MB  conda-forge\n",
      "    zipp-3.4.1                 |     pyhd8ed1ab_0          11 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        22.5 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  attrs              conda-forge/noarch::attrs-21.2.0-pyhd8ed1ab_0\n",
      "  bleach             conda-forge/noarch::bleach-3.3.0-pyh44b312d_0\n",
      "  defusedxml         conda-forge/noarch::defusedxml-0.7.1-pyhd8ed1ab_0\n",
      "  importlib-metadata conda-forge/linux-64::importlib-metadata-4.0.1-py36h5fab9bb_0\n",
      "  ipywidgets         conda-forge/noarch::ipywidgets-7.6.3-pyhd3deb0d_0\n",
      "  jinja2             conda-forge/noarch::jinja2-2.11.3-pyh44b312d_0\n",
      "  jsonschema         conda-forge/noarch::jsonschema-3.2.0-pyhd8ed1ab_3\n",
      "  jupyterlab_widgets conda-forge/noarch::jupyterlab_widgets-1.0.0-pyhd8ed1ab_1\n",
      "  markupsafe         conda-forge/linux-64::markupsafe-1.1.1-py36he6145b8_2\n",
      "  mistune            conda-forge/linux-64::mistune-0.8.4-py36h1d69622_1002\n",
      "  nbconvert          conda-forge/linux-64::nbconvert-5.6.1-py36h9f0ad1d_1\n",
      "  nbformat           conda-forge/noarch::nbformat-5.1.3-pyhd8ed1ab_0\n",
      "  notebook           conda-forge/linux-64::notebook-5.7.10-py36h9f0ad1d_0\n",
      "  packaging          conda-forge/noarch::packaging-20.9-pyh44b312d_0\n",
      "  pandoc             conda-forge/linux-64::pandoc-2.13-h7f98852_0\n",
      "  pandocfilters      conda-forge/noarch::pandocfilters-1.4.2-py_1\n",
      "  prometheus_client  conda-forge/noarch::prometheus_client-0.10.1-pyhd8ed1ab_0\n",
      "  pyrsistent         conda-forge/linux-64::pyrsistent-0.17.3-py36h1d69622_1\n",
      "  send2trash         conda-forge/noarch::send2trash-1.5.0-py_0\n",
      "  terminado          conda-forge/linux-64::terminado-0.10.0-py36h5fab9bb_0\n",
      "  testpath           conda-forge/noarch::testpath-0.5.0-pyhd8ed1ab_0\n",
      "  typing_extensions  conda-forge/noarch::typing_extensions-3.7.4.3-py_0\n",
      "  webencodings       conda-forge/noarch::webencodings-0.5.1-py_1\n",
      "  widgetsnbextension conda-forge/linux-64::widgetsnbextension-3.5.1-py36h5fab9bb_4\n",
      "  zipp               conda-forge/noarch::zipp-3.4.1-pyhd8ed1ab_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                      2020.6.20-hecda079_0 --> 2020.12.5-ha878542_0\n",
      "  certifi                          2020.6.20-py36h9f0ad1d_0 --> 2020.12.5-py36h5fab9bb_1\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "zipp-3.4.1           | 11 KB     | ##################################### | 100% \n",
      "pyrsistent-0.17.3    | 89 KB     | ##################################### | 100% \n",
      "testpath-0.5.0       | 86 KB     | ##################################### | 100% \n",
      "importlib-metadata-4 | 30 KB     | ##################################### | 100% \n",
      "jsonschema-3.2.0     | 45 KB     | ##################################### | 100% \n",
      "bleach-3.3.0         | 111 KB    | ##################################### | 100% \n",
      "nbformat-5.1.3       | 47 KB     | ##################################### | 100% \n",
      "pandocfilters-1.4.2  | 9 KB      | ##################################### | 100% \n",
      "pandoc-2.13          | 11.3 MB   | ##################################### | 100% \n",
      "nbconvert-5.6.1      | 487 KB    | ##################################### | 100% \n",
      "terminado-0.10.0     | 26 KB     | ##################################### | 100% \n",
      "mistune-0.8.4        | 54 KB     | ##################################### | 100% \n",
      "notebook-5.7.10      | 7.6 MB    | ##################################### | 100% \n",
      "typing_extensions-3. | 25 KB     | ##################################### | 100% \n",
      "ipywidgets-7.6.3     | 101 KB    | ##################################### | 100% \n",
      "ca-certificates-2020 | 137 KB    | ##################################### | 100% \n",
      "widgetsnbextension-3 | 1.8 MB    | ##################################### | 100% \n",
      "jinja2-2.11.3        | 93 KB     | ##################################### | 100% \n",
      "certifi-2020.12.5    | 143 KB    | ##################################### | 100% \n",
      "packaging-20.9       | 35 KB     | ##################################### | 100% \n",
      "attrs-21.2.0         | 44 KB     | ##################################### | 100% \n",
      "markupsafe-1.1.1     | 27 KB     | ##################################### | 100% \n",
      "webencodings-0.5.1   | 12 KB     | ##################################### | 100% \n",
      "defusedxml-0.7.1     | 23 KB     | ##################################### | 100% \n",
      "jupyterlab_widgets-1 | 130 KB    | ##################################### | 100% \n",
      "send2trash-1.5.0     | 12 KB     | ##################################### | 100% \n",
      "prometheus_client-0. | 46 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: | b'Enabling notebook extension jupyter-js-widgets/extension...\\n      - Validating: \\x1b[32mOK\\x1b[0m\\n'\n",
      "done\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!conda install -y -c conda-forge ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting transformers\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/6b/38/0ed2670578d803cb14350c54adb2a79835870aa9e3ad2e732be7359cb0e8/regex-2021.4.4-cp36-cp36m-manylinux2014_x86_64.whl (722 kB)\n",
      "\u001b[K     |████████████████████████████████| 722 kB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from transformers) (2.24.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/b0/ec/029100a156a3671a385c78898cf16896f5c29a867d310895b4e221b722f5/tokenizers-0.10.2-cp36-cp36m-manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 16.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from transformers) (4.0.1)\n",
      "Collecting huggingface-hub==0.0.8\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from transformers) (4.46.1)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Collecting filelock\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: six in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from sacremoses->transformers) (0.15.1)\n",
      "Collecting click\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/76/0a/b6c5f311e32aeb3b406e03c079ade51e905ea630fc19d1262a46249c1c86/click-8.0.1-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 15.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\n",
      "Installing collected packages: regex, tokenizers, click, sacremoses, filelock, huggingface-hub, dataclasses, transformers\n",
      "Successfully installed click-8.0.1 dataclasses-0.8 filelock-3.0.12 huggingface-hub-0.0.8 regex-2021.4.4 sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "def frame_iterator_list(filename, every_ms=1000, max_num_frames=300):\n",
    "    video_capture = cv2.VideoCapture()\n",
    "    if not video_capture.open(filename):\n",
    "        print(sys.stderr, 'Error: Cannot open video file ' + filename)\n",
    "        return\n",
    "    last_ts = -99999  # The timestamp of last retrieved frame.\n",
    "    num_retrieved = 0\n",
    "\n",
    "    frame_all = []\n",
    "    while num_retrieved < max_num_frames:\n",
    "        # Skip frames\n",
    "        while video_capture.get(cv2.CAP_PROP_POS_MSEC) < every_ms + last_ts:\n",
    "            if not video_capture.read()[0]:\n",
    "                return np.array(frame_all)\n",
    "\n",
    "        last_ts = video_capture.get(cv2.CAP_PROP_POS_MSEC)\n",
    "        has_frames, frame = video_capture.read()\n",
    "        if not has_frames:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_all.append(frame[:, :, ::-1])\n",
    "        num_retrieved += 1\n",
    "\n",
    "    return np.array(frame_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms as T\n",
    "import torchvision.models as models\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "transform = T.Compose([T.Resize(256), T.CenterCrop(224), T.ToTensor()])\n",
    "A_transform = A.Compose([\n",
    "            # Resize(CFG.image_size, CFG.image_size),\n",
    "            A.RandomResizedCrop(256, 256),\n",
    "            A.Transpose(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(p=0.5),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],  # mean on ImageNet\n",
    "                std=[0.229, 0.224, 0.225],  # std on ImageNet\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "video_path = '../dataset/videos/video_5k/train_5k/'\n",
    "file_names = os.listdir(video_path)\n",
    "save_path = '../dataset/frames/train_5k/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = file_names[0][:-4]\n",
    "save_name = save_path+video_id+'.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4c31beb71ec7480c4e4b05f22d27d472.mp4 is exit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for frame in frame_list:\n",
    "    video_id = file_names[0][:-4]\n",
    "    save_name = save_path+video_id+'.npy'\n",
    "    if(os.path.exists(save_name)):\n",
    "        print(f'{file_names[0]} is exit')\n",
    "        break\n",
    "    frame_list = frame_iterator_list(video_path+file_names[0],every_ms=1000)\n",
    "    np.save(save_name,frame_list)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every_ms 指的是每 every_ms 提取 1帧，every_ms=1000时，每秒提取 1帧\n",
    "frame_list = frame_iterator_list(video_path+file_names[0],every_ms=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 92 ms, sys: 12 ms, total: 104 ms\n",
      "Wall time: 103 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "video_processed = []\n",
    "for frame in frame_list:\n",
    "    augmented = A_transform(image=frame)\n",
    "    video_processed.append(augmented['image'].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video = torch.cat(video_processed,dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([56, 3, 256, 256])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Resnet, self).__init__()\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.model.conv1(input)\n",
    "        output = self.model.bn1(output)\n",
    "        output = self.model.relu(output)\n",
    "        output = self.model.maxpool(output)\n",
    "        output = self.model.layer1(output)\n",
    "        output = self.model.layer2(output)\n",
    "        output = self.model.layer3(output)\n",
    "        output = self.model.layer4(output)\n",
    "        output = self.model.avgpool(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /home/tione/.cache/torch/checkpoints/resnet50-19c8e357.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a5cd2c5cc643548c9b76bae83641b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Resnet(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Resnet()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 1280, 3)\n"
     ]
    }
   ],
   "source": [
    "for frame in frame_list:\n",
    "    augmented = A_transform(image = frame)\n",
    "    frame = augmented['image']\n",
    "    pred = model(frame.unsqueeze(0))\n",
    "    pred = pred.view(-1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"./a\", frame_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 854 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = np.load('./a.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 748 ms, sys: 68 ms, total: 816 ms\n",
      "Wall time: 816 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('a.npz','rb') as f:\n",
    "    a = np.load(f)\n",
    "    b = a['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 720, 1280, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 端到端训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import os\n",
    "import linecache\n",
    "import re\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import cv2\n",
    "import numpy as np\n",
    "class MultimodaRawDataset(Dataset):\n",
    "\n",
    "    def __init__(self,dataset_config,job='training'):\n",
    "        \n",
    "        self.text_max_len = dataset_config['text_max_len']\n",
    "        self.device = dataset_config['device']\n",
    "        self.data_num_per_sample = 4\n",
    "        if(job=='training'):\n",
    "            self.meta_path = dataset_config['train_raw_data_path']\n",
    "        elif(job=='valdation'):\n",
    "            self.meta_path = dataset_config['val_raw_data_path']\n",
    "        else:\n",
    "            self.meta_path = dataset_config['test_data_path']\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(dataset_config['bert_path'])\n",
    "        self.label2id = {}\n",
    "        with open(dataset_config['label_id_path'],'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip('\\r\\n')\n",
    "                line = line.split('\\t')\n",
    "                self.label2id[line[0]] = int(line[1])\n",
    "        self.A_transform = A.Compose([\n",
    "                # Resize(CFG.image_size, CFG.image_size),\n",
    "                A.RandomResizedCrop(256, 256),\n",
    "                A.Transpose(p=0.5),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "                A.ShiftScaleRotate(p=0.5),\n",
    "                A.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],  # mean on ImageNet\n",
    "                    std=[0.229, 0.224, 0.225],  # std on ImageNet\n",
    "                ),\n",
    "                ToTensorV2(),\n",
    "            ])\n",
    "    def __getitem__(self, index):\n",
    "        # 1. 从train.txt读取对应 idx 的path\n",
    "        data_list = [] # 存储对于index的各个模态数据的路径和样本标签\n",
    "        for line_i in range(self.data_num_per_sample*index+1,self.data_num_per_sample*(index+1)):\n",
    "            line = linecache.getline(self.meta_path,line_i)\n",
    "            line = line.strip('\\r\\n')\n",
    "            data_list.append(line)\n",
    "        #print(data_list)\n",
    "        video,text_ids,text_attention_mask,label_ids = self.preprocess(data_list)\n",
    "        return video,text_ids,text_attention_mask,label_ids\n",
    "    def __len__(self):\n",
    "        # TODO 不能固定长度\n",
    "        with open(self.meta_path,'r') as f:\n",
    "            lines = f.readlines()\n",
    "        return len(lines)//self.data_num_per_sample\n",
    "    def preprocess(self,data_list):\n",
    "        \n",
    "        video_path,text_path,label = data_list\n",
    "        \n",
    "        #--------------- video ----------------#\n",
    "        \n",
    "        frame_list = self.frame_iterator_list(video_path)\n",
    "        \n",
    "        video = []\n",
    "        for frame in frame_list:\n",
    "            augmented = self.A_transform(image=frame)\n",
    "            video.append(augmented['image'].unsqueeze(0))\n",
    "        video = torch.cat(video,dim=0)# shape(len,channel,H,W)\n",
    "        #--------------- text ----------------#\n",
    "        text = ''\n",
    "        with open(text_path,'r') as f:\n",
    "            for line in f:\n",
    "                dic = eval(line)\n",
    "           \n",
    "        for key in dic:\n",
    "            dic[key] = ''.join(re.findall('[\\u4e00-\\u9fa5]',dic[key]))\n",
    "            text += dic[key]\n",
    "        \n",
    "        # text = ''.join(re.findall('[\\u4e00-\\u9fa5]',dic['video_asr']))\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.text_max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        text_ids = inputs['input_ids']\n",
    "        text_attention_mask = inputs['attention_mask']\n",
    "        text_ids = torch.tensor(np.array(text_ids).astype('int64'))\n",
    "        text_attention_mask = torch.tensor(np.array(text_attention_mask).astype('int64'))\n",
    "        #--------------- label ----------------#\n",
    "        label_ids = []\n",
    "        label = label.split(',')\n",
    "        np.random.shuffle(label)\n",
    "        for i in label:\n",
    "            label_ids.append(self.label2id[i])\n",
    "        # label_ids = torch.tensor(np.array(label_ids).astype('int64'))\n",
    "        dense_label_ids = torch.zeros(82)# ,dtype=torch.int64)\n",
    "        dense_label_ids[label_ids] = 1\n",
    "        # return video,audio,label_ids\n",
    "        return video,text_ids,text_attention_mask,dense_label_ids\n",
    "    \n",
    "    def collate_fn(self,batch):\n",
    "        # 自定义dataloader 对一个batch的处理方式\n",
    "        # 需要完成的任务有：\n",
    "        # 1. 对video和audio的序列进行padding\n",
    "        # 2. 对text，label_ids同样padding\n",
    "        video_stacks = []\n",
    "        text_stacks = []\n",
    "        label_stacks = []\n",
    "        text_attention_stacks = []\n",
    "        for i in batch:\n",
    "            video_stacks.append(i[0])\n",
    "            text_stacks.append(i[1])\n",
    "            text_attention_stacks.append(i[2])\n",
    "            label_stacks.append(i[3])\n",
    "        \n",
    "        video_stacks = pad_sequence(video_stacks,batch_first=True,padding_value=0)\n",
    "        text_stacks = pad_sequence(text_stacks,batch_first=True,padding_value=0) # 实际上没有pad\n",
    "        # 实际上并没有padding，因为label变成multi-hot向量，长度都是82\n",
    "        label_stacks = pad_sequence(label_stacks,batch_first=True,padding_value=0) \n",
    "        text_attention_stacks = pad_sequence(text_attention_stacks,batch_first=True,padding_value=0) # 实际上也没有pad\n",
    "        return video_stacks,text_stacks,text_attention_stacks,label_stacks\n",
    "    \n",
    "    def frame_iterator_list(self,filename, every_ms=1000, max_num_frames=300):\n",
    "        video_capture = cv2.VideoCapture()\n",
    "        if not video_capture.open(filename):\n",
    "            print(sys.stderr, 'Error: Cannot open video file ' + filename)\n",
    "            return\n",
    "        last_ts = -99999  # The timestamp of last retrieved frame.\n",
    "        num_retrieved = 0\n",
    "\n",
    "        frame_all = []\n",
    "        while num_retrieved < max_num_frames:\n",
    "            # Skip frames\n",
    "            while video_capture.get(cv2.CAP_PROP_POS_MSEC) < every_ms + last_ts:\n",
    "                if not video_capture.read()[0]:\n",
    "                    return frame_all\n",
    "\n",
    "            last_ts = video_capture.get(cv2.CAP_PROP_POS_MSEC)\n",
    "            has_frames, frame = video_capture.read()\n",
    "            if not has_frames:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_all.append(frame[:, :, ::-1])\n",
    "            num_retrieved += 1\n",
    "\n",
    "        return frame_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/ipykernel/__main__.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from torch.utils.data import DataLoader\n",
    "config_path = './config/config.yaml'\n",
    "config = yaml.load(open(config_path))\n",
    "train_dataset = MultimodaRawDataset(config['DatasetConfig'],job='training')\n",
    "dataloader = DataLoader(train_dataset,batch_size=4,num_workers=4,collate_fn=train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "it = iter(dataloader)\n",
    "batch = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "video,text_ids,text_attention_mask,dense_label_ids = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 57, 3, 256, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.video_head.nextvlad import NeXtVLAD\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "class RawNeXtVLAD(nn.Module):\n",
    "    def __init__(self,feature_size, max_frames, nextvlad_cluster_size, expansion, groups):\n",
    "        super(RawNeXtVLAD,self).__init__()\n",
    "        self.nextvlad = NeXtVLAD(feature_size, max_frames, nextvlad_cluster_size, expansion, groups)\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "        self.resnet50.fc = nn.Linear(2048,1024)\n",
    "        \n",
    "    def forward(self,input,mask=None):\n",
    "        # 输入图像shape (batch,len,channel,H,W)\n",
    "        B,S,C,H,W = input.shape\n",
    "        input = input.contiguous().view(B*S,C,H,W)\n",
    "        output = self.resnet50(input)\n",
    "        output = output.contiguous().view(B,S,-1)\n",
    "        if(mask!=None):\n",
    "            output = self.nextvlad(output,mask)\n",
    "        else:\n",
    "            output = self.nextvlad(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /home/tione/.cache/torch/checkpoints/resnet50-19c8e357.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed15da246c44dcda99d1a162662eb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = RawNeXtVLAD(1024,300,128,2,16)\n",
    "pred = model(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16384])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 57, 3, 256, 256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  _modal_drop( x, rate=0.0, noise_shape=None):\n",
    "    \"\"\"模态dropout\"\"\"\n",
    "    random_scale = torch.rand(noise_shape)\n",
    "    keep_mask = (random_scale >= rate).type(x.dtype).to(x.device) # >= rate的才保留\n",
    "    ret = x * keep_mask\n",
    "    probs = keep_mask.type(torch.float32) # cast将张量进行类型转换\n",
    "    return ret, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_dict, prob_dict = _modal_drop(video, 0.2, [4,1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (video != 0).type(torch.float32).sum(dim=-1).sum(dim=-1).sum(dim=-1).type(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 57])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成raw_train_good.txt和raw_val_good.txt\n",
    "import os\n",
    "import linecache\n",
    "train_good_path = '../dataset/tagging/GroundTruth/datafile/val_good.txt'\n",
    "raw_train_good_path = '../dataset/tagging/GroundTruth/datafile/raw_val_good.txt'\n",
    "dataset_path = '../dataset/videos/video_5k/train_5k/'\n",
    "with open(raw_train_good_path,'w') as f:\n",
    "    for index in range(4500):\n",
    "        video_i = 6*index+1\n",
    "        text_i = 6*index+4\n",
    "        label_i = 6*index+5\n",
    "        video_line = linecache.getline(train_good_path,video_i)\n",
    "        video_line = video_line.strip('\\r\\n')\n",
    "        video_id = os.path.basename(video_line)[:-4]\n",
    "        video_raw_path = dataset_path+video_id+'.mp4'+'\\r\\n'\n",
    "        text_line = linecache.getline(train_good_path,text_i)\n",
    "        label_line = linecache.getline(train_good_path,label_i)\n",
    "        f.write(video_raw_path)\n",
    "        f.write(text_line)\n",
    "        f.write(label_line)\n",
    "        f.write('\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_py3",
   "language": "python",
   "name": "conda_pytorch_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
