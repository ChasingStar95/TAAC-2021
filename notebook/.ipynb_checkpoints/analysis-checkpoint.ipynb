{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_json_path = './results/0784.json'\n",
    "bad_json_path = './results/0751.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(good_json_path,'r') as f:\n",
    "    good_json = json.load(f)\n",
    "with open(bad_json_path,'r') as f:\n",
    "    bad_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_names = list(good_json.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dict = {}\n",
    "for file in file_names:\n",
    "    good_result = good_json[file]['result'][0]['labels']\n",
    "    bad_result = bad_json[file]['result'][0]['labels']\n",
    "    diff_temp = []\n",
    "    for label in good_result:\n",
    "        if(label not in bad_result):\n",
    "            diff_temp.append(label)\n",
    "        analysis_dict[file] = diff_temp\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 在good result中出现而bad result中没有的label\n",
    "dist = {}\n",
    "for k,v in analysis_dict.items():\n",
    "    for l in v:   \n",
    "        if l not in dist.keys():\n",
    "            dist[l] = 1\n",
    "        else:\n",
    "            dist[l] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'工作职场': 134,\n",
       " '场景-其他': 225,\n",
       " '愤怒': 288,\n",
       " '转场': 453,\n",
       " '办公室': 383,\n",
       " '特写': 448,\n",
       " '公园': 10,\n",
       " '远景': 63,\n",
       " '影棚幕布': 349,\n",
       " '拉近': 952,\n",
       " '全景': 564,\n",
       " '惊奇': 437,\n",
       " '手机电脑录屏': 338,\n",
       " '手写解题': 166,\n",
       " '才艺展示': 93,\n",
       " '家': 589,\n",
       " '宫格': 338,\n",
       " '混剪': 487,\n",
       " '幻灯片轮播': 291,\n",
       " '室外': 399,\n",
       " '知识讲解': 371,\n",
       " '填充': 278,\n",
       " '喜悦': 235,\n",
       " '亲子': 167,\n",
       " '家庭伦理': 284,\n",
       " '夫妻&恋人&相亲': 359,\n",
       " '悲伤': 379,\n",
       " '单人情景剧': 184,\n",
       " '动态': 421,\n",
       " '极端特写': 471,\n",
       " '情景演绎': 550,\n",
       " '朋友&同事(平级)': 472,\n",
       " '室内': 501,\n",
       " '配音': 234,\n",
       " '采访': 13,\n",
       " '(马路边的)人行道': 57,\n",
       " '多人情景剧': 99,\n",
       " '过渡页': 214,\n",
       " '路人': 249,\n",
       " '课件展示': 188,\n",
       " '学校': 150,\n",
       " '单人口播': 190,\n",
       " '教师(教授)': 176,\n",
       " '重点圈画': 79,\n",
       " '上下级': 67,\n",
       " '城市道路': 9,\n",
       " '拉远': 176,\n",
       " '教辅材料': 357,\n",
       " '红包': 70,\n",
       " '餐厅': 18,\n",
       " '动画': 210,\n",
       " '汽车内': 29,\n",
       " '图文快闪': 133,\n",
       " '游戏画面': 34,\n",
       " '绘画展示': 30,\n",
       " '商品展示': 47,\n",
       " '亲戚(亲情)': 97,\n",
       " '古代': 18,\n",
       " '城市景观': 20,\n",
       " '厌恶': 26,\n",
       " '企业家': 16,\n",
       " '多人口播': 23,\n",
       " '演播室': 21,\n",
       " '商场': 9,\n",
       " '医生': 1,\n",
       " '励志逆袭': 3,\n",
       " '平静': 8,\n",
       " '医院': 5,\n",
       " '过道': 6,\n",
       " '天空': 6,\n",
       " '门口': 6,\n",
       " '咖啡厅': 1,\n",
       " '中景': 2,\n",
       " '房屋外': 2,\n",
       " '大厅': 1,\n",
       " '停车场': 3,\n",
       " '外卖': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stock ml libraries\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true.union(set_pred)) )\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "text_path = '/Users/mafp/code/datasets/tagging_dataset_train_5k/text_txt/tagging/'\n",
    "text_file_names = os.listdir(text_path)\n",
    "for file_name in text_file_names:\n",
    "    text = ''\n",
    "    with open(text_path+file_name,'r') as f:\n",
    "        for line in f:\n",
    "            dic = eval(line)\n",
    "    for key in dic:\n",
    "        dic[key] = ''.join(re.findall('[\\u4e00-\\u9fa5]',dic[key]))\n",
    "        text += dic[key]\n",
    "    # tokens = ['[CLS]'] + self.tokenizer.tokenize(text)\n",
    "    # text_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "    # text_ids = torch.tensor(np.array(text_ids).astype('int64'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
