{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architectural constants.\n",
    "NUM_FRAMES = 96  # Frames in input mel-spectrogram patch.\n",
    "NUM_BANDS = 64  # Frequency bands in input mel-spectrogram patch.\n",
    "EMBEDDING_SIZE = 128  # Size of embedding layer.\n",
    "\n",
    "# Hyperparameters used in feature and example generation.\n",
    "SAMPLE_RATE = 16000\n",
    "STFT_WINDOW_LENGTH_SECONDS = 0.025\n",
    "STFT_HOP_LENGTH_SECONDS = 0.010\n",
    "NUM_MEL_BINS = NUM_BANDS\n",
    "MEL_MIN_HZ = 125\n",
    "MEL_MAX_HZ = 7500\n",
    "LOG_OFFSET = 0.01  # Offset used for stabilized log of input mel-spectrogram.\n",
    "EXAMPLE_WINDOW_SECONDS = 0.96  # Each example contains 96 10ms frames\n",
    "EXAMPLE_HOP_SECONDS = 0.96     # with zero overlap.\n",
    "\n",
    "# Parameters used for embedding postprocessing.\n",
    "PCA_EIGEN_VECTORS_NAME = 'pca_eigen_vectors'\n",
    "PCA_MEANS_NAME = 'pca_means'\n",
    "QUANTIZE_MIN_VAL = -2.0\n",
    "QUANTIZE_MAX_VAL = +2.0\n",
    "\n",
    "# Hyperparameters used in training.\n",
    "INIT_STDDEV = 0.01  # Standard deviation used to initialize weights.\n",
    "LEARNING_RATE = 1e-4  # Learning rate for the Adam optimizer.\n",
    "ADAM_EPSILON = 1e-8  # Epsilon for the Adam optimizer.\n",
    "\n",
    "# Names of ops, tensors, and features.\n",
    "INPUT_OP_NAME = 'vggish/input_features'\n",
    "INPUT_TENSOR_NAME = INPUT_OP_NAME + ':0'\n",
    "OUTPUT_OP_NAME = 'vggish/embedding'\n",
    "OUTPUT_TENSOR_NAME = OUTPUT_OP_NAME + ':0'\n",
    "AUDIO_EMBEDDING_FEATURE_NAME = 'audio_embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "video_path = '../dataset/videos/video_5k/train_5k/'\n",
    "file_names = os.listdir(video_path)\n",
    "save_path = '../dataset/eff_frames/train_5k/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_file = file_names[0]\n",
    "output_audio = example_file.replace('.mp4','.wav')\n",
    "\n",
    "command = 'ffmpeg -loglevel error -i '+video_path+example_file+' '+output_audio\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import resampy\n",
    "wav_data, sr = sf.read(output_audio, dtype='int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = wav_data / 32768.0 # 归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.mean(samples,axis=1) # 将双声道数据整合成单通道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sr!= SAMPLE_RATE:\n",
    "    data = resampy.resample(data,sr,SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.audio import mel_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_mel = mel_features.log_mel_spectrogram(\n",
    "      data,\n",
    "      audio_sample_rate=SAMPLE_RATE,\n",
    "      log_offset=LOG_OFFSET,\n",
    "      window_length_secs=STFT_WINDOW_LENGTH_SECONDS,\n",
    "      hop_length_secs=STFT_HOP_LENGTH_SECONDS,\n",
    "      num_mel_bins=NUM_MEL_BINS,\n",
    "      lower_edge_hertz=MEL_MIN_HZ,\n",
    "      upper_edge_hertz=MEL_MAX_HZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5991, 64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sample_rate = 1.0 / STFT_HOP_LENGTH_SECONDS\n",
    "example_window_length = int(round(EXAMPLE_WINDOW_SECONDS * features_sample_rate))\n",
    "example_hop_length = int(round(EXAMPLE_HOP_SECONDS * features_sample_rate))\n",
    "log_mel_examples = mel_features.frame(\n",
    "      log_mel,\n",
    "      window_length=example_window_length,\n",
    "      hop_length=example_hop_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 96, 64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_mel_examples.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_py3",
   "language": "python",
   "name": "conda_pytorch_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
