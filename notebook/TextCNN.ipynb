{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting jieba\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.2 MB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314478 sha256=e0ee3c70ba6686a9441538fee7fec32d180dfe50b51783b8e1423907509ce49e\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/de/99/39/55dd43d023169a4464b9118a252e188367c3750c62526c46f3\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextCnn(nn.Module):\n",
    "    def __init__(self, embed_num, embed_dim, class_num, kernel_num, kernel_sizes, dropout = 0.5):\n",
    "        super(TextCnn, self).__init__()\n",
    "\n",
    "        Ci = 1\n",
    "        Co = kernel_num\n",
    "\n",
    "        self.embed = nn.Embedding(embed_num, embed_dim)\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (f, embed_dim), padding = (2, 0)) for f in kernel_sizes])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(Co * len(kernel_sizes), class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)  # (N, token_num, embed_dim)\n",
    "        x = x.unsqueeze(1)  # (N, Ci, token_num, embed_dim)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, token_num) * len(kernel_sizes)]\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co) * len(kernel_sizes)]\n",
    "        x = torch.cat(x, 1) # (N, Co * len(kernel_sizes))\n",
    "        x = self.dropout(x)  # (N, Co * len(kernel_sizes))\n",
    "        logit = self.fc(x)  # (N, class_num)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting torchtext\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/52/a7/8938d232be811db0877531c497f4344361cc6d9377643407162e03fcaa57/torchtext-0.9.1-cp36-cp36m-manylinux1_x86_64.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch==1.8.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/dd/b9/824df420f6abf551e41bbaacbaa0be8321dc104f9f3803051513844dc310/torch-1.8.1-cp36-cp36m-manylinux1_x86_64.whl (804.1 MB)\n",
      "\u001b[K     |██████▌                         | 163.2 MB 2.1 MB/s eta 0:05:12     |█                               | 27.1 MB 540 kB/s eta 0:23:58     |█▌                              | 37.6 MB 748 kB/s eta 0:17:04     |██▌                             | 62.6 MB 758 kB/s eta 0:16:18     |█████▍                          | 135.1 MB 795 kB/s eta 0:14:02     |█████▋                          | 139.9 MB 1.0 MB/s eta 0:10:57"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a1a6afb639e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext.data as data\n",
    "import torchtext.datasets as datasets\n",
    "import jieba\n",
    "import argparse\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [word for word in jieba.cut(text) if word.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3efc37397712>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_field\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_field\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTabularDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./bert_finetune_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "text_field = data.Field(lower=True, tokenize = tokenize)\n",
    "fields = [('text', text_field)]\n",
    "train_dataset = data.TabularDataset('./bert_finetune_data.csv')\n",
    "text_field.build_vocab(train_dataset,min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodaFeaturesDataset(Dataset):\n",
    "\n",
    "    def __init__(self,dataset_config,job='training'):\n",
    "        \n",
    "        self.data_num_per_sample = 6 # 在train.txt中每个sample占6行\n",
    "        self.text_max_len = dataset_config['text_max_len']\n",
    "        self.device = dataset_config['device']\n",
    "        \n",
    "        if(job=='training'):\n",
    "            self.meta_path = dataset_config['train_data_path']\n",
    "        elif(job=='valdation'):\n",
    "            self.meta_path = dataset_config['val_data_path']\n",
    "        else:\n",
    "            self.meta_path = dataset_config['test_data_path']\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(dataset_config['bert_path'])\n",
    "        self.label2id = {}\n",
    "        with open(dataset_config['label_id_path'],'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip('\\r\\n')\n",
    "                line = line.split('\\t')\n",
    "                self.label2id[line[0]] = int(line[1])\n",
    "    def __getitem__(self, index):\n",
    "        # 1. 从train.txt读取对应 idx 的path\n",
    "        data_list = [] # 存储对于index的各个模态数据的路径和样本标签\n",
    "        for line_i in range(self.data_num_per_sample*index+1,self.data_num_per_sample*(index+1)):\n",
    "            line = linecache.getline(self.meta_path,line_i)\n",
    "            line = line.strip('\\r\\n')\n",
    "            data_list.append(line)\n",
    "        video,audio,text_ids,text_attention_mask,label_ids = self.preprocess(data_list)\n",
    "        return video,audio,text_ids,text_attention_mask,label_ids\n",
    "    def __len__(self):\n",
    "        # TODO 不能固定长度\n",
    "        with open(self.meta_path,'r') as f:\n",
    "            lines = f.readlines()\n",
    "        return len(lines)//self.data_num_per_sample\n",
    "    def preprocess(self,data_list):\n",
    "        \n",
    "        video_path,audio_path,image_path,text_path,label = data_list\n",
    "        \n",
    "        #--------------- video ----------------#\n",
    "        video = torch.tensor(np.load(video_path).astype(np.float32))\n",
    "        \n",
    "        #--------------- audio ----------------#\n",
    "        if os.path.exists(audio_path):\n",
    "            audio = torch.tensor(np.load(audio_path).astype(np.float32))\n",
    "        else:\n",
    "            audio = torch.tensor(np.random.random((video.shape[0],128)).astype(np.float32))\n",
    "            \n",
    "        #--------------- text ----------------#\n",
    "        \n",
    "        text = ''\n",
    "        with open(text_path,'r') as f:\n",
    "            for line in f:\n",
    "                dic = eval(line)\n",
    "           \n",
    "        for key in dic:\n",
    "            dic[key] = ''.join(re.findall('[\\u4e00-\\u9fa5]',dic[key]))\n",
    "            text += dic[key]\n",
    "        \n",
    "        # text = ''.join(re.findall('[\\u4e00-\\u9fa5]',dic['video_asr']))\n",
    "        inputs = \n",
    "        text_ids = inputs['input_ids']\n",
    "        text_attention_mask = inputs['attention_mask']\n",
    "        text_ids = torch.tensor(np.array(text_ids).astype('int64'))\n",
    "        text_attention_mask = torch.tensor(np.array(text_attention_mask).astype('int64'))\n",
    "        #--------------- label ----------------#\n",
    "        label_ids = []\n",
    "        label = label.split(',')\n",
    "        np.random.shuffle(label)\n",
    "        for i in label:\n",
    "            label_ids.append(self.label2id[i])\n",
    "        # label_ids = torch.tensor(np.array(label_ids).astype('int64'))\n",
    "        dense_label_ids = torch.zeros(82)# ,dtype=torch.int64)\n",
    "        dense_label_ids[label_ids] = 1\n",
    "        # return video,audio,label_ids\n",
    "        return video,audio,text_ids,text_attention_mask,dense_label_ids\n",
    "    \n",
    "    def collate_fn(self,batch):\n",
    "        # 自定义dataloader 对一个batch的处理方式\n",
    "        # 需要完成的任务有：\n",
    "        # 1. 对video和audio的序列进行padding\n",
    "        # 2. 对text，label_ids同样padding\n",
    "        video_stacks = []\n",
    "        audio_stacks = []\n",
    "        text_stacks = []\n",
    "        label_stacks = []\n",
    "        text_attention_stacks = []\n",
    "        for i in batch:\n",
    "            video_stacks.append(i[0])\n",
    "            audio_stacks.append(i[1])\n",
    "            text_stacks.append(i[2])\n",
    "            text_attention_stacks.append(i[3])\n",
    "            label_stacks.append(i[4])\n",
    "        \n",
    "        video_stacks = pad_sequence(video_stacks,batch_first=True,padding_value=0)\n",
    "        audio_stacks = pad_sequence(audio_stacks,batch_first=True,padding_value=0)\n",
    "        text_stacks = pad_sequence(text_stacks,batch_first=True,padding_value=0) # 实际上没有pad\n",
    "        # 实际上并没有padding，因为label变成multi-hot向量，长度都是82\n",
    "        label_stacks = pad_sequence(label_stacks,batch_first=True,padding_value=0) \n",
    "        text_attention_stacks = pad_sequence(text_attention_stacks,batch_first=True,padding_value=0) # 实际上也没有pad\n",
    "        return video_stacks,audio_stacks,text_stacks,text_attention_stacks,label_stacks\n",
    "        # return video_stacks,audio_stacks,label_stacks\n",
    "    def tokenize(text):\n",
    "        return [word for word in jieba.cut(text) if word.strip()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_py3",
   "language": "python",
   "name": "conda_pytorch_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
