{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting albumentations\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/b0/be/3db3cd8af771988748f69eace42047d5edebf01eaa7e1293f3b3f75f989e/albumentations-1.0.0-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 9.7 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from albumentations) (5.3.1)\n",
      "Collecting opencv-python-headless>=4.1.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/ce/08/baa108293aad5c07fb8aa613ec9443ce600f948dc4afaa24ab0a6c492a16/opencv_python_headless-4.5.2.54-cp36-cp36m-manylinux2014_x86_64.whl (38.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 38.2 MB 163 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from albumentations) (1.18.5)\n",
      "Collecting scikit-image>=0.16.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/0e/ba/53e1bfbdfd0f94514d71502e3acea494a8b4b57c457adbc333ef386485da/scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from albumentations) (1.5.0)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from scikit-image>=0.16.1->albumentations) (2.2.2)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
      "Collecting imageio>=2.3.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/6e/57/5d899fae74c1752f52869b613a8210a2480e1a69688e65df6cb26117d45d/imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 8.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyWavelets>=1.1.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/59/bb/d2b85265ec9fa3c1922210c9393d4cdf7075cc87cce6fe671d7455f80fbc/PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 37.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx>=2.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/f3/b7/c7f488101c0bb5e4178f3cde416004280fd40262433496830de8a8c21613/networkx-2.5.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 45.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tifffile>=2019.7.26\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/42/6b/93a8ee61c6fbe20fa9c17928bd3b80484902b7fd454cecaffba42f5052cb/tifffile-2020.9.3-py3-none-any.whl (148 kB)\n",
      "\u001b[K     |████████████████████████████████| 148 kB 16.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2020.1)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.2.0)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n",
      "Installing collected packages: opencv-python-headless, imageio, PyWavelets, networkx, tifffile, scikit-image, albumentations\n",
      "Successfully installed PyWavelets-1.1.1 albumentations-1.0.0 imageio-2.9.0 networkx-2.5.1 opencv-python-headless-4.5.2.54 scikit-image-0.17.2 tifffile-2020.9.3\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting transformers\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 8.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from transformers) (4.46.1)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/bf/20/3605db440db4f96d5ffd66b231a043ae451ec7e5e4d1a2fb6f20608006c4/tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 8.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from transformers) (20.4)\n",
      "Collecting sacremoses\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 10.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from transformers) (1.7.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/6b/38/0ed2670578d803cb14350c54adb2a79835870aa9e3ad2e732be7359cb0e8/regex-2021.4.4-cp36-cp36m-manylinux2014_x86_64.whl (722 kB)\n",
      "\u001b[K     |████████████████████████████████| 722 kB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from packaging->transformers) (1.15.0)\n",
      "Collecting click\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/76/0a/b6c5f311e32aeb3b406e03c079ade51e905ea630fc19d1262a46249c1c86/click-8.0.1-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 14.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from sacremoses->transformers) (0.15.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.1.0)\n",
      "Installing collected packages: filelock, huggingface-hub, tokenizers, click, regex, sacremoses, dataclasses, transformers\n",
      "Successfully installed click-8.0.1 dataclasses-0.8 filelock-3.0.12 huggingface-hub-0.0.8 regex-2021.4.4 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting tensorboard\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/44/f5/7feea02a3fb54d5db827ac4b822a7ba8933826b36de21880518250b8733a/tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 165 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/bd/24/11c3ea5a7e866bf2d97f0501d0b4b1c9bbeade102bb4b588f0d2919a5212/Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 14.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.4\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/92/c9/ef0fae29182d7a867d203f0eff8296b60da92098cc41db33a434f4be84bf/absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 13.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.24.3\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/0e/4b/c4e1839cdb3248e1006837e1d427e1843f80f2e6ba69f3af77f00bb51ac4/grpcio-1.38.0-cp36-cp36m-manylinux2014_x86_64.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 9.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/6e/33/1ae0f71395e618d6140fbbc9587cc3156591f748226075e0f7d6f9176522/Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 21.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/9d/d3/7541e89f1fc456eef157224f597a8bba22589db6369a03eaba68c11f07a0/google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/96/21/bfde897baf6f0df050205a23bbfd9dcbedcac522b22eb9382d29e39fdd15/google_auth-1.30.2-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 18.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.12.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from tensorboard) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from tensorboard) (47.3.1.post20200616)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/60/f9/802efd84988bffd9f644c03b6e66fde8e76c3aa33db4279ddd11c5d61f4b/tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 164 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/1a/c1/499e600ba0c618b451cd9c425ae1c177249940a2086316552fee7d86c954/tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 26.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from tensorboard) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from tensorboard) (3.12.2)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from werkzeug>=0.11.15->tensorboard) (0.8)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from absl-py>=0.4->tensorboard) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.7.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/bf/28/c4f5796c67ad06bb91d98d543a5e01805c1ff065e08871f78e52d2a331ad/cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/e9/93/0c0f002031f18b53af7a6166103c02b9c0667be528944137cc954ec921b3/rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 16.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (3.1.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/e8/5d/9dd1c29e5a786525f6342f6c1d812ed2e37edc653ad297048c1668988053/oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 13.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1>=0.1.3\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 18.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: werkzeug, absl-py, grpcio, markdown, oauthlib, requests-oauthlib, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard-data-server, tensorboard-plugin-wit, tensorboard\n",
      "Successfully installed absl-py-0.12.0 cachetools-4.2.2 google-auth-1.30.2 google-auth-oauthlib-0.4.4 grpcio-1.38.0 markdown-3.3.4 oauthlib-3.1.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 werkzeug-2.0.1\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting torchtext==0.5\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 9.7 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchtext==0.5) (1.15.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchtext==0.5) (4.46.1)\n",
      "Requirement already satisfied: torch in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchtext==0.5) (1.4.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchtext==0.5) (2.24.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchtext==0.5) (1.18.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->torchtext==0.5) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->torchtext==0.5) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->torchtext==0.5) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from requests->torchtext==0.5) (2020.6.20)\n",
      "Installing collected packages: sentencepiece, torchtext\n",
      "Successfully installed sentencepiece-0.1.95 torchtext-0.5.0\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting jieba\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.2 MB 164 kB/s eta 0:00:01    |█████████▌                      | 5.7 MB 8.2 MB/s eta 0:00:02\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314478 sha256=3a08c91067f7796e8f7b6b79a705c0b98a8e56224566c30a89f31314f2b7a2e3\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/de/99/39/55dd43d023169a4464b9118a252e188367c3750c62526c46f3\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting timm\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/ee/08/1ccaf8d516935666b7fa5f6aaddf157c66208ea0c93bb847ae09f166354f/timm-0.4.9-py3-none-any.whl (346 kB)\n",
      "\u001b[K     |████████████████████████████████| 346 kB 11.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from timm) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from timm) (0.5.0)\n",
      "Requirement already satisfied: six in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchvision->timm) (1.15.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchvision->timm) (7.1.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from torchvision->timm) (1.18.5)\n",
      "Installing collected packages: timm\n",
      "Successfully installed timm-0.4.9\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting soundfile\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from soundfile) (1.14.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from cffi>=1.0->soundfile) (2.20)\n",
      "Installing collected packages: soundfile\n",
      "Successfully installed soundfile-0.10.3.post1\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting resampy\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/79/75/e22272b9c2185fc8f3af6ce37229708b45e8b855fd4bc38b4d6b040fff65/resampy-0.2.2.tar.gz (323 kB)\n",
      "\u001b[K     |████████████████████████████████| 323 kB 6.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.10 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from resampy) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.13 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from resampy) (1.5.0)\n",
      "Collecting numba>=0.32\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/4a/c1/e7fdbfc886a9d9c11767533903db0d816c0f656fd6029f4a061742893694/numba-0.53.1-cp36-cp36m-manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 6.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.3 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from resampy) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from numba>=0.32->resampy) (47.3.1.post20200616)\n",
      "Collecting llvmlite<0.37,>=0.36.0rc1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/4d/5a/707cc7e072d71bc19869d093e5cf9b7be98cb42d2398489465474d007ce8/llvmlite-0.36.0-cp36-cp36m-manylinux2010_x86_64.whl (25.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.3 MB 160 kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: resampy\n",
      "  Building wheel for resampy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=320720 sha256=1cf9eda42811e849f9c14cfee83894f271343e3a05407769419635605a9be076\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/83/39/6b/c82c4d996c0be8bdf4ee84ded29b2294f81aced663aeb3413e\n",
      "Successfully built resampy\n",
      "Installing collected packages: llvmlite, numba, resampy\n",
      "Successfully installed llvmlite-0.36.0 numba-0.53.1 resampy-0.2.2\n"
     ]
    }
   ],
   "source": [
    "!./setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "train (0): 100%|██████████████████████████████| 141/141 [01:36<00:00,  1.47it/s]\n",
      "epoch(0)(video):  0.6482096685392892\n",
      "epoch(0)(audio):  0.634513039061845\n",
      "epoch(0)(text):  0.6507512827718295\n",
      "epoch(0)(fusion):  0.6635245517715165\n",
      "train (1): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.72it/s]\n",
      "train (2): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(2)(video):  0.6825316921619797\n",
      "epoch(2)(audio):  0.658005511853313\n",
      "epoch(2)(text):  0.68377814847154\n",
      "epoch(2)(fusion):  0.7164793808068958\n",
      "train (3): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (4): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(4)(video):  0.6978470196496707\n",
      "epoch(4)(audio):  0.6563006256746932\n",
      "epoch(4)(text):  0.6934482043044924\n",
      "epoch(4)(fusion):  0.7307478142567317\n",
      "train (5): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (6): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(6)(video):  0.6956477698358886\n",
      "epoch(6)(audio):  0.6632107071097291\n",
      "epoch(6)(text):  0.7074001458171614\n",
      "epoch(6)(fusion):  0.754342517825369\n",
      "train (7): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (8): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(8)(video):  0.7177713391827485\n",
      "epoch(8)(audio):  0.6646809891312178\n",
      "epoch(8)(text):  0.7126931652992821\n",
      "epoch(8)(fusion):  0.758925411598933\n",
      "train (9): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (10): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(10)(video):  0.7228428261016849\n",
      "epoch(10)(audio):  0.670038834943339\n",
      "epoch(10)(text):  0.7098672593688208\n",
      "epoch(10)(fusion):  0.7614512141408306\n",
      "train (11): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (12): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(12)(video):  0.7292725222290269\n",
      "epoch(12)(audio):  0.6708198564432607\n",
      "epoch(12)(text):  0.7046485585304719\n",
      "epoch(12)(fusion):  0.7633110896003005\n",
      "train (13): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (14): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(14)(video):  0.7274059334958773\n",
      "epoch(14)(audio):  0.6682195343011723\n",
      "epoch(14)(text):  0.7131325040953097\n",
      "epoch(14)(fusion):  0.7645309381491278\n",
      "train (15): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (16): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(16)(video):  0.7303133054464217\n",
      "epoch(16)(audio):  0.6712876083586382\n",
      "epoch(16)(text):  0.71082769413351\n",
      "epoch(16)(fusion):  0.7684514061145669\n",
      "train (17): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (18): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(18)(video):  0.7297115916672892\n",
      "epoch(18)(audio):  0.6664233258877238\n",
      "epoch(18)(text):  0.7180626533573978\n",
      "epoch(18)(fusion):  0.7683658270777125\n",
      "train (19): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (20): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(20)(video):  0.7453442296801583\n",
      "epoch(20)(audio):  0.6805762644496383\n",
      "epoch(20)(text):  0.7189651660067033\n",
      "epoch(20)(fusion):  0.7774199987269717\n",
      "train (21): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (22): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(22)(video):  0.7457597028885259\n",
      "epoch(22)(audio):  0.6828601160066176\n",
      "epoch(22)(text):  0.7185645596327537\n",
      "epoch(22)(fusion):  0.7792947705514444\n",
      "train (23): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (24): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(24)(video):  0.7485576443013583\n",
      "epoch(24)(audio):  0.6809235534071544\n",
      "epoch(24)(text):  0.7198643488301466\n",
      "epoch(24)(fusion):  0.7781847102569334\n",
      "train (25): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (26): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(26)(video):  0.7438771590635798\n",
      "epoch(26)(audio):  0.6810117243033993\n",
      "epoch(26)(text):  0.7206844599933511\n",
      "epoch(26)(fusion):  0.7785487014379515\n",
      "train (27): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (28): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(28)(video):  0.7458187825206997\n",
      "epoch(28)(audio):  0.6805273521157909\n",
      "epoch(28)(text):  0.7202642606528599\n",
      "epoch(28)(fusion):  0.7791856898833789\n",
      "train (29): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (30): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(30)(video):  0.7428260266340333\n",
      "epoch(30)(audio):  0.6811973407502413\n",
      "epoch(30)(text):  0.7178205511601777\n",
      "epoch(30)(fusion):  0.7793983258456011\n",
      "train (31): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (32): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(32)(video):  0.7455716388733408\n",
      "epoch(32)(audio):  0.6793865191426125\n",
      "epoch(32)(text):  0.7193509532811998\n",
      "epoch(32)(fusion):  0.7797002306427419\n",
      "train (33): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (34): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(34)(video):  0.7449485633753375\n",
      "epoch(34)(audio):  0.6809957521111524\n",
      "epoch(34)(text):  0.7185702269875583\n",
      "epoch(34)(fusion):  0.7797698849651884\n",
      "train (35): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (36): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(36)(video):  0.7403850620408603\n",
      "epoch(36)(audio):  0.6809813795921847\n",
      "epoch(36)(text):  0.718838809772626\n",
      "epoch(36)(fusion):  0.7817811181048786\n",
      "train (37): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (38): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "epoch(38)(video):  0.7398135209839731\n",
      "epoch(38)(audio):  0.6806664886847769\n",
      "epoch(38)(text):  0.716096172567275\n",
      "epoch(38)(fusion):  0.7810829992568235\n",
      "train (39): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (40): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(40)(video):  0.7402664915825502\n",
      "epoch(40)(audio):  0.6778703001412896\n",
      "epoch(40)(text):  0.718339318465071\n",
      "epoch(40)(fusion):  0.7799253786404825\n",
      "train (41): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (42): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(42)(video):  0.7422714310813914\n",
      "epoch(42)(audio):  0.6810601717885372\n",
      "epoch(42)(text):  0.7193423141557473\n",
      "epoch(42)(fusion):  0.7815773882894099\n",
      "train (43): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (44): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(44)(video):  0.7436108538103314\n",
      "epoch(44)(audio):  0.6793668138105755\n",
      "epoch(44)(text):  0.7177758911743812\n",
      "epoch(44)(fusion):  0.7815385621974036\n",
      "train (45): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (46): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(46)(video):  0.7431701237373193\n",
      "epoch(46)(audio):  0.6814590139620669\n",
      "epoch(46)(text):  0.7184695331529979\n",
      "epoch(46)(fusion):  0.7821113349661457\n",
      "train (47): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (48): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(48)(video):  0.7419699061276659\n",
      "epoch(48)(audio):  0.6811877816524158\n",
      "epoch(48)(text):  0.7183077583566306\n",
      "epoch(48)(fusion):  0.7814906219116384\n",
      "train (49): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (50): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(50)(video):  0.7428650598465212\n",
      "epoch(50)(audio):  0.6811089463122293\n",
      "epoch(50)(text):  0.7171191585990251\n",
      "epoch(50)(fusion):  0.7811972959386059\n",
      "train (51): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (52): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(52)(video):  0.7432425003651786\n",
      "epoch(52)(audio):  0.6813037446201009\n",
      "epoch(52)(text):  0.7179463716854964\n",
      "epoch(52)(fusion):  0.7821524642792268\n",
      "train (53): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (54): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(54)(video):  0.7421581787803797\n",
      "epoch(54)(audio):  0.681568583450445\n",
      "epoch(54)(text):  0.7177555494836095\n",
      "epoch(54)(fusion):  0.7817643443344455\n",
      "train (55): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (56): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(56)(video):  0.7445497386924406\n",
      "epoch(56)(audio):  0.6798404599761527\n",
      "epoch(56)(text):  0.7183954063092083\n",
      "epoch(56)(fusion):  0.7819554367731253\n",
      "train (57): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (58): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(58)(video):  0.7429213939795895\n",
      "epoch(58)(audio):  0.680611585980551\n",
      "epoch(58)(text):  0.7190558750319914\n",
      "epoch(58)(fusion):  0.7819784942167961\n",
      "train (59): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (60): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(60)(video):  0.7409777657957275\n",
      "epoch(60)(audio):  0.6810024037088491\n",
      "epoch(60)(text):  0.7172512450575996\n",
      "epoch(60)(fusion):  0.7821263105591763\n",
      "train (61): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (62): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(62)(video):  0.7420725794784246\n",
      "epoch(62)(audio):  0.6818576417129077\n",
      "epoch(62)(text):  0.7172394380162641\n",
      "epoch(62)(fusion):  0.78180490308466\n",
      "train (63): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (64): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(64)(video):  0.7419054305207443\n",
      "epoch(64)(audio):  0.6818674056295401\n",
      "epoch(64)(text):  0.7166581980480071\n",
      "epoch(64)(fusion):  0.7818762094272969\n",
      "train (65): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (66): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(66)(video):  0.740282745396029\n",
      "epoch(66)(audio):  0.6800156884383768\n",
      "epoch(66)(text):  0.7181278303551015\n",
      "epoch(66)(fusion):  0.7811928754836478\n",
      "train (67): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.72it/s]\n",
      "train (68): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(68)(video):  0.7424138435954628\n",
      "epoch(68)(audio):  0.6799140556948695\n",
      "epoch(68)(text):  0.7167298640297233\n",
      "epoch(68)(fusion):  0.780716265644469\n",
      "train (69): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (70): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(70)(video):  0.7421087014234\n",
      "epoch(70)(audio):  0.681524412885842\n",
      "epoch(70)(text):  0.716780752126323\n",
      "epoch(70)(fusion):  0.7806338878190304\n",
      "train (71): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (72): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(72)(video):  0.741677078896532\n",
      "epoch(72)(audio):  0.6808687024113345\n",
      "epoch(72)(text):  0.7184592054662663\n",
      "epoch(72)(fusion):  0.7814856735579314\n",
      "train (73): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (74): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(74)(video):  0.7430335032378912\n",
      "epoch(74)(audio):  0.6819894156116436\n",
      "epoch(74)(text):  0.7178569360886091\n",
      "epoch(74)(fusion):  0.781972276972001\n",
      "train (75): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (76): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(76)(video):  0.7420612000183087\n",
      "epoch(76)(audio):  0.6805830614851668\n",
      "epoch(76)(text):  0.7175187145894125\n",
      "epoch(76)(fusion):  0.781741776697633\n",
      "train (77): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (78): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "epoch(78)(video):  0.7426878288111229\n",
      "epoch(78)(audio):  0.6808827647066984\n",
      "epoch(78)(text):  0.7175511323821409\n",
      "epoch(78)(fusion):  0.7819088083178675\n",
      "train (79): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (80): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "epoch(80)(video):  0.7408534860719443\n",
      "epoch(80)(audio):  0.6813882557662185\n",
      "epoch(80)(text):  0.718452276022728\n",
      "epoch(80)(fusion):  0.7816993633298154\n",
      "train (81): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (82): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(82)(video):  0.743048358140217\n",
      "epoch(82)(audio):  0.6806476931508297\n",
      "epoch(82)(text):  0.719140252133609\n",
      "epoch(82)(fusion):  0.7814696315970735\n",
      "train (83): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (84): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(84)(video):  0.7403550275335015\n",
      "epoch(84)(audio):  0.680455978611015\n",
      "epoch(84)(text):  0.7186185016543486\n",
      "epoch(84)(fusion):  0.781540136162549\n",
      "train (85): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "train (86): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.71it/s]\n",
      "epoch(86)(video):  0.7409845081904827\n",
      "epoch(86)(audio):  0.6818045015196231\n",
      "epoch(86)(text):  0.7186369604908758\n",
      "epoch(86)(fusion):  0.7816153816148376\n",
      "train (87):  34%|██████████▏                   | 48/141 [00:39<00:43,  2.13it/s]^C\n"
     ]
    }
   ],
   "source": [
    "# 采用asr的结果\n",
    "!python -W ignore main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "train (0): 100%|██████████████████████████████| 141/141 [01:25<00:00,  1.66it/s]\n",
      "epoch(0)(video):  0.6453660831056209\n",
      "epoch(0)(audio):  0.6310173637975652\n",
      "epoch(0)(text):  0.6393302034628048\n",
      "epoch(0)(fusion):  0.6541986625848379\n",
      "train (1): 100%|██████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (2): 100%|██████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(2)(video):  0.6827392172159827\n",
      "epoch(2)(audio):  0.6570411049694206\n",
      "epoch(2)(text):  0.6857805290566336\n",
      "epoch(2)(fusion):  0.7144750555489762\n",
      "train (3): 100%|██████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (4): 100%|██████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(4)(video):  0.6904383118876172\n",
      "epoch(4)(audio):  0.6550363140166056\n",
      "epoch(4)(text):  0.7036508054175634\n",
      "epoch(4)(fusion):  0.7336560067461474\n",
      "train (5): 100%|██████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (6): 100%|██████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(6)(video):  0.7078987848860042\n",
      "epoch(6)(audio):  0.6692717283037988\n",
      "epoch(6)(text):  0.7209426581035547\n",
      "epoch(6)(fusion):  0.757785171932195\n",
      "train (7): 100%|██████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (8): 100%|██████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(8)(video):  0.7319875394079428\n",
      "epoch(8)(audio):  0.6758056829465021\n",
      "epoch(8)(text):  0.725259469560105\n",
      "epoch(8)(fusion):  0.7623471952721884\n",
      "train (9): 100%|██████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (10): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.68it/s]\n",
      "epoch(10)(video):  0.7387579442470666\n",
      "epoch(10)(audio):  0.6768616782727327\n",
      "epoch(10)(text):  0.7267811693947507\n",
      "epoch(10)(fusion):  0.7673652448846422\n",
      "train (11): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (12): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(12)(video):  0.7379696604976747\n",
      "epoch(12)(audio):  0.6767724943373816\n",
      "epoch(12)(text):  0.7264266752871837\n",
      "epoch(12)(fusion):  0.7669348660125249\n",
      "train (13): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (14): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(14)(video):  0.7278324692219378\n",
      "epoch(14)(audio):  0.6707284999607492\n",
      "epoch(14)(text):  0.7234565914518012\n",
      "epoch(14)(fusion):  0.7681672845957277\n",
      "train (15): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (16): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "epoch(16)(video):  0.7391841596978351\n",
      "epoch(16)(audio):  0.6727707839706167\n",
      "epoch(16)(text):  0.7270484115492969\n",
      "epoch(16)(fusion):  0.7721218799573051\n",
      "train (17): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (18): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(18)(video):  0.7440976173799672\n",
      "epoch(18)(audio):  0.6764732426575691\n",
      "epoch(18)(text):  0.7261807220844498\n",
      "epoch(18)(fusion):  0.7737911741294248\n",
      "train (19): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (20): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(20)(video):  0.7407867485573459\n",
      "epoch(20)(audio):  0.6743090279651691\n",
      "epoch(20)(text):  0.7276882790442957\n",
      "epoch(20)(fusion):  0.7728262951479035\n",
      "train (21): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (22): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(22)(video):  0.7439811560813203\n",
      "epoch(22)(audio):  0.6747846340385295\n",
      "epoch(22)(text):  0.7272797146728293\n",
      "epoch(22)(fusion):  0.7762283503722575\n",
      "train (23): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (24): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.68it/s]\n",
      "epoch(24)(video):  0.7420077105942109\n",
      "epoch(24)(audio):  0.6757087853300497\n",
      "epoch(24)(text):  0.7264562767404108\n",
      "epoch(24)(fusion):  0.7773843034946512\n",
      "train (25): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (26): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(26)(video):  0.723795945766733\n",
      "epoch(26)(audio):  0.6761534763480076\n",
      "epoch(26)(text):  0.7257256138882798\n",
      "epoch(26)(fusion):  0.7754687457813317\n",
      "train (27): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (28): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(28)(video):  0.7448012076104469\n",
      "epoch(28)(audio):  0.6765914235439522\n",
      "epoch(28)(text):  0.7262806754042925\n",
      "epoch(28)(fusion):  0.7776563505619235\n",
      "train (29): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (30): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(30)(video):  0.7436475197589095\n",
      "epoch(30)(audio):  0.6773924511002861\n",
      "epoch(30)(text):  0.7268763340841028\n",
      "epoch(30)(fusion):  0.7798764069718627\n",
      "train (31): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (32): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(32)(video):  0.7437236811667854\n",
      "epoch(32)(audio):  0.676500386196634\n",
      "epoch(32)(text):  0.7259502512867803\n",
      "epoch(32)(fusion):  0.7814630177476046\n",
      "train (33): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (34): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "epoch(34)(video):  0.7417111678421201\n",
      "epoch(34)(audio):  0.6726678913270788\n",
      "epoch(34)(text):  0.7265230951675474\n",
      "epoch(34)(fusion):  0.7785910930730207\n",
      "train (35): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (36): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(36)(video):  0.7333042692165861\n",
      "epoch(36)(audio):  0.6750203847228229\n",
      "epoch(36)(text):  0.7267693253120661\n",
      "epoch(36)(fusion):  0.7779878364367154\n",
      "train (37): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (38): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(38)(video):  0.737540880481447\n",
      "epoch(38)(audio):  0.675165883798575\n",
      "epoch(38)(text):  0.7262512630329544\n",
      "epoch(38)(fusion):  0.7801943620942057\n",
      "train (39): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (40): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(40)(video):  0.7400649036840813\n",
      "epoch(40)(audio):  0.6752167728740447\n",
      "epoch(40)(text):  0.7275859398294192\n",
      "epoch(40)(fusion):  0.7801549678924113\n",
      "train (41): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (42): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(42)(video):  0.7423969883644785\n",
      "epoch(42)(audio):  0.6771465906747086\n",
      "epoch(42)(text):  0.7261127528807577\n",
      "epoch(42)(fusion):  0.7794708594629087\n",
      "train (43): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (44): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(44)(video):  0.7416090701164532\n",
      "epoch(44)(audio):  0.6712321745286438\n",
      "epoch(44)(text):  0.724400470907011\n",
      "epoch(44)(fusion):  0.7777214411624441\n",
      "train (45): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (46): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(46)(video):  0.7348046453789401\n",
      "epoch(46)(audio):  0.67625823030341\n",
      "epoch(46)(text):  0.7223775955206829\n",
      "epoch(46)(fusion):  0.7759051506693359\n",
      "train (47): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (48): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "epoch(48)(video):  0.7323869607911657\n",
      "epoch(48)(audio):  0.6722165865067051\n",
      "epoch(48)(text):  0.725871709078348\n",
      "epoch(48)(fusion):  0.7772692938673047\n",
      "train (49): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (50): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(50)(video):  0.7419488958163116\n",
      "epoch(50)(audio):  0.6715163333601177\n",
      "epoch(50)(text):  0.7263390063068835\n",
      "epoch(50)(fusion):  0.7782964698838185\n",
      "train (51): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (52): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(52)(video):  0.7311251459197559\n",
      "epoch(52)(audio):  0.6732511642390807\n",
      "epoch(52)(text):  0.725406957961234\n",
      "epoch(52)(fusion):  0.7791317720496866\n",
      "train (53): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (54): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(54)(video):  0.7294979200399258\n",
      "epoch(54)(audio):  0.6730078952825916\n",
      "epoch(54)(text):  0.7263520127464698\n",
      "epoch(54)(fusion):  0.7780633593754281\n",
      "train (55): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (56): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(56)(video):  0.7271220331404484\n",
      "epoch(56)(audio):  0.6719618267150426\n",
      "epoch(56)(text):  0.7239189583448419\n",
      "epoch(56)(fusion):  0.7784380225191001\n",
      "train (57): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (58): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(58)(video):  0.7358100572022739\n",
      "epoch(58)(audio):  0.673940349495208\n",
      "epoch(58)(text):  0.7265032349237255\n",
      "epoch(58)(fusion):  0.777719099790068\n",
      "train (59): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (60): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(60)(video):  0.7314873811910538\n",
      "epoch(60)(audio):  0.6743631475791502\n",
      "epoch(60)(text):  0.7231814877524404\n",
      "epoch(60)(fusion):  0.7777006921986037\n",
      "train (61): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (62): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "epoch(62)(video):  0.7366010753059691\n",
      "epoch(62)(audio):  0.6736759624473044\n",
      "epoch(62)(text):  0.7241297857381654\n",
      "epoch(62)(fusion):  0.7774024576626901\n",
      "train (63): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (64): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(64)(video):  0.7328470229632484\n",
      "epoch(64)(audio):  0.6697744633957211\n",
      "epoch(64)(text):  0.7226836212016619\n",
      "epoch(64)(fusion):  0.7769104984046806\n",
      "train (65): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (66): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(66)(video):  0.7335088898857443\n",
      "epoch(66)(audio):  0.673338648566348\n",
      "epoch(66)(text):  0.7229687803678938\n",
      "epoch(66)(fusion):  0.7756446116126072\n",
      "train (67): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (68): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "epoch(68)(video):  0.7314750129192411\n",
      "epoch(68)(audio):  0.6716962466043664\n",
      "epoch(68)(text):  0.722546851500117\n",
      "epoch(68)(fusion):  0.7767100960749819\n",
      "train (69): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (70): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(70)(video):  0.72616726413008\n",
      "epoch(70)(audio):  0.6713693918852315\n",
      "epoch(70)(text):  0.7245001335532578\n",
      "epoch(70)(fusion):  0.7755506694852582\n",
      "train (71): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (72): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(72)(video):  0.7294367938230315\n",
      "epoch(72)(audio):  0.6708936990124832\n",
      "epoch(72)(text):  0.7221122595810142\n",
      "epoch(72)(fusion):  0.7749120579572336\n",
      "train (73): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (74): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "epoch(74)(video):  0.7262237710697242\n",
      "epoch(74)(audio):  0.6732987349855295\n",
      "epoch(74)(text):  0.7238740033687354\n",
      "epoch(74)(fusion):  0.7765363073354874\n",
      "train (75): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (76): 100%|█████████████████████████████| 141/141 [01:24<00:00,  1.68it/s]\n",
      "epoch(76)(video):  0.7307481180082267\n",
      "epoch(76)(audio):  0.6711732107498197\n",
      "epoch(76)(text):  0.7237386464648639\n",
      "epoch(76)(fusion):  0.7773943538996915\n",
      "train (77): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (78): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(78)(video):  0.7308696387650929\n",
      "epoch(78)(audio):  0.6700671840619686\n",
      "epoch(78)(text):  0.7244799603973733\n",
      "epoch(78)(fusion):  0.7765642689428358\n",
      "train (79): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (80): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(80)(video):  0.719449652723984\n",
      "epoch(80)(audio):  0.6707651927059042\n",
      "epoch(80)(text):  0.7235395928232669\n",
      "epoch(80)(fusion):  0.7743371117428066\n",
      "train (81): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (82): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(82)(video):  0.7281619732833289\n",
      "epoch(82)(audio):  0.6704741427143828\n",
      "epoch(82)(text):  0.723168007402543\n",
      "epoch(82)(fusion):  0.7756721130371982\n",
      "train (83): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (84): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(84)(video):  0.7217268507501633\n",
      "epoch(84)(audio):  0.6709815163219989\n",
      "epoch(84)(text):  0.7208032869738222\n",
      "epoch(84)(fusion):  0.7758232771507706\n",
      "train (85): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (86): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(86)(video):  0.7263057634666867\n",
      "epoch(86)(audio):  0.667769246087238\n",
      "epoch(86)(text):  0.7245824715147339\n",
      "epoch(86)(fusion):  0.773388372851644\n",
      "train (87): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (88): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(88)(video):  0.7231976830209861\n",
      "epoch(88)(audio):  0.6634307456283456\n",
      "epoch(88)(text):  0.7213613594560491\n",
      "epoch(88)(fusion):  0.7730601888308147\n",
      "train (89): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (90): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(90)(video):  0.7217621151258341\n",
      "epoch(90)(audio):  0.6662160398144317\n",
      "epoch(90)(text):  0.7223200009088583\n",
      "epoch(90)(fusion):  0.7732265557120689\n",
      "train (91): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n",
      "train (92): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(92)(video):  0.7174118606224864\n",
      "epoch(92)(audio):  0.6663206262376494\n",
      "epoch(92)(text):  0.7219746156878906\n",
      "epoch(92)(fusion):  0.7727533029795\n",
      "train (93): 100%|█████████████████████████████| 141/141 [01:22<00:00,  1.70it/s]\n",
      "train (94): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(94)(video):  0.7239782740506473\n",
      "epoch(94)(audio):  0.6674195434851593\n",
      "epoch(94)(text):  0.7217852044346231\n",
      "epoch(94)(fusion):  0.7730330033882259\n",
      "train (95): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (96): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(96)(video):  0.7104932338777554\n",
      "epoch(96)(audio):  0.6663982299351929\n",
      "epoch(96)(text):  0.7230367087953083\n",
      "epoch(96)(fusion):  0.7697696932227208\n",
      "train (97): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "train (98): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.69it/s]\n",
      "epoch(98)(video):  0.717726326718626\n",
      "epoch(98)(audio):  0.6682744731533162\n",
      "epoch(98)(text):  0.7191298045956719\n",
      "epoch(98)(fusion):  0.7722978617332252\n",
      "train (99): 100%|█████████████████████████████| 141/141 [01:23<00:00,  1.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# 采用CLR\n",
    "!python -W ignore main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "train (0): 100%|██████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "epoch(0)(video):  0.6877457526799775\n",
      "epoch(0)(audio):  0.6308331982614531\n",
      "epoch(0)(text):  0.670376392229117\n",
      "epoch(0)(fusion):  0.6747854759542581\n",
      "train (1): 100%|██████████████████████████████| 141/141 [02:12<00:00,  1.06it/s]\n",
      "train (2): 100%|██████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(2)(video):  0.693667140584023\n",
      "epoch(2)(audio):  0.6585518457524178\n",
      "epoch(2)(text):  0.7010523575275538\n",
      "epoch(2)(fusion):  0.7304162270066559\n",
      "train (3): 100%|██████████████████████████████| 141/141 [02:15<00:00,  1.04it/s]\n",
      "train (4): 100%|██████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "epoch(4)(video):  0.6871334554560866\n",
      "epoch(4)(audio):  0.6549115172508388\n",
      "epoch(4)(text):  0.7104762004986902\n",
      "epoch(4)(fusion):  0.7440553837973364\n",
      "train (5): 100%|██████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (6): 100%|██████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(6)(video):  0.7093188990806428\n",
      "epoch(6)(audio):  0.6622505098565559\n",
      "epoch(6)(text):  0.7234418533812326\n",
      "epoch(6)(fusion):  0.7587983008870778\n",
      "train (7): 100%|██████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "train (8): 100%|██████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "epoch(8)(video):  0.7201194360245657\n",
      "epoch(8)(audio):  0.6708403142293068\n",
      "epoch(8)(text):  0.7269139569786893\n",
      "epoch(8)(fusion):  0.7631799288882269\n",
      "train (9): 100%|██████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "train (10): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "epoch(10)(video):  0.720841341458806\n",
      "epoch(10)(audio):  0.6637340520022453\n",
      "epoch(10)(text):  0.7283278028287158\n",
      "epoch(10)(fusion):  0.7601562208279993\n",
      "train (11): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "train (12): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "epoch(12)(video):  0.7261648052936946\n",
      "epoch(12)(audio):  0.6688374547713961\n",
      "epoch(12)(text):  0.7261890312875523\n",
      "epoch(12)(fusion):  0.767991830957142\n",
      "train (13): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "train (14): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "epoch(14)(video):  0.737609202783865\n",
      "epoch(14)(audio):  0.6764206024317176\n",
      "epoch(14)(text):  0.7288424479477645\n",
      "epoch(14)(fusion):  0.7759771498300665\n",
      "train (15): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (16): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(16)(video):  0.7357137716040549\n",
      "epoch(16)(audio):  0.674570015030125\n",
      "epoch(16)(text):  0.7198262158781776\n",
      "epoch(16)(fusion):  0.7723540691567763\n",
      "train (17): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "train (18): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(18)(video):  0.7446727018954007\n",
      "epoch(18)(audio):  0.6781827532036829\n",
      "epoch(18)(text):  0.7269762502237026\n",
      "epoch(18)(fusion):  0.7751891579523572\n",
      "train (19): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (20): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(20)(video):  0.7587661681286204\n",
      "epoch(20)(audio):  0.680364542224928\n",
      "epoch(20)(text):  0.7290474558058682\n",
      "epoch(20)(fusion):  0.7858614313927084\n",
      "train (21): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (22): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(22)(video):  0.7612528443150754\n",
      "epoch(22)(audio):  0.6845513510664163\n",
      "epoch(22)(text):  0.7329895835072398\n",
      "epoch(22)(fusion):  0.7866581255499548\n",
      "train (23): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (24): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(24)(video):  0.7621455970579666\n",
      "epoch(24)(audio):  0.6850910253336556\n",
      "epoch(24)(text):  0.7321089639990818\n",
      "epoch(24)(fusion):  0.7871244559870102\n",
      "train (25): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (26): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(26)(video):  0.7641629624056928\n",
      "epoch(26)(audio):  0.6824368383440687\n",
      "epoch(26)(text):  0.7315101283969863\n",
      "epoch(26)(fusion):  0.7870338842148494\n",
      "train (27): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (28): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(28)(video):  0.7626996079623679\n",
      "epoch(28)(audio):  0.6848484522089693\n",
      "epoch(28)(text):  0.7308978212479307\n",
      "epoch(28)(fusion):  0.7877035802380745\n",
      "train (29): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (30): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(30)(video):  0.7623606566498379\n",
      "epoch(30)(audio):  0.6865352385688155\n",
      "epoch(30)(text):  0.7302210914153234\n",
      "epoch(30)(fusion):  0.7879875852555087\n",
      "train (31): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (32): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(32)(video):  0.7589459492025693\n",
      "epoch(32)(audio):  0.6821452150429115\n",
      "epoch(32)(text):  0.7305624571994158\n",
      "epoch(32)(fusion):  0.7890155996102577\n",
      "train (33): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (34): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "epoch(34)(video):  0.7631203998151656\n",
      "epoch(34)(audio):  0.6821015719233003\n",
      "epoch(34)(text):  0.7275704755995162\n",
      "epoch(34)(fusion):  0.7881429282405406\n",
      "train (35): 100%|█████████████████████████████| 141/141 [02:12<00:00,  1.06it/s]\n",
      "train (36): 100%|█████████████████████████████| 141/141 [02:12<00:00,  1.06it/s]\n",
      "epoch(36)(video):  0.7627640034953077\n",
      "epoch(36)(audio):  0.683320700368351\n",
      "epoch(36)(text):  0.7297011243611362\n",
      "epoch(36)(fusion):  0.7875950182092403\n",
      "train (37): 100%|█████████████████████████████| 141/141 [02:12<00:00,  1.06it/s]\n",
      "train (38): 100%|█████████████████████████████| 141/141 [02:12<00:00,  1.06it/s]\n",
      "epoch(38)(video):  0.7617057844600484\n",
      "epoch(38)(audio):  0.6837361624792228\n",
      "epoch(38)(text):  0.7299591280181192\n",
      "epoch(38)(fusion):  0.7864787507567869\n",
      "train (39):  67%|████████████████████▏         | 95/141 [01:34<00:37,  1.22it/s]"
     ]
    }
   ],
   "source": [
    "# early fusion 去掉audio\n",
    "!python -W ignore main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "100%|███████████████████████████████████████| 5000/5000 [21:45<00:00,  3.83it/s]\n"
     ]
    }
   ],
   "source": [
    "!python -W ignore ensemble_inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -W ignore end2end_main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "train (0): 100%|██████████████████████████████| 142/142 [03:49<00:00,  1.62s/it]\n",
      "9.366913352214114\n",
      "train (1): 100%|██████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "6.514036913992653\n",
      "train (2): 100%|██████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "4.723316543538806\n",
      "train (3): 100%|██████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "3.946944408013787\n",
      "train (4): 100%|██████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "3.360135189244445\n",
      "train (5): 100%|██████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "3.105098417107488\n",
      "train (6): 100%|██████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "2.880410173409422\n",
      "train (7): 100%|██████████████████████████████| 142/142 [03:49<00:00,  1.61s/it]\n",
      "2.7021657161309687\n",
      "train (8): 100%|██████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "2.6136323457032864\n",
      "train (9): 100%|██████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "2.4704924729508413\n",
      "train (10): 100%|█████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "2.4048215176018193\n",
      "train (11): 100%|█████████████████████████████| 142/142 [04:53<00:00,  2.07s/it]\n",
      "2.3034196210579134\n",
      "train (12): 100%|█████████████████████████████| 142/142 [03:53<00:00,  1.65s/it]\n",
      "2.264077472854668\n",
      "train (13): 100%|█████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "2.178941273353469\n",
      "train (14): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.61s/it]\n",
      "2.086400933668647\n",
      "train (15): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.61s/it]\n",
      "2.155493936068575\n",
      "train (16): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.61s/it]\n",
      "2.0587834153376834\n",
      "train (17): 100%|█████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "2.08125823820141\n",
      "train (18): 100%|█████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "1.9688180030231746\n",
      "train (19): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.61s/it]\n",
      "1.947150190951119\n",
      "train (20): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.62s/it]\n",
      "1.8860853205264454\n",
      "train (21): 100%|█████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "1.9366214006719455\n",
      "train (22): 100%|█████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "1.889982842223745\n",
      "train (23): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.61s/it]\n",
      "1.8743702517428869\n",
      "train (24): 100%|█████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "1.85834508882442\n",
      "train (25): 100%|█████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "1.8802690321290996\n",
      "train (26): 100%|█████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "1.8813851437938045\n",
      "train (27): 100%|█████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "1.8514005315135902\n",
      "train (28): 100%|█████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "1.8128094975377473\n",
      "train (29): 100%|█████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "1.8589697186376009\n",
      "train (30): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.61s/it]\n",
      "1.8440852484232944\n",
      "train (31): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.61s/it]\n",
      "1.7915971094453838\n",
      "train (32): 100%|█████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "1.7964106051015183\n",
      "train (33): 100%|█████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "1.8941896776078453\n",
      "train (34): 100%|█████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "1.799225585561403\n",
      "train (35): 100%|█████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "1.8272384547851455\n",
      "train (36): 100%|█████████████████████████████| 142/142 [03:48<00:00,  1.61s/it]\n",
      "1.8538921945531603\n",
      "train (37): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.61s/it]\n",
      "1.8031935792573741\n",
      "train (38): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.61s/it]\n",
      "1.8023949953871714\n",
      "train (39): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.61s/it]\n",
      "1.813293134662467\n",
      "train (40): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.61s/it]\n",
      "1.8314421420365992\n",
      "train (41): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.61s/it]\n",
      "1.7644122998479386\n",
      "train (42): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.62s/it]\n",
      "1.7876035747393755\n",
      "train (43): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.61s/it]\n",
      "1.829881256734821\n",
      "train (44): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.61s/it]\n",
      "1.848603925234835\n",
      "train (45): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.61s/it]\n",
      "1.7831532291963066\n",
      "train (46): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.61s/it]\n",
      "1.8093742921318807\n",
      "train (47): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.61s/it]\n",
      "1.7935681452213879\n",
      "train (48): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.61s/it]\n",
      "1.811235333832217\n",
      "train (49): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.62s/it]\n",
      "1.7537535957887138\n",
      "train (50): 100%|█████████████████████████████| 142/142 [03:49<00:00,  1.61s/it]\n",
      "1.807280120715289\n"
     ]
    }
   ],
   "source": [
    "# without projector\n",
    "!python -W ignore enhance_main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "train (0): 100%|██████████████████████████████| 142/142 [02:37<00:00,  1.11s/it]\n",
      "4.337402454564269\n",
      "train (1): 100%|██████████████████████████████| 142/142 [02:16<00:00,  1.04it/s]\n",
      "2.98784107221684\n",
      "train (2): 100%|██████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "2.7946327504977373\n",
      "train (3): 100%|██████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "2.7608521958472023\n",
      "train (4): 100%|██████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "2.7310451772851003\n",
      "train (5): 100%|██████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "2.6302693855594583\n",
      "train (6): 100%|██████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "2.5157128085552807\n",
      "train (7): 100%|██████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "2.4987536005570856\n",
      "train (8): 100%|██████████████████████████████| 142/142 [02:18<00:00,  1.03it/s]\n",
      "2.4160651430277755\n",
      "train (9): 100%|██████████████████████████████| 142/142 [02:18<00:00,  1.03it/s]\n",
      "2.3888443834345106\n",
      "train (10): 100%|█████████████████████████████| 142/142 [02:18<00:00,  1.03it/s]\n",
      "2.3101551885336216\n",
      "train (11): 100%|█████████████████████████████| 142/142 [02:18<00:00,  1.03it/s]\n",
      "2.249055890969827\n",
      "train (12): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "2.1818922044525686\n",
      "train (13): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "2.0879664874412645\n",
      "train (14): 100%|█████████████████████████████| 142/142 [02:18<00:00,  1.03it/s]\n",
      "2.1119546613223115\n",
      "train (15): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "2.128420476342591\n",
      "train (16): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "2.0466151741189016\n",
      "train (17): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "2.1216982880108795\n",
      "train (18): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "2.025287222694343\n",
      "train (19): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "2.0182325168394706\n",
      "train (20): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.7752763880810267\n",
      "train (21): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.6982080814704088\n",
      "train (22): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.6479625013512624\n",
      "train (23): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.6290364861488342\n",
      "train (24): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.5502891133368855\n",
      "train (25): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.593741541177454\n",
      "train (26): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.5513160623295206\n",
      "train (27): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.5942608243982557\n",
      "train (28): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.581561931422059\n",
      "train (29): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.5136235935587279\n",
      "train (30): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.5489438990472069\n",
      "train (31): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.5749913487635867\n",
      "train (32): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.5229730350030979\n",
      "train (33): 100%|█████████████████████████████| 142/142 [02:18<00:00,  1.03it/s]\n",
      "1.5322386303418118\n",
      "train (34): 100%|█████████████████████████████| 142/142 [02:18<00:00,  1.03it/s]\n",
      "1.5451569855213165\n",
      "train (35): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.5149604304575583\n",
      "train (36): 100%|█████████████████████████████| 142/142 [02:18<00:00,  1.03it/s]\n",
      "1.5414072577382478\n",
      "train (37): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.4827025343834515\n",
      "train (38): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.5246380063849436\n",
      "train (39): 100%|█████████████████████████████| 142/142 [02:18<00:00,  1.03it/s]\n",
      "1.5144769822207975\n",
      "train (40): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.497603993600523\n",
      "train (41): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.4641303793645242\n",
      "train (42): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.4528612469283628\n",
      "train (43): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.4949052875310602\n",
      "train (44): 100%|█████████████████████████████| 142/142 [02:17<00:00,  1.03it/s]\n",
      "1.456768491738279\n",
      "train (45):  30%|█████████                     | 43/142 [00:52<01:25,  1.16it/s]^C\n"
     ]
    }
   ],
   "source": [
    "# with projector\n",
    "!python -W ignore enhance_main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"enhance_main.py\", line 37, in <module>\n",
      "    model = Dual(config['ModelConfig'])\n",
      "  File \"/home/tione/notebook/TAAC-2021/src/model/baseline_model.py\", line 181, in __init__\n",
      "    self.fusion_head_dict[modal] = fusion_head.get_instance(model_config['fusion_head_type'], fusion_head_params)\n",
      "  File \"/home/tione/notebook/TAAC-2021/src/fusion_head/__init__.py\", line 4, in get_instance\n"
     ]
    }
   ],
   "source": [
    "# resnet50 feature without projector\n",
    "!python -W ignore enhance_main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "train (0): 100%|██████████████████████████████| 157/157 [04:18<00:00,  1.65s/it]\n",
      "10.099449710481485\n",
      "train (1): 100%|██████████████████████████████| 157/157 [04:16<00:00,  1.64s/it]\n",
      "6.720394966708627\n",
      "train (2): 100%|██████████████████████████████| 157/157 [04:16<00:00,  1.63s/it]\n",
      "5.695182124520563\n",
      "train (3): 100%|██████████████████████████████| 157/157 [04:16<00:00,  1.63s/it]\n",
      "5.634158877050801\n",
      "train (4): 100%|██████████████████████████████| 157/157 [04:16<00:00,  1.63s/it]\n",
      "5.640968224045578\n",
      "train (5): 100%|██████████████████████████████| 157/157 [04:16<00:00,  1.63s/it]\n",
      "5.377309761229594\n",
      "train (6): 100%|██████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "5.245106627227394\n",
      "train (7): 100%|██████████████████████████████| 157/157 [04:16<00:00,  1.64s/it]\n",
      "5.143808392202779\n",
      "train (8): 100%|██████████████████████████████| 157/157 [04:16<00:00,  1.64s/it]\n",
      "4.9996907088407285\n",
      "train (9): 100%|██████████████████████████████| 157/157 [04:16<00:00,  1.64s/it]\n",
      "4.748236827789598\n",
      "train (10): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "4.634229583345401\n",
      "train (11): 100%|█████████████████████████████| 157/157 [04:16<00:00,  1.63s/it]\n",
      "4.384598065333761\n",
      "train (12): 100%|█████████████████████████████| 157/157 [04:16<00:00,  1.64s/it]\n",
      "4.4830267186377455\n",
      "train (13): 100%|█████████████████████████████| 157/157 [04:16<00:00,  1.64s/it]\n",
      "4.290951101643265\n",
      "train (14): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "4.180877489648807\n",
      "train (15): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "4.27319211792794\n",
      "train (16): 100%|█████████████████████████████| 157/157 [04:16<00:00,  1.64s/it]\n",
      "4.110105426448166\n",
      "train (17): 100%|█████████████████████████████| 157/157 [04:16<00:00,  1.64s/it]\n",
      "4.060144415326938\n",
      "train (18): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "4.097904786942111\n",
      "train (19): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "3.970664246826415\n",
      "train (20): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "2.991795173875845\n",
      "train (21): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "2.6754772663116455\n",
      "train (22): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "2.5296825808324632\n",
      "train (23): 100%|█████████████████████████████| 157/157 [04:16<00:00,  1.63s/it]\n",
      "2.581327748906081\n",
      "train (24): 100%|█████████████████████████████| 157/157 [04:16<00:00,  1.63s/it]\n",
      "2.4815419532690837\n",
      "train (25): 100%|█████████████████████████████| 157/157 [04:16<00:00,  1.63s/it]\n",
      "2.400214227901143\n",
      "train (26): 100%|█████████████████████████████| 157/157 [04:15<00:00,  1.63s/it]\n",
      "2.3407862285140215\n",
      "train (27): 100%|█████████████████████████████| 157/157 [04:16<00:00,  1.63s/it]\n",
      "2.33920259916099\n",
      "train (28): 100%|█████████████████████████████| 157/157 [04:16<00:00,  1.63s/it]\n",
      "2.265246440650551\n",
      "train (29): 100%|█████████████████████████████| 157/157 [04:16<00:00,  1.63s/it]\n",
      "2.2717663247114532\n",
      "train (30): 100%|█████████████████████████████| 157/157 [04:16<00:00,  1.63s/it]\n",
      "2.2725206400938096\n",
      "train (31):   6%|█▊                             | 9/157 [00:32<06:24,  2.60s/it]^C\n"
     ]
    }
   ],
   "source": [
    "# effcientnet feature with projector\n",
    "!python -W ignore enhance_main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "train (0): 100%|██████████████████████████████| 157/157 [04:19<00:00,  1.65s/it]\n",
      "9.509103556347501\n",
      "train (1): 100%|██████████████████████████████| 157/157 [04:16<00:00,  1.64s/it]\n",
      "6.535109586776442\n",
      "train (2): 100%|██████████████████████████████| 157/157 [04:16<00:00,  1.64s/it]\n",
      "5.6055108665660685\n",
      "train (3): 100%|██████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "5.567284230214016\n",
      "train (4): 100%|██████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "5.689026061137011\n",
      "train (5): 100%|██████████████████████████████| 157/157 [04:16<00:00,  1.64s/it]\n",
      "5.7825144612865085\n",
      "train (6): 100%|██████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "5.185401480668669\n",
      "train (7): 100%|██████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "4.902723602428558\n",
      "train (8): 100%|██████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "4.826621070788924\n",
      "train (9): 100%|██████████████████████████████| 157/157 [04:16<00:00,  1.64s/it]\n",
      "4.910003584661301\n",
      "train (10): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "4.357350833856376\n",
      "train (11): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "4.207617548620625\n",
      "train (12): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "4.387609202391023\n",
      "train (13): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "4.299264263954892\n",
      "train (14): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "3.9888358890630635\n",
      "train (15): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "4.090574650248145\n",
      "train (16): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "3.7468650902912115\n",
      "train (17): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "3.737072317463577\n",
      "train (18): 100%|█████████████████████████████| 157/157 [04:18<00:00,  1.65s/it]\n",
      "3.776727295984888\n",
      "train (19): 100%|█████████████████████████████| 157/157 [04:18<00:00,  1.64s/it]\n",
      "3.5440889650089726\n",
      "train (20): 100%|█████████████████████████████| 157/157 [04:18<00:00,  1.64s/it]\n",
      "3.427895918773238\n",
      "train (21): 100%|█████████████████████████████| 157/157 [04:18<00:00,  1.65s/it]\n",
      "3.2587924800860653\n",
      "train (22): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "3.2063731676454\n",
      "train (23): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "3.0843244510091794\n",
      "train (24): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "2.990848140352091\n",
      "train (25): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "2.939927296274027\n",
      "train (26): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "2.7967005717526576\n",
      "train (27): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "2.649218814388202\n",
      "train (28): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "2.6834882824284256\n",
      "train (29): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "2.562411314362933\n",
      "train (30): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "2.516355159176383\n",
      "train (31): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "2.4362496046503637\n",
      "train (32): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "2.359967173284786\n",
      "train (33): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "2.2854866981506348\n",
      "train (34): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "2.2155081033706665\n",
      "train (35): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "2.153927718757824\n",
      "train (36): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "2.17452581293264\n",
      "train (37): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "2.0588503863401475\n",
      "train (38): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "2.068834319995467\n",
      "train (39): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "2.025561758667041\n",
      "train (40): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "1.9456414189308313\n",
      "train (41): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "1.8942094168085961\n",
      "train (42): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "1.9362664374576253\n",
      "train (43): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "1.8973630697104582\n",
      "train (44): 100%|█████████████████████████████| 157/157 [04:18<00:00,  1.64s/it]\n",
      "1.8163754499641953\n",
      "train (45): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "1.8396130018173509\n",
      "train (46): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "1.823185654202844\n",
      "train (47): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "1.7418964638072214\n",
      "train (48): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "1.7298546809299735\n",
      "train (49): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "1.8005130564331249\n",
      "train (50): 100%|█████████████████████████████| 157/157 [04:17<00:00,  1.64s/it]\n",
      "1.7776247392034834\n"
     ]
    }
   ],
   "source": [
    "# effcientnet feature with projector\n",
    "!python -W ignore enhance_main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "train (0): 100%|██████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(0)(video):  0.6446036487041119\n",
      "epoch(0)(audio):  0.6310663583830755\n",
      "epoch(0)(text):  0.6413023622410992\n",
      "epoch(0)(fusion):  0.6529440333438447\n",
      "train (1): 100%|██████████████████████████████| 141/141 [02:12<00:00,  1.07it/s]\n",
      "train (2): 100%|██████████████████████████████| 141/141 [02:12<00:00,  1.06it/s]\n",
      "epoch(2)(video):  0.6580629264882779\n",
      "epoch(2)(audio):  0.6596856529245048\n",
      "epoch(2)(text):  0.696222468960726\n",
      "epoch(2)(fusion):  0.7201469747451641\n",
      "train (3): 100%|██████████████████████████████| 141/141 [02:12<00:00,  1.06it/s]\n",
      "train (4): 100%|██████████████████████████████| 141/141 [02:12<00:00,  1.06it/s]\n",
      "epoch(4)(video):  0.6691071530066378\n",
      "epoch(4)(audio):  0.6609075602155617\n",
      "epoch(4)(text):  0.706789503442907\n",
      "epoch(4)(fusion):  0.7388235143328459\n",
      "train (5): 100%|██████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (6): 100%|██████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(6)(video):  0.6796647510822745\n",
      "epoch(6)(audio):  0.6636212949481117\n",
      "epoch(6)(text):  0.7132334244143081\n",
      "epoch(6)(fusion):  0.7473254006009679\n",
      "train (7): 100%|██████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (8): 100%|██████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(8)(video):  0.6867039115962422\n",
      "epoch(8)(audio):  0.671601749533195\n",
      "epoch(8)(text):  0.7197959486458482\n",
      "epoch(8)(fusion):  0.7581076052374783\n",
      "train (9): 100%|██████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (10): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "epoch(10)(video):  0.6941366670069864\n",
      "epoch(10)(audio):  0.6688128921291089\n",
      "epoch(10)(text):  0.7238488072836912\n",
      "epoch(10)(fusion):  0.7581053484549565\n",
      "train (11): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (12): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(12)(video):  0.7068485468392748\n",
      "epoch(12)(audio):  0.6754475608712771\n",
      "epoch(12)(text):  0.7216816232754547\n",
      "epoch(12)(fusion):  0.7568399234090151\n",
      "train (13): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (14): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "epoch(14)(video):  0.7186686902408215\n",
      "epoch(14)(audio):  0.6705062745146894\n",
      "epoch(14)(text):  0.7217819184477062\n",
      "epoch(14)(fusion):  0.7630696017707085\n",
      "train (15): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (16): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(16)(video):  0.7269330590188111\n",
      "epoch(16)(audio):  0.6713871171134418\n",
      "epoch(16)(text):  0.7199488718082447\n",
      "epoch(16)(fusion):  0.770073240745269\n",
      "train (17): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (18): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "epoch(18)(video):  0.71715654982996\n",
      "epoch(18)(audio):  0.680163018032094\n",
      "epoch(18)(text):  0.7238441880671189\n",
      "epoch(18)(fusion):  0.7656596464474938\n",
      "train (19): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "train (20): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "epoch(20)(video):  0.7503389614169028\n",
      "epoch(20)(audio):  0.6835138581356198\n",
      "epoch(20)(text):  0.72372054441771\n",
      "epoch(20)(fusion):  0.7773619094963947\n",
      "train (21): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (22): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(22)(video):  0.7505020369972765\n",
      "epoch(22)(audio):  0.6852687406714297\n",
      "epoch(22)(text):  0.7253044627208417\n",
      "epoch(22)(fusion):  0.7799459068578279\n",
      "train (23): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (24): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(24)(video):  0.7487973331948801\n",
      "epoch(24)(audio):  0.6841104782093435\n",
      "epoch(24)(text):  0.7245276990706043\n",
      "epoch(24)(fusion):  0.7794385246865393\n",
      "train (25): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (26): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(26)(video):  0.7540907920194754\n",
      "epoch(26)(audio):  0.6849787655292563\n",
      "epoch(26)(text):  0.7264554483849047\n",
      "epoch(26)(fusion):  0.7806969765841402\n",
      "train (27): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (28): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(28)(video):  0.7528814734412171\n",
      "epoch(28)(audio):  0.6832826889869164\n",
      "epoch(28)(text):  0.7243725160408747\n",
      "epoch(28)(fusion):  0.7822184162157099\n",
      "train (29): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (30): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(30)(video):  0.7563383671392498\n",
      "epoch(30)(audio):  0.684582844836468\n",
      "epoch(30)(text):  0.7256136345312678\n",
      "epoch(30)(fusion):  0.7817666821302663\n",
      "train (31): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (32): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(32)(video):  0.7577776878003472\n",
      "epoch(32)(audio):  0.6837345036799253\n",
      "epoch(32)(text):  0.7252728400457193\n",
      "epoch(32)(fusion):  0.7821223992858185\n",
      "train (33): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "train (34): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "epoch(34)(video):  0.7564641924341253\n",
      "epoch(34)(audio):  0.6842632689693545\n",
      "epoch(34)(text):  0.7260161522322571\n",
      "epoch(34)(fusion):  0.7827187199836263\n",
      "train (35): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (36): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(36)(video):  0.7571650784783808\n",
      "epoch(36)(audio):  0.6845636129387831\n",
      "epoch(36)(text):  0.7256784668615589\n",
      "epoch(36)(fusion):  0.7814190798514861\n",
      "train (37): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (38): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(38)(video):  0.7587599572618527\n",
      "epoch(38)(audio):  0.682958455670887\n",
      "epoch(38)(text):  0.7256342825610004\n",
      "epoch(38)(fusion):  0.7822217936874913\n",
      "train (39): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (40): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(40)(video):  0.7573437818941128\n",
      "epoch(40)(audio):  0.6841313376282709\n",
      "epoch(40)(text):  0.7240829060409436\n",
      "epoch(40)(fusion):  0.7811885712322693\n",
      "train (41): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (42): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(42)(video):  0.7566114332470611\n",
      "epoch(42)(audio):  0.6839431190833247\n",
      "epoch(42)(text):  0.7245872166469339\n",
      "epoch(42)(fusion):  0.7810110602594287\n",
      "train (43): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (44): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "epoch(44)(video):  0.7572661298745521\n",
      "epoch(44)(audio):  0.6829504791526141\n",
      "epoch(44)(text):  0.7249770597494456\n",
      "epoch(44)(fusion):  0.7818251182130472\n",
      "train (45): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.05it/s]\n",
      "train (46): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "epoch(46)(video):  0.7566258034646733\n",
      "epoch(46)(audio):  0.6839171959903212\n",
      "epoch(46)(text):  0.7253645333870977\n",
      "epoch(46)(fusion):  0.7823021373621243\n",
      "train (47): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "train (48): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "epoch(48)(video):  0.7575233249400177\n",
      "epoch(48)(audio):  0.6854248218075996\n",
      "epoch(48)(text):  0.7247578521267279\n",
      "epoch(48)(fusion):  0.783040449002405\n",
      "train (49): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "train (50): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "epoch(50)(video):  0.7589922804889343\n",
      "epoch(50)(audio):  0.682878426996442\n",
      "epoch(50)(text):  0.7257698082971269\n",
      "epoch(50)(fusion):  0.7835190032112658\n",
      "train (51): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "train (52): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "epoch(52)(video):  0.7577561466712104\n",
      "epoch(52)(audio):  0.6842916554243091\n",
      "epoch(52)(text):  0.7240685686434891\n",
      "epoch(52)(fusion):  0.7827822302935444\n",
      "train (53): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "train (54): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "epoch(54)(video):  0.7571245129054455\n",
      "epoch(54)(audio):  0.6849510422132673\n",
      "epoch(54)(text):  0.7239773448968874\n",
      "epoch(54)(fusion):  0.7832424508263734\n",
      "train (55): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "train (56): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "epoch(56)(video):  0.7583351473287534\n",
      "epoch(56)(audio):  0.6840648370315161\n",
      "epoch(56)(text):  0.7248664749136373\n",
      "epoch(56)(fusion):  0.7832717609241033\n",
      "train (57): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "train (58): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "epoch(58)(video):  0.7586632745919851\n",
      "epoch(58)(audio):  0.683345822656272\n",
      "epoch(58)(text):  0.7255402580673471\n",
      "epoch(58)(fusion):  0.7828915735765488\n",
      "train (59): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "train (60): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "epoch(60)(video):  0.756924052334665\n",
      "epoch(60)(audio):  0.6818630979754321\n",
      "epoch(60)(text):  0.7248411247796127\n",
      "epoch(60)(fusion):  0.7823233727036538\n",
      "train (61): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "train (62): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "epoch(62)(video):  0.7591582951397278\n",
      "epoch(62)(audio):  0.6837285718550569\n",
      "epoch(62)(text):  0.7241323965480074\n",
      "epoch(62)(fusion):  0.7822194519983542\n",
      "train (63): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "train (64): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "epoch(64)(video):  0.7591727111900894\n",
      "epoch(64)(audio):  0.6840928580644907\n",
      "epoch(64)(text):  0.7237045388677816\n",
      "epoch(64)(fusion):  0.7822105333931434\n",
      "train (65):   0%|                                       | 0/141 [00:00<?, ?it/s]^C\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 88, in _path_is_mode_type\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 82, in _path_stat\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/google/__init__.py'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/spawn.py\", line 105, in spawn_main\n",
      "    exitcode = _main(fd)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/spawn.py\", line 114, in _main\n",
      "train (65):   0%|                                       | 0/141 [00:04<?, ?it/s]\n",
      "    prepare(preparation_data)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/spawn.py\", line 225, in prepare\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 120, in <module>\n",
      "    _fixup_main_from_path(data['init_main_from_path'])\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/spawn.py\", line 277, in _fixup_main_from_path\n",
      "    loss = training_loop(model, train_loader, loss_compute, modal_name_list,train_dataset.device, epoch,TBoard)\n",
      "  File \"/home/tione/notebook/TAAC-2021/src/loop/run_epoch.py\", line 15, in training_loop\n",
      "    run_name=\"__mp_main__\")\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/runpy.py\", line 263, in run_path\n",
      "    for i, batch in enumerate(tqdm(loader, desc=f'train ({epoch})')):\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/tqdm/std.py\", line 1129, in __iter__\n",
      "    pkg_name=pkg_name, script_name=fname)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/runpy.py\", line 96, in _run_module_code\n",
      "    mod_name, mod_spec, pkg_name, script_name)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/tione/notebook/TAAC-2021/main.py\", line 10, in <module>\n",
      "    from dataloader.dataloader import MultimodaFeaturesDataset,Datasetfortextcnn\n",
      "  File \"/home/tione/notebook/TAAC-2021/dataloader/dataloader.py\", line 20, in <module>\n",
      "    import jieba\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/jieba/__init__.py\", line 14, in <module>\n",
      "    from . import finalseg\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/jieba/finalseg/__init__.py\", line 6, in <module>\n",
      "    from .._compat import *\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/jieba/_compat.py\", line 18, in <module>\n",
      "    import pkg_resources\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 3260, in <module>\n",
      "    for obj in iterable:\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 279, in __iter__\n",
      "    return _MultiProcessingDataLoaderIter(self)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 719, in __init__\n",
      "    w.start()\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/process.py\", line 105, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/context.py\", line 223, in _Popen\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/context.py\", line 284, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n",
      "    super().__init__(process_obj)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 62, in _launch\n",
      "    f.write(fp.getbuffer())\n",
      "KeyboardInterrupt\n",
      "    @_call_aside\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 3244, in _call_aside\n",
      "    f(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 3288, in _initialize_master_working_set\n",
      "    for dist in working_set\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 3288, in <genexpr>\n",
      "    for dist in working_set\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 2791, in activate\n",
      "    fixup_namespace_packages(self.location)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 2312, in fixup_namespace_packages\n",
      "    subpath = _handle_ns(package, path_item)\n",
      "  File \"/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 2213, in _handle_ns\n",
      "    loader = importer.find_spec(packageName).loader\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1260, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 96, in _path_isfile\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 88, in _path_is_mode_type\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# 使用eff特征，感觉学习率下降太快了 rd = 2021\n",
    "!python -W ignore main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "train (0): 100%|██████████████████████████████| 141/141 [02:19<00:00,  1.01it/s]\n",
      "epoch(0)(video):  0.6737959510809641\n",
      "epoch(0)(audio):  0.6325109449641705\n",
      "epoch(0)(text):  0.6727785931081434\n",
      "epoch(0)(fusion):  0.6791459705921509\n",
      "train (1): 100%|██████████████████████████████| 141/141 [02:17<00:00,  1.03it/s]\n",
      "train (2): 100%|██████████████████████████████| 141/141 [02:17<00:00,  1.02it/s]\n",
      "epoch(2)(video):  0.6939735114843086\n",
      "epoch(2)(audio):  0.6590888508492984\n",
      "epoch(2)(text):  0.7021770166206738\n",
      "epoch(2)(fusion):  0.7317602054853215\n",
      "train (3): 100%|██████████████████████████████| 141/141 [02:17<00:00,  1.03it/s]\n",
      "train (4): 100%|██████████████████████████████| 141/141 [02:15<00:00,  1.04it/s]\n",
      "epoch(4)(video):  0.7004971265481379\n",
      "epoch(4)(audio):  0.6644567624366364\n",
      "epoch(4)(text):  0.7160551789192584\n",
      "epoch(4)(fusion):  0.743014667871989\n",
      "train (5): 100%|██████████████████████████████| 141/141 [02:16<00:00,  1.03it/s]\n",
      "train (6): 100%|██████████████████████████████| 141/141 [02:15<00:00,  1.04it/s]\n",
      "epoch(6)(video):  0.721405263214548\n",
      "epoch(6)(audio):  0.661810060885675\n",
      "epoch(6)(text):  0.7116808019583566\n",
      "epoch(6)(fusion):  0.7563108887660064\n",
      "train (7): 100%|██████████████████████████████| 141/141 [02:16<00:00,  1.04it/s]\n",
      "train (8): 100%|██████████████████████████████| 141/141 [02:15<00:00,  1.04it/s]\n",
      "epoch(8)(video):  0.7232070394316487\n",
      "epoch(8)(audio):  0.6667046165623629\n",
      "epoch(8)(text):  0.7248138388834356\n",
      "epoch(8)(fusion):  0.7623988576630887\n",
      "train (9): 100%|██████████████████████████████| 141/141 [02:15<00:00,  1.04it/s]\n",
      "train (10): 100%|█████████████████████████████| 141/141 [02:16<00:00,  1.03it/s]\n",
      "epoch(10)(video):  0.7194986197678113\n",
      "epoch(10)(audio):  0.6684610416708254\n",
      "epoch(10)(text):  0.7234808229544235\n",
      "epoch(10)(fusion):  0.7599711649303406\n",
      "train (11): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "train (12): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "epoch(12)(video):  0.7284312900716086\n",
      "epoch(12)(audio):  0.671994763572517\n",
      "epoch(12)(text):  0.7266655254305857\n",
      "epoch(12)(fusion):  0.7696796326883923\n",
      "train (13): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "train (14): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "epoch(14)(video):  0.7297109536158611\n",
      "epoch(14)(audio):  0.6737620820378243\n",
      "epoch(14)(text):  0.7204840668011716\n",
      "epoch(14)(fusion):  0.7676765343946812\n",
      "train (15): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "train (16): 100%|█████████████████████████████| 141/141 [02:15<00:00,  1.04it/s]\n",
      "epoch(16)(video):  0.7325840174840214\n",
      "epoch(16)(audio):  0.6729179459279278\n",
      "epoch(16)(text):  0.7275185199573624\n",
      "epoch(16)(fusion):  0.7721084024244966\n",
      "train (17): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "train (18): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "epoch(18)(video):  0.7024532204009464\n",
      "epoch(18)(audio):  0.6728363196426801\n",
      "epoch(18)(text):  0.7252664442027531\n",
      "epoch(18)(fusion):  0.7713998207425641\n",
      "train (19): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "train (20): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "epoch(20)(video):  0.7579354265487659\n",
      "epoch(20)(audio):  0.6798752867534139\n",
      "epoch(20)(text):  0.7262644713362367\n",
      "epoch(20)(fusion):  0.7841525414678641\n",
      "train (21): 100%|█████████████████████████████| 141/141 [02:17<00:00,  1.03it/s]\n",
      "train (22): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "epoch(22)(video):  0.7588003585004267\n",
      "epoch(22)(audio):  0.681058095733041\n",
      "epoch(22)(text):  0.728804414433248\n",
      "epoch(22)(fusion):  0.7853010283751339\n",
      "train (23): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "train (24): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "epoch(24)(video):  0.7614882887042898\n",
      "epoch(24)(audio):  0.6821975915741781\n",
      "epoch(24)(text):  0.7295102987666584\n",
      "epoch(24)(fusion):  0.7856847193357468\n",
      "train (25): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "train (26): 100%|█████████████████████████████| 141/141 [02:19<00:00,  1.01it/s]\n",
      "epoch(26)(video):  0.7622648452776805\n",
      "epoch(26)(audio):  0.6806405481791767\n",
      "epoch(26)(text):  0.7273087592053029\n",
      "epoch(26)(fusion):  0.7859596501086926\n",
      "train (27): 100%|█████████████████████████████| 141/141 [02:19<00:00,  1.01it/s]\n",
      "train (28): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "epoch(28)(video):  0.7610095132493924\n",
      "epoch(28)(audio):  0.680179493202187\n",
      "epoch(28)(text):  0.7278107413363378\n",
      "epoch(28)(fusion):  0.7869902027136345\n",
      "train (29): 100%|█████████████████████████████| 141/141 [02:17<00:00,  1.03it/s]\n",
      "train (30): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "epoch(30)(video):  0.7633080362979902\n",
      "epoch(30)(audio):  0.6797545629501269\n",
      "epoch(30)(text):  0.7280090650666479\n",
      "epoch(30)(fusion):  0.7856931030558808\n",
      "train (31): 100%|█████████████████████████████| 141/141 [02:19<00:00,  1.01it/s]\n",
      "train (32): 100%|█████████████████████████████| 141/141 [02:19<00:00,  1.01it/s]\n",
      "epoch(32)(video):  0.7573063744351723\n",
      "epoch(32)(audio):  0.6798222831294207\n",
      "epoch(32)(text):  0.7246192900766408\n",
      "epoch(32)(fusion):  0.7846512338251567\n",
      "train (33): 100%|█████████████████████████████| 141/141 [02:19<00:00,  1.01it/s]\n",
      "train (34): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "epoch(34)(video):  0.7630806063753964\n",
      "epoch(34)(audio):  0.6797378019059765\n",
      "epoch(34)(text):  0.7280213582562058\n",
      "epoch(34)(fusion):  0.7847618256696572\n",
      "train (35): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "train (36): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "epoch(36)(video):  0.7625117202416929\n",
      "epoch(36)(audio):  0.6786140495397599\n",
      "epoch(36)(text):  0.7257805422647157\n",
      "epoch(36)(fusion):  0.7856752844332009\n",
      "train (37): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "train (38): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "epoch(38)(video):  0.7608831409237855\n",
      "epoch(38)(audio):  0.6787719927448869\n",
      "epoch(38)(text):  0.7271322422045641\n",
      "epoch(38)(fusion):  0.7844957748370659\n",
      "train (39): 100%|█████████████████████████████| 141/141 [02:17<00:00,  1.02it/s]\n",
      "train (40): 100%|█████████████████████████████| 141/141 [02:17<00:00,  1.02it/s]\n",
      "epoch(40)(video):  0.7642125296602533\n",
      "epoch(40)(audio):  0.6788262307737544\n",
      "epoch(40)(text):  0.7263539756171065\n",
      "epoch(40)(fusion):  0.7863153620487565\n",
      "train (41): 100%|█████████████████████████████| 141/141 [02:17<00:00,  1.03it/s]\n",
      "train (42): 100%|█████████████████████████████| 141/141 [02:17<00:00,  1.02it/s]\n",
      "epoch(42)(video):  0.7634695075270369\n",
      "epoch(42)(audio):  0.6803894242435473\n",
      "epoch(42)(text):  0.7266911148289494\n",
      "epoch(42)(fusion):  0.7863281345357631\n",
      "train (43): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "train (44): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "epoch(44)(video):  0.7652421098209435\n",
      "epoch(44)(audio):  0.6791244137507456\n",
      "epoch(44)(text):  0.7254006679674847\n",
      "epoch(44)(fusion):  0.7861832182487207\n",
      "train (45): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "train (46): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "epoch(46)(video):  0.7635162299556184\n",
      "epoch(46)(audio):  0.6783352383500862\n",
      "epoch(46)(text):  0.7270413906831462\n",
      "epoch(46)(fusion):  0.7864098884361208\n",
      "train (47): 100%|█████████████████████████████| 141/141 [02:13<00:00,  1.06it/s]\n",
      "train (48): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "epoch(48)(video):  0.7626305786103434\n",
      "epoch(48)(audio):  0.6788647511945082\n",
      "epoch(48)(text):  0.7260905339083819\n",
      "epoch(48)(fusion):  0.7854149129413965\n",
      "train (49): 100%|█████████████████████████████| 141/141 [02:15<00:00,  1.04it/s]\n",
      "train (50): 100%|█████████████████████████████| 141/141 [02:17<00:00,  1.03it/s]\n",
      "epoch(50)(video):  0.7637185590026485\n",
      "epoch(50)(audio):  0.67914163913239\n",
      "epoch(50)(text):  0.7271512518552831\n",
      "epoch(50)(fusion):  0.787309013797199\n",
      "train (51): 100%|█████████████████████████████| 141/141 [02:17<00:00,  1.03it/s]\n",
      "train (52): 100%|█████████████████████████████| 141/141 [02:16<00:00,  1.04it/s]\n",
      "epoch(52)(video):  0.7619614593097469\n",
      "epoch(52)(audio):  0.6800117793074132\n",
      "epoch(52)(text):  0.7266554433401029\n",
      "epoch(52)(fusion):  0.7865492393650912\n",
      "train (53): 100%|█████████████████████████████| 141/141 [02:17<00:00,  1.03it/s]\n",
      "train (54): 100%|█████████████████████████████| 141/141 [02:16<00:00,  1.03it/s]\n",
      "epoch(54)(video):  0.7637760349941863\n",
      "epoch(54)(audio):  0.6791043552742805\n",
      "epoch(54)(text):  0.7270928328073019\n",
      "epoch(54)(fusion):  0.7863721967823727\n",
      "train (55): 100%|█████████████████████████████| 141/141 [02:17<00:00,  1.03it/s]\n",
      "train (56): 100%|█████████████████████████████| 141/141 [02:15<00:00,  1.04it/s]\n",
      "epoch(56)(video):  0.7630295247927129\n",
      "epoch(56)(audio):  0.6792532062218125\n",
      "epoch(56)(text):  0.7272919908026768\n",
      "epoch(56)(fusion):  0.7866663867456084\n",
      "train (57): 100%|█████████████████████████████| 141/141 [02:16<00:00,  1.03it/s]\n",
      "train (58): 100%|█████████████████████████████| 141/141 [02:16<00:00,  1.03it/s]\n",
      "epoch(58)(video):  0.7626922514531732\n",
      "epoch(58)(audio):  0.6781729569648578\n",
      "epoch(58)(text):  0.7270601915431187\n",
      "epoch(58)(fusion):  0.7859164144672228\n",
      "train (59): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "train (60): 100%|█████████████████████████████| 141/141 [02:16<00:00,  1.03it/s]\n",
      "epoch(60)(video):  0.7625636556900772\n",
      "epoch(60)(audio):  0.6783530681255631\n",
      "epoch(60)(text):  0.7262837790667357\n",
      "epoch(60)(fusion):  0.7863950794840853\n",
      "train (61): 100%|█████████████████████████████| 141/141 [02:16<00:00,  1.03it/s]\n",
      "train (62): 100%|█████████████████████████████| 141/141 [02:17<00:00,  1.02it/s]\n",
      "epoch(62)(video):  0.7645127698181372\n",
      "epoch(62)(audio):  0.6791270052142881\n",
      "epoch(62)(text):  0.727355468699524\n",
      "epoch(62)(fusion):  0.7871420416790454\n",
      "train (63): 100%|█████████████████████████████| 141/141 [02:15<00:00,  1.04it/s]\n",
      "train (64): 100%|█████████████████████████████| 141/141 [02:16<00:00,  1.04it/s]\n",
      "epoch(64)(video):  0.7641073664564243\n",
      "epoch(64)(audio):  0.679281632829659\n",
      "epoch(64)(text):  0.7266631031225081\n",
      "epoch(64)(fusion):  0.7861012039933793\n",
      "train (65): 100%|█████████████████████████████| 141/141 [02:16<00:00,  1.03it/s]\n",
      "train (66): 100%|█████████████████████████████| 141/141 [02:16<00:00,  1.03it/s]\n",
      "epoch(66)(video):  0.7624510098480696\n",
      "epoch(66)(audio):  0.6798101628064654\n",
      "epoch(66)(text):  0.7264691328811882\n",
      "epoch(66)(fusion):  0.7861522445736763\n",
      "train (67): 100%|█████████████████████████████| 141/141 [02:16<00:00,  1.04it/s]\n",
      "train (68): 100%|█████████████████████████████| 141/141 [02:16<00:00,  1.03it/s]\n",
      "epoch(68)(video):  0.7638292959870593\n",
      "epoch(68)(audio):  0.6792514137695232\n",
      "epoch(68)(text):  0.727696971221782\n",
      "epoch(68)(fusion):  0.7866828665309901\n",
      "train (69): 100%|█████████████████████████████| 141/141 [02:18<00:00,  1.02it/s]\n",
      "train (70): 100%|█████████████████████████████| 141/141 [02:17<00:00,  1.03it/s]\n",
      "epoch(70)(video):  0.7641978878495578\n",
      "epoch(70)(audio):  0.6787531425042923\n",
      "epoch(70)(text):  0.7266458890621358\n",
      "epoch(70)(fusion):  0.7860327471072426\n",
      "train (71): 100%|█████████████████████████████| 141/141 [02:16<00:00,  1.03it/s]\n",
      "train (72): 100%|█████████████████████████████| 141/141 [02:17<00:00,  1.03it/s]\n",
      "epoch(72)(video):  0.7640157040239931\n",
      "epoch(72)(audio):  0.6797666549962507\n",
      "epoch(72)(text):  0.7264958956705437\n",
      "epoch(72)(fusion):  0.7874313904425488\n",
      "train (73): 100%|█████████████████████████████| 141/141 [02:16<00:00,  1.03it/s]\n",
      "train (74): 100%|█████████████████████████████| 141/141 [02:16<00:00,  1.03it/s]\n",
      "epoch(74)(video):  0.7636876073932353\n",
      "epoch(74)(audio):  0.6801426731409708\n",
      "epoch(74)(text):  0.726465888400453\n",
      "epoch(74)(fusion):  0.7861975860387431\n",
      "train (75): 100%|█████████████████████████████| 141/141 [02:16<00:00,  1.03it/s]\n",
      "train (76): 100%|█████████████████████████████| 141/141 [02:17<00:00,  1.03it/s]\n",
      "epoch(76)(video):  0.7634917449344003\n",
      "epoch(76)(audio):  0.6797940151712398\n",
      "epoch(76)(text):  0.726369579602804\n",
      "epoch(76)(fusion):  0.7867958911503484\n",
      "train (77): 100%|█████████████████████████████| 141/141 [02:16<00:00,  1.03it/s]\n",
      "train (78): 100%|█████████████████████████████| 141/141 [02:16<00:00,  1.03it/s]\n",
      "epoch(78)(video):  0.7655450421864085\n",
      "epoch(78)(audio):  0.6803580718733127\n",
      "epoch(78)(text):  0.7259028093559373\n",
      "epoch(78)(fusion):  0.7868947043694932\n",
      "train (79): 100%|█████████████████████████████| 141/141 [02:17<00:00,  1.02it/s]\n",
      "train (80): 100%|█████████████████████████████| 141/141 [02:16<00:00,  1.03it/s]\n",
      "epoch(80)(video):  0.7619526529795895\n",
      "epoch(80)(audio):  0.6793200125335219\n",
      "epoch(80)(text):  0.728501182296453\n",
      "epoch(80)(fusion):  0.7858023597166031\n",
      "train (81): 100%|█████████████████████████████| 141/141 [02:17<00:00,  1.03it/s]\n",
      "train (82): 100%|█████████████████████████████| 141/141 [02:16<00:00,  1.03it/s]\n",
      "epoch(82)(video):  0.7628468424515843\n",
      "epoch(82)(audio):  0.6793786793314789\n",
      "epoch(82)(text):  0.7276359960878461\n",
      "epoch(82)(fusion):  0.786906172497887\n",
      "train (83): 100%|█████████████████████████████| 141/141 [02:15<00:00,  1.04it/s]\n",
      "train (84): 100%|█████████████████████████████| 141/141 [02:15<00:00,  1.04it/s]\n",
      "epoch(84)(video):  0.7642336130336334\n",
      "epoch(84)(audio):  0.6778271855856118\n",
      "epoch(84)(text):  0.7263933924539431\n",
      "epoch(84)(fusion):  0.786734677642688\n",
      "train (85): 100%|█████████████████████████████| 141/141 [02:15<00:00,  1.04it/s]\n",
      "train (86): 100%|█████████████████████████████| 141/141 [02:15<00:00,  1.04it/s]\n",
      "epoch(86)(video):  0.7648421328267209\n",
      "epoch(86)(audio):  0.679749214858312\n",
      "epoch(86)(text):  0.726059725534307\n",
      "epoch(86)(fusion):  0.7877762823712448\n",
      "train (87): 100%|█████████████████████████████| 141/141 [02:15<00:00,  1.04it/s]\n",
      "train (88): 100%|█████████████████████████████| 141/141 [02:15<00:00,  1.04it/s]\n",
      "epoch(88)(video):  0.762512574867405\n",
      "epoch(88)(audio):  0.6791110685911588\n",
      "epoch(88)(text):  0.7273109877165717\n",
      "epoch(88)(fusion):  0.7868415325463501\n",
      "train (89): 100%|█████████████████████████████| 141/141 [02:15<00:00,  1.04it/s]\n",
      "train (90): 100%|█████████████████████████████| 141/141 [02:17<00:00,  1.03it/s]\n",
      "epoch(90)(video):  0.7623500822881172\n",
      "epoch(90)(audio):  0.6788039069803329\n",
      "epoch(90)(text):  0.7258380939261533\n",
      "epoch(90)(fusion):  0.7865249243670681\n",
      "train (91): 100%|█████████████████████████████| 141/141 [02:15<00:00,  1.04it/s]\n",
      "train (92): 100%|█████████████████████████████| 141/141 [02:15<00:00,  1.04it/s]\n",
      "epoch(92)(video):  0.7637769169611347\n",
      "epoch(92)(audio):  0.6780630555161863\n",
      "epoch(92)(text):  0.7277980237843417\n",
      "epoch(92)(fusion):  0.7857671352601149\n",
      "train (93): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "train (94): 100%|█████████████████████████████| 141/141 [02:15<00:00,  1.04it/s]\n",
      "epoch(94)(video):  0.7625604926916831\n",
      "epoch(94)(audio):  0.6792729575774198\n",
      "epoch(94)(text):  0.7268126621449758\n",
      "epoch(94)(fusion):  0.787360416264361\n",
      "train (95): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "train (96): 100%|█████████████████████████████| 141/141 [02:14<00:00,  1.05it/s]\n",
      "epoch(96)(video):  0.7636182170873425\n",
      "epoch(96)(audio):  0.6797703680109142\n",
      "epoch(96)(text):  0.7267323428825982\n",
      "epoch(96)(fusion):  0.7868263530215965\n",
      "train (97):  13%|███▊                          | 18/141 [00:33<01:46,  1.16it/s]^C\n"
     ]
    }
   ],
   "source": [
    "# 使用eff特征，自监督 rd = 2022\n",
    "!python -W ignore main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "initialization: Kaiming\n",
      "100%|███████████████████████████████████████| 5000/5000 [02:40<00:00, 31.09it/s]\n"
     ]
    }
   ],
   "source": [
    "!python -W ignore inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/ipykernel/__main__.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization: Kaiming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization: Kaiming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization: Kaiming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization: Kaiming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization: Kaiming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization: Kaiming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization: Kaiming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization: Kaiming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization: Kaiming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained/bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization: Kaiming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "0it [00:00, ?it/s]/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "63it [00:49,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'video': 0.7680026128688282, 'audio': 0.6899082404671032, 'text': 0.7450814895545617, 'fusion': 0.7916295465394785}\n"
     ]
    }
   ],
   "source": [
    "# ensemble 模型在验证集上测试\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5,6,7'\n",
    "import yaml\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import utils.train_util as train_util\n",
    "from dataloader.dataloader import TestingDataset\n",
    "from src.loss.loss_compute import SimpleLossCompute\n",
    "from src.model.baseline_model import Baseline\n",
    "from src.loop.run_epoch import training_loop,validating_loop\n",
    "from dataloader.dataloader import MultimodaFeaturesDataset,Datasetfortextcnn\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 8\n",
    "modal_name_list = ['video','audio','text']\n",
    "config_path = './config/config.yaml'\n",
    "config = yaml.load(open(config_path))\n",
    "dataset = MultimodaFeaturesDataset(config['DatasetConfig'],job='valdation')\n",
    "loader = DataLoader(dataset,num_workers=8,\n",
    "                    batch_size=batch_size,\n",
    "                    pin_memory=False,\n",
    "                    collate_fn=dataset.collate_fn)\n",
    "\n",
    "model_path_1 = '../checkpoint/0607/01/epoch_56 0.7846.pt'\n",
    "model_path_2 = '../checkpoint/0607/01/epoch_42 0.7843.pt'\n",
    "model_path_3 = '../checkpoint/0607/02/epoch_28 0.7822.pt'\n",
    "model_path_4 = '../checkpoint/0607/02/epoch_50 0.7835.pt'\n",
    "model_path_5 = '../checkpoint/0607/01/epoch_28 0.7831.pt'\n",
    "model_path_6 = '../checkpoint/0607/02/epoch_48 0.7830.pt'\n",
    "model_path_7 = '../checkpoint/0607/03/epoch_88 0.7873.pt'\n",
    "model_path_8 = '../checkpoint/0607/03/epoch_46 0.7862.pt'\n",
    "model_path_9 = '../checkpoint/0607/03/epoch_28 0.7855.pt'\n",
    "model_path_10 = '../checkpoint/0608/01/epoch_86 0.7877.pt'\n",
    "model_path_11 = '../checkpoint/0608/01/epoch_50 0.7873.pt'\n",
    "model_path_12 = '../checkpoint/0608/01/epoch_28 0.7869.pt'\n",
    "\n",
    "model_path_13 = '../checkpoint/0608/03/epoch_28 0.7877.pt'\n",
    "model_path_14 = '../checkpoint/0608/03/epoch_52 0.7890.pt'\n",
    "model_path_15 = '../checkpoint/0608/03/epoch_74 0.7896.pt'\n",
    "models_path = [model_path_1,\n",
    "               model_path_7,model_path_8,model_path_9,\n",
    "               model_path_10,model_path_11,\n",
    "               model_path_14,model_path_15]\n",
    "device = 'cuda'\n",
    "top_k=20\n",
    "# output_json = './0604_resnet_ensemble.json'\n",
    "models = []\n",
    "for path in models_path:\n",
    "    if(path.split('/')[2]+path.split('/')[3]!='060803'):\n",
    "        config['ModelConfig']['fusion_head_params']['concat_feat_dim']['fusion'] = 30720\n",
    "        config['ModelConfig']['audio_head_params']['max_frames'] = 300\n",
    "    else:\n",
    "        config['ModelConfig']['fusion_head_params']['concat_feat_dim']['fusion'] = 29696\n",
    "        config['ModelConfig']['audio_head_params']['max_frames'] = 200\n",
    "    model = Baseline(config['ModelConfig'])\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "tagging_class_num = 82\n",
    "evl_metrics = [train_util.EvaluationMetrics(tagging_class_num, top_k=20)\n",
    "                           for i in range(len(modal_name_list)+1)] #+1 for fusion\n",
    "for i in range(len(evl_metrics)):\n",
    "    evl_metrics[i].clear()\n",
    "metric_dict = {}\n",
    "gap_dict = {}\n",
    "with torch.no_grad():\n",
    "    for i,batch in tqdm(enumerate(loader)):\n",
    "        if(len(batch)==5):\n",
    "            video,audio,text,text_mask,label = batch\n",
    "            video = video.to(device)\n",
    "            audio = audio.to(device)\n",
    "            text = text.to(device)\n",
    "            text_mask = text_mask.to(device)\n",
    "            label = label.to(device)\n",
    "        else:\n",
    "            video,audio,text,label = batch\n",
    "            video = video.to(device)\n",
    "            audio = audio.to(device)\n",
    "            text = text.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "        inputs_dict={}\n",
    "        inputs_dict['video'] = video\n",
    "        inputs_dict['audio'] = audio\n",
    "        inputs_dict['text'] = text \n",
    "        if(len(batch)==5):\n",
    "            inputs_dict['attention_mask'] = text_mask\n",
    "        else:\n",
    "            inputs_dict['attention_mask'] = None\n",
    "\n",
    "        B = video.shape[0]\n",
    "        pred_dict_ensemble = {}\n",
    "        for modal_name in (modal_name_list+['fusion']):\n",
    "            pred_dict_ensemble['tagging_output_'+modal_name] = {}\n",
    "            pred_dict_ensemble['tagging_output_'+modal_name]['predictions'] = torch.zeros(B,82).cuda()\n",
    "\n",
    "        for model in models:\n",
    "            pred_dict = model(inputs_dict)\n",
    "            for modal_name in (modal_name_list+['fusion']):\n",
    "                pred_dict_ensemble['tagging_output_'+modal_name]['predictions'] += pred_dict['tagging_output_'+modal_name]['predictions']\n",
    "\n",
    "        for modal_name in (modal_name_list+['fusion']):\n",
    "            pred_dict_ensemble['tagging_output_'+modal_name]['predictions'] = pred_dict_ensemble['tagging_output_'+modal_name]['predictions']/len(models)\n",
    "\n",
    "        for index,modal_name in enumerate(modal_name_list+['fusion']):\n",
    "            pred = pred_dict_ensemble['tagging_output_'+modal_name]\n",
    "            pred = pred['predictions'].detach().cpu().numpy()\n",
    "            val_label = label.cpu().numpy()\n",
    "            gap = train_util.calculate_gap(pred, val_label)\n",
    "            evl_metrics[index].accumulate(pred, val_label, loss=0)\n",
    "    for index,modal_name in enumerate(modal_name_list+['fusion']):\n",
    "        metric_dict[modal_name] = evl_metrics[index].get()\n",
    "        gap_dict[modal_name] = metric_dict[modal_name]['gap']\n",
    "    print(gap_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../checkpoint/0607/03/epoch_88 0.7873.pt'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_py3",
   "language": "python",
   "name": "conda_pytorch_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
